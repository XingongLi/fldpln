{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to fldpln","text":"<p>Flood inundation modeling and mapping using the FLDPLN model</p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://xingongli.github.io/fldpln</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"common/","title":"common module","text":"<p>The common module contains common variables, functions and classes used by the other modules.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/xingongli/fldpln/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>fldpln could always use more documentation, whether as part of the official fldpln docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/xingongli/fldpln/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up fldpln for local development.</p> <ol> <li> <p>Fork the fldpln repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/fldpln.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv fldpln\n$ cd fldpln/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 fldpln tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/xingongli/fldpln/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"gauge/","title":"gauge module","text":"<p>Module for handling USGS and AHPS gauge data for flood inundation mapping.</p>"},{"location":"gauge/#fldpln.gauge.GetAhpsGaugeDatumElevation","title":"<code>GetAhpsGaugeDatumElevation(ahpsGaugeUrl)</code>","text":"<p>Retrieve gauge stage datum elevation and vertical datum name from AHPS web HTML page.</p> <p>Parameters:</p> Name Type Description Default <code>ahpsGaugeUrl</code> <code>str</code> <p>URL of AHPS gauge web page.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>vertical datum name (str), gauge stage datum elevation (float).</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetAhpsGaugeDatumElevation(ahpsGaugeUrl):\n    \"\"\" Retrieve gauge stage datum elevation and vertical datum name from AHPS web HTML page.\n\n        Args:\n            ahpsGaugeUrl (str): URL of AHPS gauge web page.\n\n        Return:\n            tuple: vertical datum name (str), gauge stage datum elevation (float).\n    \"\"\"\n\n    # request the html page\n    response = requests.get(ahpsGaugeUrl)\n    htmlTree= html.fromstring(response.content)\n\n    navd88Path= '//table[@class=\"aboutthislocation_toggle\"]/tr/td/table[1]/tr[4]/td/text()'\n    ngvd29Path= '//table[@class=\"aboutthislocation_toggle\"]/tr/td/table[1]/tr[5]/td/text()'\n    otherPath= '//table[@class=\"aboutthislocation_toggle\"]/tr/td/table[1]/tr[7]/td/text()'\n    datums = [htmlTree.xpath(path)[0:2] for path in [navd88Path,ngvd29Path,otherPath]]\n\n    datumEle = np.nan\n    for datum in datums:\n        datumName=datum[0].replace(' ','')\n        if datumName == 'Other':\n            datumName = np.nan\n        datumEleStr= datum[1]\n        if datumEleStr != 'Not Available':\n            datumEleStr=datumEleStr.split(' ')[0].replace(',','')\n            datumEle = float(datumEleStr)\n            break\n    # print(f'Datum name: {datumName}, Datum elevation: {datumEle}')\n    return (datumName,datumEle)\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetAhpsGaugeForecast","title":"<code>GetAhpsGaugeForecast(scratchFolder, fcstLength, gaugeDatumFile)</code>","text":"<p>Get AHPS gauge forecast for a specified number of future days.</p> <p>Parameters:</p> Name Type Description Default <code>scratchFolder</code> <code>str</code> <p>scratch folder to store downloaded files.</p> required <code>fcstLength</code> <code>int</code> <p>forecast length in days between 0 and 14 days. 0 is current observation.</p> required <code>gaugeDatumFile</code> <code>str</code> <p>file name of gauge datum information.</p> required <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of AHPS gauge forecast.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetAhpsGaugeForecast(scratchFolder,fcstLength,gaugeDatumFile):\n    \"\"\" Get AHPS gauge forecast for a specified number of future days.\n\n        Args:\n            scratchFolder (str): scratch folder to store downloaded files.\n            fcstLength (int): forecast length in days between 0 and 14 days. 0 is current observation.\n            gaugeDatumFile (str): file name of gauge datum information.\n\n        Return:\n            data frame: a data frame of AHPS gauge forecast.\n    \"\"\"\n\n    from osgeo import ogr # ogr cannot be installed using pip. So we put it only in the functions that use it and DO NOT include it in requirements.txt. It's user's responsibility to install it.\n\n    print('Downloading AHPS gauge data ...')\n\n    if isinstance(fcstLength,int) and (fcstLength&gt;=0) and (fcstLength&lt;=14):\n        if fcstLength == 0:\n            # current observation\n            ahpsFileName = 'tgz_obs'\n        else:\n            # forecast between 1 to 14 days\n            numOfHours=fcstLength*24\n            fcstHours = f'f{numOfHours:03}'\n            ahpsFileName = 'tgz_fcst_'+fcstHours\n    else:\n        print('Illegal type!')\n        return None\n\n    # Clean out existing obs file\n    if os.path.isfile(os.path.join(scratchFolder,'ahps_download.tgz')):\n        os.remove(os.path.join(scratchFolder,'ahps_download.tgz'))\n\n    # delete the shapefile if existing\n    if fcstLength == 0:\n        # current observation\n        shpFullName = os.path.join(scratchFolder,'national_shapefile_obs.shp')\n    else:\n        # forecast\n        shpFullName = os.path.join(scratchFolder,f'national_shapefile_fcst_{fcstHours}.shp')\n\n    shpDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n    if os.path.exists(shpFullName):\n        shpDriver.DeleteDataSource(shpFullName)\n\n    # download current gauge observations\n    ahpsUrl = r\"https://water.weather.gov/ahps/download.php?data=\" + ahpsFileName #tgz_obs\"\n    ahpsDownload = requests.get(ahpsUrl)\n    # save the download in a file\n    zippedFile = os.path.join(scratchFolder, \"ahps_download.tgz\")\n    with open(zippedFile, \"wb\") as f:\n        f.write(ahpsDownload.content)\n\n    # unzipping the tgz and tar\n    with tarfile.open(zippedFile, 'r:gz') as tf:\n        tf.extractall(path=scratchFolder)\n\n    # read gauge stage\n    if fcstLength == 0:\n        # current observation\n        gaugeColumns = [\"GaugeLID\",\"Status\",\"Observed\",\"ObsTime\"]\n        gaugeObsDf = gpd.read_file(shpFullName)\n        gaugeObsDf = gaugeObsDf[gaugeColumns]\n        # print(gaugeObsDf)\n\n        # some gauge obs may have latency \n        nowTime = datetime.now()\n        threeDaysAgo = nowTime - timedelta(days=3)\n        # select obs within 3 days\n        gaugeObsDf = gaugeObsDf[gaugeObsDf.ObsTime &gt; str(threeDaysAgo)]\n        # print(gaugeObsDf)\n\n        # select the stage based on obs type\n        stageColumn = 'Observed'\n    else:\n        # forecast between 1 to 14 days\n        gaugeColumns = [\"GaugeLID\",\"Status\",\"Forecast\",\"FcstTime\",\"FcstIssunc\"]\n        gaugeObsDf = gpd.read_file(shpFullName)\n        gaugeObsDf = gaugeObsDf[gaugeColumns]\n        # print(gaugeObsDf)\n\n        # select the stage based on obs type\n        stageColumn = 'Forecast'\n\n    ## Merge with precomputed datum, clean up resulting frame\n    # gaugeDatumFile = os.path.join(libFolder,ahpsGaugeDatumFileName)\n    gaugeWithDatum = pd.read_csv(gaugeDatumFile)[['GaugeLID','X','Y','Datum']]\n    gaugeObsDf = gaugeObsDf[['GaugeLID',stageColumn]]\n    gaugeObsDf = gaugeObsDf.merge(gaugeWithDatum, how='left', on='GaugeLID')\n    # print(gaugeObsDf)\n\n    # gaugeObsDf['Stage'] = pd.to_numeric(gaugeObsDf[stageColumn], errors='coerce')\n    gaugeObsDf[stageColumn] = pd.to_numeric(gaugeObsDf[stageColumn], errors='coerce')\n    gaugeObsDf['Datum'] = pd.to_numeric(gaugeObsDf['Datum'])\n\n    # Create DTF value\n    gaugeObsDf['GaugeWSE'] = gaugeObsDf[stageColumn] + gaugeObsDf['Datum']\n    # remove nodata rows/gauges\n    # gaugeObsDf = gaugeObsDf[gaugeObsDf['GaugeWSE'].notna()]\n    # print(gaugeObsDf)\n\n    # gaugeObsDf.to_csv('GaugeObs.csv',index=False)\n\n    return gaugeObsDf\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetAhpsGaugeHistoricalFloodStages","title":"<code>GetAhpsGaugeHistoricalFloodStages(scratchFolder, gaugeDatumFile)</code>","text":"<p>Get AHPS gauge historical flood stages: 'Action','Flood','Moderate','Major'.</p> <p>Parameters:</p> Name Type Description Default <code>scratchFolder</code> <code>str</code> <p>scratch folder to store downloaded files.</p> required <code>gaugeDatumFile</code> <code>str</code> <p>file name of gauge datum information.</p> required <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of AHPS gauge historical flood stages.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetAhpsGaugeHistoricalFloodStages(scratchFolder,gaugeDatumFile):\n    \"\"\" Get AHPS gauge historical flood stages: 'Action','Flood','Moderate','Major'.\n\n        Args:\n            scratchFolder (str): scratch folder to store downloaded files.\n            gaugeDatumFile (str): file name of gauge datum information.\n\n        Return:\n            data frame: a data frame of AHPS gauge historical flood stages.\n    \"\"\"\n\n    from osgeo import ogr # ogr cannot be installed using pip. So we put it only in the functions that use it and DO NOT include it in requirements.txt. It's user's responsibility to install it.\n\n    print('Downloading AHPS historical flood stages ...')\n\n    # Clean out existing obs file\n    if os.path.isfile(os.path.join(scratchFolder,'ahps_download.tgz')):\n        os.remove(os.path.join(scratchFolder,'ahps_download.tgz'))\n\n    # delete the shapefile if existing\n    shpFullName = os.path.join(scratchFolder,'national_shapefile_obs.shp')\n    shpDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n    if os.path.exists(shpFullName):\n        shpDriver.DeleteDataSource(shpFullName)\n\n    # download current gauge observations\n    ahpsUrl = r\"https://water.weather.gov/ahps/download.php?data=tgz_obs\"\n    ahpsDownload = requests.get(ahpsUrl)\n    # save the download in a file\n    zippedFile = os.path.join(scratchFolder, \"ahps_download.tgz\")\n    with open(zippedFile, \"wb\") as f:\n        f.write(ahpsDownload.content)\n\n    # unzipping the tgz and tar\n    with tarfile.open(zippedFile, 'r:gz') as tf:\n        tf.extractall(path=scratchFolder)\n\n    # read historical gauge stages\n    gaugeColumns = [\"GaugeLID\",\"Status\",'Action','Flood','Moderate','Major']\n    gaugeObsDf = gpd.read_file(shpFullName)\n    gaugeObsDf = gaugeObsDf[gaugeColumns]\n    # print(gaugeObsDf)\n\n    # Merge with precomputed datum, clean up resulting frame\n    # gaugeDatumFile = os.path.join(libFolder,ahpsGaugeDatumFileName)\n    gaugeWithDatum = pd.read_csv(gaugeDatumFile)[['GaugeLID','X','Y','Datum']]\n    gaugeObsDf = gaugeObsDf.merge(gaugeWithDatum, how='left', on='GaugeLID')\n    # print(gaugeObsDf)\n\n    # convert stage columns into numeric type\n    gaugeObsDf['Datum'] = pd.to_numeric(gaugeObsDf['Datum'])\n    for stageColumn in ['Action','Flood','Moderate','Major']:\n        gaugeObsDf[stageColumn] = pd.to_numeric(gaugeObsDf[stageColumn], errors='coerce')\n        # convert stage to water surface elevation\n        gaugeObsDf[stageColumn] = gaugeObsDf[stageColumn] + gaugeObsDf['Datum']\n\n    # gaugeObsDf.to_csv('GaugeHistoricalStages.csv',index=False)\n\n    return gaugeObsDf\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetAhpsGaugeStageFromWebService","title":"<code>GetAhpsGaugeStageFromWebService(ahpsIds, fcstDays=0, histFloodType=None)</code>","text":"<p>Get AHPS gauge stage from web service.</p> <p>Parameters:</p> Name Type Description Default <code>ahpsIds</code> <code>list</code> <p>a list of AHPS gauge IDs.</p> required <code>fcstDays</code> <code>int</code> <p>forecasted time in 0 to 14 days. 0 is current obersevation. default to 0.</p> <code>0</code> <code>histFloodType</code> <code>str</code> <p>historical flood types of Major, Moderate, Flood, Action. default to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of AHPS gauge stage.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetAhpsGaugeStageFromWebService(ahpsIds, fcstDays=0, histFloodType=None):\n    \"\"\" Get AHPS gauge stage from web service.\n\n        Args:\n            ahpsIds (list): a list of AHPS gauge IDs.\n            fcstDays (int): forecasted time in 0 to 14 days. 0 is current obersevation. default to 0.\n            histFloodType (str): historical flood types of Major, Moderate, Flood, Action. default to None.\n\n        Return:\n            data frame: a data frame of AHPS gauge stage.\n    \"\"\"\n\n    # # NWS/AHPS local WFS layers. NOT working\n    # ahpsWfsServices = ['https://idpgis.ncep.noaa.gov/arcgis/rest/services/NWS_Observations/ahps_riv_gauges/MapServer/{}/query'.format(id) for id in range(16)]\n    # # Select a WFS service\n    # sUrl = ahpsWfsServices[0]\n\n    # NWS/NOAA/AHPS AWS feature service layers\n    # fields in the features are available best through the shapefiles from https://water.weather.gov/ahps/download.php\n    # Meaning of the WFS service ID: 0--Observed; 1-- forecast 1-day; ...; 14--forecast 14-day; 15--same as 14?\n    ahpsAwsWfsServices = ['https://mapservices.weather.noaa.gov/eventdriven/rest/services/water/ahps_riv_gauges/MapServer/{}/query'.format(id) for id in range(16)]\n\n    #\n    # Prepare service query\n    #\n    # creating the query strings and request a json response from the site\n    whereClause = \" OR \".join([f\"gaugelid='{x}'\" for x in ahpsIds])\n    # query parameters\n    payload = {'where': whereClause, 'outFields':'*','f':'pjson'} # pjson = plain JSON?\n\n    # Select services based fcstDays: 0--Observed; 1-- forecast 1-day; ...; 14--forecast 14-day; 15--same as 14?\n    if fcstDays in range(16):\n        sUrl = ahpsAwsWfsServices[fcstDays]\n    else:\n        print('Incorrect fcstDays!')\n        return None\n\n    # send request\n    response = requests.get(sUrl, params=payload, verify=False) # trust the service\n    # print(response.request.url)\n\n    # get the features and put them into a dataframe\n    features = response.json()['features']\n    attributes = [x['attributes'] for x in features]\n\n    # select attributes based on stage type\n    if histFloodType is None:\n        # get forecast stage\n        if fcstDays == 0:\n            df = pd.DataFrame(attributes)[['gaugelid','observed','obstime']] #,'status']]\n            df = df.rename(columns={'gaugelid':'stationid','observed':'stage_ft','obstime':'stage_time'})\n        else:\n            df = pd.DataFrame(attributes)[['gaugelid','forecast','fcsttime']] #,'status']]\n            df = df.rename(columns={'gaugelid':'stationid','forecast':'stage_ft','fcsttime':'stage_time'})\n    elif histFloodType in ['Major', 'Moderate', 'Flood', 'Action']:\n        # get historical flood stage\n        df = pd.DataFrame(attributes)[['gaugelid',histFloodType.lower()]]\n        df = df.rename(columns={'gaugelid':'stationid',histFloodType.lower():'stage_ft'})\n        df['stage_time'] = ''\n\n    # turn field \"stage_ft\" into numeric\n    df['stage_ft'] = pd.to_numeric(df['stage_ft'], errors='coerce')\n\n    # reset index\n    df.reset_index(drop=True,inplace=True)\n\n    return df\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetAhpsGauges","title":"<code>GetAhpsGauges(geobox, epsg=32614)</code>","text":"<p>Get AHPS gauges within a box and project them to a specified coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>list</code> <p>a list of geographic box elements of [minX,minY,maxX,maxY]</p> required <code>epsg</code> <code>int</code> <p>EPSG integer representing the projected coordinate system, default to UTM14 (epsg = 32614) for Kansas.</p> <code>32614</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of AHPS gauges projected to the specified coordinate system.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetAhpsGauges(geobox,epsg=32614):\n    \"\"\" Get AHPS gauges within a box and project them to a specified coordinate system.\n\n        Args:\n            geobox (list): a list of geographic box elements of [minX,minY,maxX,maxY]\n            epsg (int): EPSG integer representing the projected coordinate system, default to UTM14 (epsg = 32614) for Kansas.\n\n        Return:\n            data frame: a data frame of AHPS gauges projected to the specified coordinate system.\n    \"\"\"\n\n    from osgeo import ogr # ogr cannot be installed using pip. So we put it only in the functions that use it and DO NOT include it in requirements.txt. It's user's responsibility to install it.\n\n    with tempfile.TemporaryDirectory() as scratchFolder:\n        print('Downloading AHPS gauge shapefile ...')\n\n        # Clean out existing obs file\n        if os.path.isfile(os.path.join(scratchFolder,'ahps_download.tgz')):\n            os.remove(os.path.join(scratchFolder,'ahps_download.tgz'))\n\n        # delete the shapefile if existing\n        shpFullName = os.path.join(scratchFolder,'national_shapefile_obs.shp')\n        shpDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n        if os.path.exists(shpFullName):\n            shpDriver.DeleteDataSource(shpFullName)\n\n        # download current gauge observations\n        ahpsUrl = r\"https://water.weather.gov/ahps/download.php?data=tgz_obs\"\n        ahpsDownload = requests.get(ahpsUrl)\n        # save the download in a file\n        zippedFile = os.path.join(scratchFolder, \"ahps_download.tgz\")\n        with open(zippedFile, \"wb\") as f:\n            f.write(ahpsDownload.content)\n\n        # unzipping the tgz and tar\n        with tarfile.open(zippedFile, 'r:gz') as tf:\n            tf.extractall(path=scratchFolder)\n\n        # read AHPS shapefile\n        gauges = gpd.read_file(shpFullName)\n        # fields in the shapefile:\n        # 'GaugeLID', 'count', 'Status', 'Location', 'Latitude', 'Longitude',\n        #    'Waterbody', 'State', 'Observed', 'ObsTime', 'Units', 'Action', 'Flood',\n        #    'Moderate', 'Major', 'LowThresh', 'LowThreshU', 'WFO', 'HDatum',\n        #    'PEDTS', 'SecValue', 'SecUnit', 'URL'\n\n        # select the gauges within a geographical box\n        minX,minY,maxX,maxY = geobox\n        gauges = gauges[(gauges['Longitude']&gt;=minX) &amp; (gauges['Longitude']&lt;=maxX) &amp; (gauges['Latitude']&gt;=minY) &amp; (gauges['Latitude']&lt;=maxY)]\n\n        print('Project the shapefile to library coordinate system ...')\n        # project gauges to library coordinate system, i.e., UTM 14N\n        gauges = gauges.to_crs(epsg=epsg)\n        # save the projected shapefile\n        # gauges.to_file(shpFullName)\n\n        # add gauge's coordinates as fields\n        gauges['x'] = gauges['geometry'].x\n        gauges['y'] = gauges['geometry'].y\n\n        # turn gpd to pd\n        gauges = gauges.drop(columns=['geometry'])\n        # # select cols\n        # cols = ['GaugeLID','Status','Location','Latitude','Longitude','Waterbody','State','WFO','HDatum','URL','Units','Action','Flood','Moderate','Major','X','Y']\n        # gauges = gauges[cols]\n        # print('Gauges with UTM 14N coordinates:',gauges)\n        # print(f'Total gauges: {len(gauges)}')\n\n    return gauges\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetAhpsUsgsGaugeStageFromWebServices","title":"<code>GetAhpsUsgsGaugeStageFromWebServices(gaugeIdOrgs, whichStage='Nowcast', periodInDays=7)</code>","text":"<p>Read AHPS/USGS gauge stage from their respective web services. </p> <p>Parameters:</p> Name Type Description Default <code>gaugeIdOrgs</code> <code>data frame</code> <p>a data frame with columns of [\"stationid\", \"organization\"]. stationid is either USGS, AHPS, or the combination of their gauge IDs. Organization is either USGS, AHPS or both.</p> required <code>whichStage</code> <code>str</code> <p>Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major.</p> <code>'Nowcast'</code> <code>periodInDays</code> <code>int</code> <p>period in days (0 - 14) for forecast. Default to 7.</p> <code>7</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetAhpsUsgsGaugeStageFromWebServices(gaugeIdOrgs, whichStage='Nowcast', periodInDays=7):\n    \"\"\" Read AHPS/USGS gauge stage from their respective web services. \n\n        Args:\n            gaugeIdOrgs (data frame): a data frame with columns of [\"stationid\", \"organization\"]. stationid is either USGS, AHPS, or the combination of their gauge IDs. Organization is either USGS, AHPS or both.\n            whichStage (str): Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major.\n            periodInDays (int): period in days (0 - 14) for forecast. Default to 7.\n\n        Return:\n            data frame: a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time.\n    \"\"\"\n\n    # get gauge id of each organization and put them into 4 gauge ID groups\n    ahpsIds = [] # only AHPS gauges\n    usgsIds = [] # only USGS gauges \n    commAhpsIds=[]; commUsgsIds=[] # both AHPS and USGS gauges\n    idDict={} # dict to map common gauges from AHPS or USGS ID to combined ID (e.g., sid)\n    for row in gaugeIdOrgs.itertuples(): \n        sid, org = row.stationid, row.organization\n        if 'AHPS' in org: \n            if org == 'AHPS':\n                # only AHPS\n                ahpsId = sid\n                ahpsIds.append(ahpsId)\n                idDict.update({ahpsId:sid})\n            else: # common gauges\n                commUsgsId, commAhpsId = sid.split(',')\n                commAhpsIds.append(commAhpsId)\n                commUsgsIds.append(commUsgsId)\n                idDict.update({commAhpsId:sid})\n                idDict.update({commUsgsId:sid})\n        elif org == 'USGS':\n            # only USGS\n            usgsId = sid\n            usgsIds.append(sid)\n            idDict.update({usgsId:sid})\n    # print(f'AHPS gauges: {len(ahpsIds)}, USGS gauges: {len(usgsIds)}')\n\n    #\n    # Get gauge stage from their web services\n    #\n    # Use different gauges for different stage types\n    # Action, Flood, Moderate, Major: only AHPS + common AHPS\n    # Nowcast : only AHPS + common AHPS + only USGS\n    # Forecast: only AHPS + common AHPS\n    # Postcast: only USGS + common USGS\n    #\n    ahpsIds = ahpsIds + commAhpsIds\n    # use just AHPS gauges\n    if whichStage in ['Action', 'Flood', 'Moderate', 'Major']:\n        gaugeStages = GetAhpsGaugeStageFromWebService(ahpsIds,fcstDays=0,histFloodType=whichStage)\n\n    # use all the gauges\n    if whichStage in ['Nowcast']:\n        ahpsStages = GetAhpsGaugeStageFromWebService(ahpsIds,fcstDays=0)\n        print(f'AHPS gauges with stages: {len(ahpsStages)}')\n        usgsStages = GetUsgsGaugeStageFromWebService(usgsIds,startDate='Now',endDate='MostRecent')\n        print(f'USGS gauges with stages: {len(usgsStages)}')\n        # combine the gauges\n        gaugeStages = pd.concat([ahpsStages, usgsStages])\n\n    # use just AHPS gauges\n    if whichStage in ['Forecast']:\n        gaugeStages = GetAhpsGaugeStageFromWebService(ahpsIds, periodInDays)\n\n    # use just USGS gauges\n    if whichStage in ['Postcast']:\n        gaugeStages = GetUsgsGaugeStageFromWebService(usgsIds+commUsgsIds, startDate='Now',endDate=periodInDays)\n        # find the max stage within the time period\n        maxStages = gaugeStages.groupby(['stationid'],as_index=False).agg({'stage_ft':'max'})\n        # find the most recent time with the max stage\n        df = pd.merge(gaugeStages, maxStages, how='inner', on=['stationid','stage_ft'])\n        gaugeStages = df.groupby(['stationid'], as_index=False).agg({'stationid':'first','stage_ft':'first','stage_time':'max'})\n\n    # change stationid to the original id (i.e., AHPS ID + USGS ID)\n    gaugeStages.reset_index(drop=True,inplace=True)\n    for idx, row in gaugeStages.iterrows():    \n        gaugeStages.at[idx,'stationid'] = idDict[row['stationid']]\n    # print(gaugeStages)\n    # gaugeStages.to_excel('gauge_stages.xlsx', index=False)\n\n    return gaugeStages\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetGaugeStageFromAhpsUsgsWebServices","title":"<code>GetGaugeStageFromAhpsUsgsWebServices(gaugeFile, whichStage='Nowcast')</code>","text":"<p>Read gauge stage from AHPS or USGS web services.</p> <p>Parameters:</p> Name Type Description Default <code>gaugeFile</code> <code>str</code> <p>file name of gauge information.</p> required <code>whichStage</code> <code>str</code> <p>Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major. Default to 'Nowcast'.</p> <code>'Nowcast'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time, status.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetGaugeStageFromAhpsUsgsWebServices(gaugeFile, whichStage='Nowcast'):\n    \"\"\" Read gauge stage from AHPS or USGS web services.\n\n        Args:\n            gaugeFile (str): file name of gauge information.\n            whichStage (str): Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major. Default to 'Nowcast'.\n\n        Return:\n            data frame: a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time, status.\n    \"\"\"\n\n    # read gauge file\n    gauges = pd.read_excel(gaugeFile, sheet_name='Sheet1')\n\n    # Get gauge IDs and organization from the DB table\n    gaugeIdOrgs = gauges[[\"stationid\", \"organization\"]]\n    # print(gaugeIdOrgs)\n\n    # get gauge id from the database table for each organization\n    ahpsIds = []; usgsIds = []; idDict={}\n    for row in gaugeIdOrgs.itertuples(): \n        sid, org = row.stationid, row.organization\n        if 'AHPS' in org: # common gauges put into AHPS gauges\n            if org == 'AHPS':\n                ahpsId = sid\n            else: # common gauges\n                ahpsId = sid.split(',')[1]\n            ahpsIds.append(ahpsId)\n            idDict.update({ahpsId:sid})\n        elif org == 'USGS':\n            usgsIds.append(sid)\n    # print(f'AHPS gauges: {len(ahpsIds)}, USGS gauges: {len(usgsIds)}')\n\n    #\n    # Get gauge stage from their web services\n    #\n    ahpsStages = GetAhpsGaugeStageFromWebService(ahpsIds)\n    # change stationid to the original id\n    for idx, row in ahpsStages.iterrows():    \n        # set vertical datum adjustment\n        ahpsStages.at[idx,'stationid'] = idDict[row['stationid']]\n    print(f'AHPS gauges with stages: {len(ahpsStages)}')\n    usgsStages = GetUsgsGaugeStageFromWebService(usgsIds)\n    print(f'USGS gauges with stages: {len(usgsStages)}')\n    gaugeStages = pd.concat([ahpsStages, usgsStages])\n    # gaugeStages.to_excel('gauge_stages.xlsx', index=False)\n    # print(gaugeStages)\n\n    # join with the gauges using station ID\n    gauges = pd.merge(gauges,gaugeStages,how='left',on='stationid')\n    # print(gauges)\n\n    # calculate stage elevation based on \"whichStage\"\n    stageFieldDic = {'Nowcast': 'stage_ft',\n                'Forecast': 'stage_ft',\n                'Postcast': 'stage_ft',\n                'Action': 'action_stage',\n                'Flood': 'flood_stage',\n                'Moderate': 'moderate_stage',\n                'Major': 'major_stage'\n                }\n    stages = pd.to_numeric(gauges[stageFieldDic[whichStage]], errors='coerce')\n    gauges['stage_elevation'] = stages + gauges['datum_elevation'] + gauges['to_navd88']\n    # print(gauges)\n\n    # only keep necessary fields\n    keptFields = ['stationid','x','y','stage_elevation','stage_time','status']\n    gauges = gauges[keptFields]\n    # print(gauges)\n\n    # remove stations with nan stage elevation (because of no datum elevation)\n    # gauges = gauges[gauges['stage_elevation'].isnull()]\n\n    return gauges\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetGaugeStageFromGaugeFc","title":"<code>GetGaugeStageFromGaugeFc(gaugeFcName, whichStage='Nowcast')</code>","text":"<p>Read gauge stage from a gauge feature class (FC) in PostgreSQL database. </p> <p>Parameters:</p> Name Type Description Default <code>gaugeFcName</code> <code>str</code> <p>gauge feature class name.</p> required <code>whichStage</code> <code>str</code> <p>Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major. Default to 'Nowcast'.</p> <code>'Nowcast'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time, status.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetGaugeStageFromGaugeFc(gaugeFcName, whichStage='Nowcast'):\n    \"\"\" Read gauge stage from a gauge feature class (FC) in PostgreSQL database. \n\n        Args:\n            gaugeFcName (str): gauge feature class name.\n            whichStage (str): Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major. Default to 'Nowcast'.\n\n        Return:\n            data frame: a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time, status.\n    \"\"\"\n\n    import psycopg2 # MacOs cannot install the package using pip. So we put it only in the functions that use it and DO NOT include it in requirements.txt. It's user's responsibility to install it.\n\n    #\n    # connect to the db\n    #\n    # read connection parameters\n    params = config() # see config.py, parameters are in database.ini\n    # connect to the PostgreSQL server\n    conn = psycopg2.connect(**params)\n\n    # field dic for whichStage\n    fieldDic = {'Nowcast': ['stationid','x','y','status','stage_ft','stage_time','datum_elevation','to_navd88'],\n                'Forecast': ['stationid','x','y','status','stage_ft','stage_time','datum_elevation','to_navd88'],\n                'Postcast': ['stationid','x','y','status','stage_ft','stage_time','datum_elevation','to_navd88'],\n                'Action': ['stationid','x','y','status','action_stage','stage_time','datum_elevation','to_navd88'],\n                'Flood': ['stationid','x','y','status','flood_stage','stage_time','datum_elevation','to_navd88'],\n                'Moderate': ['stationid','x','y','status','moderate_stage','stage_time','datum_elevation','to_navd88'],\n                'Major': ['stationid','x','y','status','major_stage','stage_time','datum_elevation','to_navd88'],\n                }\n    stageFieldDic = {'Nowcast': 'stage_ft',\n                'Forecast': 'stage_ft',\n                'Postcast': 'stage_ft',\n                'Action': 'action_stage',\n                'Flood': 'flood_stage',\n                'Moderate': 'moderate_stage',\n                'Major': 'major_stage'\n                }\n    # perform query\n    fields = fieldDic[whichStage]\n    fieldStr = ','.join(fields)\n    sqlQuery = f'SELECT {fieldStr} FROM {gaugeFcName}'\n    gauges = psql.read_sql(sqlQuery, conn) # or gauges = psql.read_sql_query('select * from fldpln_ks_gauges', conn)\n    # print(gauges)\n\n    # calculate stage elevation\n    # gauges['stage_elevation'] = gauges['stage_ft']+gauges['datum_elevation']+gauges['to_navd88']\n    gauges['stage_elevation'] = gauges[stageFieldDic[whichStage]]+gauges['datum_elevation']+gauges['to_navd88']\n\n    # only keep necessary fields\n    keptFields = ['stationid','x','y','stage_elevation','stage_time','status']\n    gauges = gauges[keptFields]\n    # print(gauges)\n\n    # remove stations with nan stage elevation (because of no datum elevation)\n    # gauges = gauges[gauges['stage_elevation'].isnull()]\n\n    return gauges\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetGaugeStageFromPostgres","title":"<code>GetGaugeStageFromPostgres(dbName, gaugeTableName, whichStage='Nowcast')</code>","text":"<p>Read gauge stage from a gauge table in PostgreSQL database. This function is OLD CODE in task_lib2map_gauge_table.py</p> <p>Parameters:</p> Name Type Description Default <code>dbName</code> <code>str</code> <p>database name.</p> required <code>gaugeTableName</code> <code>str</code> <p>gauge table name.</p> required <code>whichStage</code> <code>str</code> <p>Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major. Default to 'Nowcast'.</p> <code>'Nowcast'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time, status.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetGaugeStageFromPostgres(dbName, gaugeTableName, whichStage='Nowcast'):\n    \"\"\" Read gauge stage from a gauge table in PostgreSQL database. This function is OLD CODE in task_lib2map_gauge_table.py\n\n        Args:\n            dbName (str): database name.\n            gaugeTableName (str): gauge table name.\n            whichStage (str): Nowcast, Forecast, Postcast, and historical stages Action, Flood, Moderate, Major. Default to 'Nowcast'.\n\n        Return:\n            data frame: a data frame of gauge stage with the fields of stationid, x, y, stage_elevation, stage_time, status.\n    \"\"\"\n\n    import psycopg2 # MacOs cannot install the package using pip. So we put it only in the functions that use it and DO NOT include it in requirements.txt. It's user's responsibility to install it.\n\n    # connect to the db\n    conn = psycopg2.connect(\n        host=\"itprdkarsap.home.ku.edu\",\n        database=dbName,\n        user=\"sde\",\n        password=\"avhRR=landsat\") \n\n    # field dic for whichStage\n    fieldDic = {'Nowcast': ['stationid','x_utm14','y_utm14','status','stage_ft','stage_time','datum_elevation','to_navd88'],\n                'Forecast': ['stationid','x_utm14','y_utm14','status','stage_ft','stage_time','datum_elevation','to_navd88'],\n                'Postcast': ['stationid','x_utm14','y_utm14','status','stage_ft','stage_time','datum_elevation','to_navd88'],\n                'Action': ['stationid','x_utm14','y_utm14','action_stage','datum_elevation','to_navd88'],\n                'Flood': ['stationid','x_utm14','y_utm14','flood_stage','datum_elevation','to_navd88'],\n                'Moderate': ['stationid','x_utm14','y_utm14','moderate_stage','datum_elevation','to_navd88'],\n                'Major': ['stationid','x_utm14','y_utm14','major_stage','datum_elevation','to_navd88'],\n                }\n\n    # perform query\n    fields = fieldDic[whichStage]\n    fieldStr = ','.join(fields)\n    sqlQuery = f'SELECT {fieldStr} FROM {gaugeTableName}'\n    gauges = psql.read_sql(sqlQuery, conn) # or gauges = psql.read_sql_query('select * from fldpln_ks_gauges', conn)\n    # print(gauges)\n\n    # calculate stage elevation\n    gauges['stage_elevation'] = gauges['stage_ft']+gauges['datum_elevation']+gauges['to_navd88']\n\n    # only keep necessary fields\n    keptFields = ['stationid','x_utm14','y_utm14','stage_elevation','stage_time','status']\n    gauges = gauges[keptFields]\n    # print(gauges)\n\n    # remove stations with nan stage elevation (because of no datum elevation)\n    # gauges = gauges[gauges['stage_elevation'].isnull()]\n\n    return gauges\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetUsgsGaugeInfo","title":"<code>GetUsgsGaugeInfo(ids)</code>","text":"<p>Get USGS gauge information with a list of gauge IDs.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list</code> <p>a list of USGS gauge IDs.</p> required <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of USGS gauge information.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetUsgsGaugeInfo(ids):\n    \"\"\" Get USGS gauge information with a list of gauge IDs.\n\n        Args:\n            ids (list): a list of USGS gauge IDs.\n\n        Return:\n            data frame: a data frame of USGS gauge information.\n    \"\"\"\n\n    # USGS site/gauge table fields:\n    #  agency_cd       -- Agency\n    #  site_no         -- Site identification number\n    #  station_nm      -- Site name\n    #  site_tp_cd      -- Site type\n    #  dec_lat_va      -- Decimal latitude\n    #  dec_long_va     -- Decimal longitude\n    #  coord_acy_cd    -- Latitude-longitude accuracy\n    #  dec_coord_datum_cd -- Decimal Latitude-longitude datum\n    #  alt_va          -- Altitude of gauge/land surface\n    #  alt_acy_va      -- Altitude accuracy\n    #  alt_datum_cd    -- Altitude datum\n    #  huc_cd          -- Hydrologic unit code\n    #\n    # # read in USGS gauges from Excel file which is manually generated from USGS gauge web site\n    # # See Chapter 5 in book Flood Mapping in Kansas.\n    # # This can be automated too!\n    # gaugeExcelFile = 'usgs_gauges_ks_nearby.xlsx'\n    # sheetName = 'usgs_gauges_ks_nearby'\n    # gdf = pd.read_excel(gaugeExcelFile, sheet_name=sheetName,dtype={'site_no':str,'huc_cd':str})\n    # # print('USGS gauges from web:',gdf)\n\n    # USGS Site Web Service\n    usgsSiteServiceUrl = 'http://waterservices.usgs.gov/nwis/site'\n\n    # prepare a query URL to retrieve active lake and stream USGS gauges with instantaneous values\n    # usgsSiteServiceUrl = https://waterservices.usgs.gov/nwis/site/?format=rdb&amp;sites=01646500,06891478&amp;siteStatus=all\n    # prepare parameters\n    idstr = \",\".join(ids)\n    params = {'format':'rdb', 'sites':idstr}\n\n    # Send the request with the get method\n    response = requests.get(usgsSiteServiceUrl, params=params, verify=False)\n    # print(response.request.url)\n\n    # turn content into list of lines\n    contentLst = response.text.split('\\n')\n\n    # find the number of lines of the header in the RDB file\n    numOfHeaderLn = 0\n    for ln in contentLst:\n        if ln[0] == '#':\n            numOfHeaderLn += 1\n        else:\n            break\n\n    # get field names\n    fieldNames = contentLst[numOfHeaderLn].split('\\t')\n    # print(fieldNames)\n\n    # turn each line into a list\n    gLst = [row.split('\\t') for row in contentLst[numOfHeaderLn+2:-1]]\n    # convert lat, longitude, and datum elevation to floats\n    for g in gLst:\n        g[4] = float(g[4])\n        g[5] = float(g[5])\n        if g[8] != '':\n            g[8] = float(g[8])\n        else:\n            g[8] = None\n        if g[9] != '':\n            g[9] = float(g[9])\n        else:\n            g[9] = None\n\n    # create a df from the list\n    gInfo = pd.DataFrame(gLst, columns=fieldNames)\n    # print(gInfo)\n\n    return gInfo\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetUsgsGaugeStageFromWebService","title":"<code>GetUsgsGaugeStageFromWebService(usgsIds, startDate='Now', endDate='MostRecent')</code>","text":"<p>Get USGS gauge stage from web service. USGS Instantaneous Values Service URL: https://waterservices.usgs.gov/rest/IV-Test-Tool.html</p> <p>Parameters:</p> Name Type Description Default <code>usgsIds</code> <code>list</code> <p>a list of USGS IDs.</p> required <code>startDate</code> <code>str</code> <p>start date of the query. Default to 'Now'.</p> <code>'Now'</code> <code>endDate</code> <code>str</code> <p>end date of the query. Default to 'MostRecent'.</p> <code>'MostRecent'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of USGS gauge stage.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetUsgsGaugeStageFromWebService(usgsIds, startDate='Now', endDate='MostRecent'):\n    \"\"\" Get USGS gauge stage from web service. USGS Instantaneous Values Service URL: https://waterservices.usgs.gov/rest/IV-Test-Tool.html\n\n        Args:\n            usgsIds (list): a list of USGS IDs.\n            startDate (str): start date of the query. Default to 'Now'.\n            endDate (str): end date of the query. Default to 'MostRecent'.\n\n        Return:\n            data frame: a data frame of USGS gauge stage.\n    \"\"\"\n\n    # base URL\n    ivUrl = r'https://waterservices.usgs.gov/nwis/iv'\n    # For stream gauges:\n    #   Most recent: /?format=json&amp;indent=on&amp;sites={}&amp;parameterCd=00065&amp;siteStatus=all'\n    #   From a period from now: /?format=json&amp;indent=on&amp;sites={}&amp;period=P7D&amp;parameterCd=00065&amp;siteStatus=all'\n    #   Between two dates: /?format=json&amp;indent=on&amp;sites={}&amp;startDT=2018-09-02&amp;endDT=2018-09-04&amp;parameterCd=00065&amp;siteStatus=all'\n    # For lake gauges:\n    #   parameterCd=62614, Lake or reservoir water surface elevation above NGVD 1929, feet \n\n    # prepare FIXed parameters for stream and lake gauges\n    if startDate == 'Now':\n        if endDate == 'MostRecent':\n            # most recent stage\n            paramsSt = {'format':'json', 'parameterCd':'00065','siteStatus':'all'}\n            paramsLk = {'format':'json', 'parameterCd':'62614','siteStatus':'all'}\n        else:\n            # from Now to endDate (an integer!)\n            periodPara = f'P{endDate}D'\n            paramsSt = {'format':'json', 'period':periodPara, 'parameterCd':'00065','siteStatus':'all'} \n            paramsLk = {'format':'json', 'period':periodPara, 'parameterCd':'62614','siteStatus':'all'}\n    else:\n        # between startDate and endDate\n        paramsSt = {'format':'json', 'startDT':startDate, 'endDT':endDate, 'parameterCd':'00065','siteStatus':'all'}\n        paramsLk = {'format':'json', 'startDT':startDate, 'endDT':endDate, 'parameterCd':'62614','siteStatus':'all'}\n\n    # get gauge type: ST or LK\n    gt = GetUsgsGaugeInfo(usgsIds)['site_tp_cd'].to_list()\n\n    # split gauges into STREAM and LAKE groups\n    stIds=[]; lkIds=[]\n    for t, id in zip(gt, usgsIds):\n        if t == 'ST':\n            stIds.append(id)\n        if t == 'LK':\n            lkIds.append(id)\n\n    # concatenate ids as strings\n    stIdStr = \",\".join(stIds)\n    lkIdStr = \",\".join(lkIds)\n\n    # prepare query for stream and lake gauges\n    paramsSt.update({'sites':stIdStr})\n    paramsLk.update({'sites':lkIdStr})\n\n    # print('querying the USGS web service')\n    cols = ['stationid','stage_ft', 'stage_time']\n    df =pd.DataFrame(columns=cols)\n    for params in [paramsSt, paramsLk]:\n        if params['sites'] != '': # only query web service if there are sites!\n            response = requests.get(ivUrl, params=params, verify=False)\n            # print(response.request.url)\n            response = response.json()['value']['timeSeries']\n            # print(response)\n\n            # extracting information from the web service json response\n            for row in response:\n                # site_name = row['sourceInfo']['siteName']\n                site_code = row['sourceInfo']['siteCode'][0]['value']\n                siteValues = row['values']\n                for v in row['values']:\n                    for v2 in v['value']:\n                        gauge_height = v2['value'] # assume each gauge only has one value. But some gauge (for example, 06891080, KANSAS R AT LAWRENCE, KS) has TWO values, one above the dam and one below the dam\n                        obs_time     = v2['dateTime']\n                        # add the nearest point to the nearest point DF\n                        t = pd.DataFrame([[site_code, gauge_height,obs_time]], columns=cols)\n                        # df = df.append(t,ignore_index=False)\n                        df = pd.concat([df, t],ignore_index=False)\n\n    # turn field \"stage_ft\" into numeric\n    df['stage_ft'] = pd.to_numeric(df['stage_ft'], errors='coerce')\n\n    # reset index\n    df.reset_index(drop=True,inplace=True)\n\n    return df\n</code></pre>"},{"location":"gauge/#fldpln.gauge.GetUsgsGauges","title":"<code>GetUsgsGauges(geobox, epsg=32614)</code>","text":"<p>Get USGS gauges within a box and project them to a specified coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>list</code> <p>a list of geographic box elements of [minX,minY,maxX,maxY].</p> required <code>epsg</code> <code>int</code> <p>EPSG integer representing the projected coordinate system, default to UTM14 (epsg = 32614) for Kansas. </p> <code>32614</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a geo data frame of USGS gauges projected to the specified coordinate system.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def GetUsgsGauges(geobox, epsg=32614):\n    \"\"\" Get USGS gauges within a box and project them to a specified coordinate system.\n\n        Args:\n            geobox (list): a list of geographic box elements of [minX,minY,maxX,maxY].\n            epsg (int): EPSG integer representing the projected coordinate system, default to UTM14 (epsg = 32614) for Kansas. \n\n        Return:\n           data frame: a geo data frame of USGS gauges projected to the specified coordinate system.\n    \"\"\"\n\n    # USGS site/gauge table fields:\n    #  agency_cd       -- Agency\n    #  site_no         -- Site identification number\n    #  station_nm      -- Site name\n    #  site_tp_cd      -- Site type\n    #  dec_lat_va      -- Decimal latitude\n    #  dec_long_va     -- Decimal longitude\n    #  coord_acy_cd    -- Latitude-longitude accuracy\n    #  dec_coord_datum_cd -- Decimal Latitude-longitude datum\n    #  alt_va          -- Altitude of gauge/land surface\n    #  alt_acy_va      -- Altitude accuracy\n    #  alt_datum_cd    -- Altitude datum\n    #  huc_cd          -- Hydrologic unit code\n    #\n    # # read in USGS gauges from Excel file which is manually generated from USGS gauge web site\n    # # See Chapter 5 in book Flood Mapping in Kansas.\n    # # This can be automated too!\n    # gaugeExcelFile = 'usgs_gauges_ks_nearby.xlsx'\n    # sheetName = 'usgs_gauges_ks_nearby'\n    # gdf = pd.read_excel(gaugeExcelFile, sheet_name=sheetName,dtype={'site_no':str,'huc_cd':str})\n    # # print('USGS gauges from web:',gdf)\n\n    # USGS Site Web Service\n    usgsSiteServiceUrl = 'http://waterservices.usgs.gov/nwis/site'\n\n    # prepare a query URL to retrieve active lake and stream USGS gauges with instantaneous values\n    # usgsSiteServiceUrl = 'http://waterservices.usgs.gov/nwis/site/?format=rdb,1.0&amp;bBox=-99.610000,36.810000,-94.200000,40.250000&amp;siteType=LK,ST&amp;siteStatus=active&amp;hasDataTypeCd=iv'\n    # parameters\n    bBox = ','.join([str(c) for c in geobox])\n    params = {'format':'rdb', 'bBox':bBox, 'siteType': 'LK,ST', 'siteStatus':'active', 'hasDataTypeCd': 'iv'}\n\n    # Send the request with the get method\n    response = requests.get(usgsSiteServiceUrl, params=params, verify=False)\n    print(response.request.url)\n\n    # turn content into list of lines\n    contentLst = response.text.split('\\n')\n\n    # find the number of lines of the header in the RDB file\n    numOfHeaderLn = 0\n    for ln in contentLst:\n        if ln[0] == '#':\n            numOfHeaderLn += 1\n        else:\n            break\n\n    # get field names\n    fieldNames = contentLst[numOfHeaderLn].split('\\t')\n    # print(fieldNames)\n\n    # turn each line into a list\n    gLst = [row.split('\\t') for row in contentLst[numOfHeaderLn+2:-1]]\n    # convert lat, longitude, and datum elevation to floats\n    for g in gLst:\n        g[4] = float(g[4])\n        g[5] = float(g[5])\n        if g[8] != '':\n            g[8] = float(g[8])\n        else:\n            g[8] = None\n        if g[9] != '':\n            g[9] = float(g[9])\n        else:\n            g[9] = None\n\n    # create a df from the list\n    gdf = pd.DataFrame(gLst, columns=fieldNames)\n\n    # turn pd into gpd using gauge's latitude and longitude coordinates\n    ggdf = gpd.GeoDataFrame(gdf, geometry=gpd.points_from_xy(gdf.dec_long_va, gdf.dec_lat_va))\n\n    # Project gauge location \n    # define gauge CRS, i.e., GCS on NAD83, assuming all USGS gauges are based on NAD83\n    ggdf = ggdf.set_crs(epsg=4326)\n    # project ggdf to library coordinate system, i.e., UTM 14N\n    ggdf = ggdf.to_crs(epsg=epsg)\n    # save the projected shapefile\n    # ggdf.to_file(shpFullName)\n\n    # add gauge's coordinates as fields\n    ggdf['x'] = ggdf['geometry'].x\n    ggdf['y'] = ggdf['geometry'].y\n\n    # USGS gauges in UTM 14N\n    gdf = ggdf.drop(columns=['geometry'])\n    # print('Gauges with UTM 14N coordinates:',gdf)\n    # print(f'Total gauges: {len(gdf)}')\n\n    return gdf\n</code></pre>"},{"location":"gauge/#fldpln.gauge.MergeUsgsAhpsGauges","title":"<code>MergeUsgsAhpsGauges(usgsGauges, ahpsGauges, nearDist=350)</code>","text":"<p>Merge USGS and AHPS gauges based on the nearest neighbor that is within a specified distance.</p> <p>Parameters:</p> Name Type Description Default <code>usgsGauges</code> <code>data frame</code> <p>USGS gauges data frame.</p> required <code>ahpsGauges</code> <code>data frame</code> <p>AHPS gauges data frame.</p> required <code>nearDist</code> <code>float</code> <p>distance threshold for finding the nearest gauge, default to 350 meters.</p> <code>350</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of merged USGS and AHPS gauges.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def MergeUsgsAhpsGauges(usgsGauges, ahpsGauges, nearDist=350):\n    \"\"\" Merge USGS and AHPS gauges based on the nearest neighbor that is within a specified distance.\n\n        Args:\n            usgsGauges (data frame): USGS gauges data frame.\n            ahpsGauges (data frame): AHPS gauges data frame.\n            nearDist (float): distance threshold for finding the nearest gauge, default to 350 meters.\n\n        Return:\n            data frame: a data frame of merged USGS and AHPS gauges.\n    \"\"\"\n\n    distFieldName='dist'\n    # find the nearest AHPS gauge for each USGS gauge. Note that multiple USGS gauges have the same nearest AHPS gauge!\n    usgsNearGauges = NearestPoint(usgsGauges, 'x', 'y', ahpsGauges, 'x', 'y', distFieldName=distFieldName,otherColumns=['GaugeLID'])\n    usgsNearGauges = usgsNearGauges[usgsNearGauges[distFieldName]&lt;=nearDist][['GaugeLID','site_no']]\n    print(f'USGS nearest AHPS gauges: {len(usgsNearGauges)}')\n\n    # find the nearest USGS gauge for each AHPS gauge. Note that multiple AHPS gauges have the same nearest USGS gauge!\n    ahpsNearGauges = NearestPoint(ahpsGauges, 'x', 'y', usgsGauges, 'x', 'y', distFieldName=distFieldName, otherColumns=['site_no'])\n    ahpsNearGauges = ahpsNearGauges[ahpsNearGauges[distFieldName]&lt;=nearDist][['GaugeLID','site_no']]\n    print(f'AHPS nearest USGS gauges: {len(ahpsNearGauges)}')\n\n    # find the USGS and AHPS gauges that are nearest gauge to each other\n    # There is ONE AHPS gauge which is the nearest gauge of two USGS gauges\n    commGaugeIds = pd.merge(ahpsNearGauges,usgsNearGauges,how='inner',on=['GaugeLID','site_no'])\n\n    # # clean up the original gauges. This is necessary when using NearestPointInPlace()\n    # usgsGauges.drop(columns=['GaugeLID',distFieldName],inplace=True)\n    # ahpsGauges.drop(columns=['site_no',distFieldName],inplace=True)\n\n    # get the common gauges from USGS gauges\n    mGauges = pd.merge(usgsGauges,commGaugeIds,how='left',on='site_no')\n    commGauges = mGauges[~mGauges['GaugeLID'].isna()]\n\n    # get just USGS gauges\n    justUsgsGauges = mGauges[mGauges['GaugeLID'].isna()]\n\n    # just AHPS gauges\n    justAhpsGauges = pd.merge(ahpsGauges,commGaugeIds,how='left',on='GaugeLID')\n    justAhpsGauges = justAhpsGauges[justAhpsGauges['site_no'].isna()]\n    print(f'Common: {len(commGauges)}, Just USGS: {len(justUsgsGauges)}, Just AHPS: {len(justAhpsGauges)}')\n    # print(justAhpsGauges.columns)\n\n    #\n    # Standardize gauges\n    #\n    # USGS site/gauge table fields:\n        #  agency_cd       -- Agency\n        #  site_no         -- Site identification number\n        #  station_nm      -- Site name\n        #  site_tp_cd      -- Site type\n        #  dec_lat_va      -- Decimal latitude\n        #  dec_long_va     -- Decimal longitude\n        #  coord_acy_cd    -- Latitude-longitude accuracy\n        #  dec_coord_datum_cd -- Decimal Latitude-longitude datum\n        #  alt_va          -- Altitude of gauge/land surface\n        #  alt_acy_va      -- Altitude accuracy\n        #  alt_datum_cd    -- Altitude datum\n    # AHPS fields:\n        # 'GaugeLID', 'count', 'Status', 'Location', 'Latitude', 'Longitude',\n        #    'Waterbody', 'State', 'Observed', 'ObsTime', 'Units', 'Action', 'Flood',\n        #    'Moderate', 'Major', 'LowThresh', 'LowThreshU', 'WFO', 'HDatum',\n        #    'PEDTS', 'SecValue', 'SecUnit', 'URL'\n\n    # create an empty DF\n    fields=['stationid','name','organization','stype','stationurl','graphurl',\n                'action_stage','flood_stage','moderate_stage','major_stage',\n                'datum_elevation','vdatum','to_navd88',\n                'latitude','longitude','hdatum','x','y']\n    gauges = pd.DataFrame(columns=fields)\n\n    # add common gauges\n    for row in commGauges.itertuples():  \n        stationid,name,organization,stype,datum_elevation,vdatum,latitude,longitude,hdatum,x,y = (row.site_no,row.station_nm,row.agency_cd,row.site_tp_cd,row.alt_va,row.alt_datum_cd,row.dec_lat_va,row.dec_long_va,row.dec_coord_datum_cd,row.x,row.y)\n        to_navd88 = np.nan\n\n        # get values from the AHPS gauge\n        r = ahpsGauges.loc[ahpsGauges.GaugeLID==row.GaugeLID]\n        # turn the row DF into a list of objects\n        r = r.to_records(index=False)[0]\n\n        # station URL uses AHPS instead of USGS as AHPS station may have vertical datum information when USGS station URL doesn't\n        stationurl = f'http://water.weather.gov/ahps2/hydrograph.php?wfo={r.WFO}&amp;gage={r.GaugeLID}' # station URL from AHPS\n        # stationurl = 'https://waterdata.usgs.gov/nwis/inventory/?site_no='+str(stationid) # USGS station URL\n\n        # add datum elevation from AHPS station HTML page\n        if math.isnan(datum_elevation):\n            datumName, datumEle = GetAhpsGaugeDatumElevation(stationurl)\n            datum_elevation, vdatum = (datumEle,datumName)\n\n        # other fields from AHPS\n        action_stage,flood_stage,moderate_stage,major_stage = (r.Action, r.Flood, r.Moderate, r.Major)\n        graphurl = f'http://water.weather.gov/resources/hydrographs/{r.GaugeLID.lower()}_hg.png'\n\n        # create a new row\n        nr = {'stationid':stationid+','+r.GaugeLID,'name':name,'organization':organization+','+'AHPS','stype':stype,'stationurl':stationurl,'graphurl':graphurl,\n            'action_stage':action_stage,'flood_stage':flood_stage,'moderate_stage':moderate_stage,'major_stage':major_stage,\n            'datum_elevation':datum_elevation,'vdatum':vdatum,'to_navd88':to_navd88,\n            'latitude':latitude,'longitude':longitude,'hdatum':hdatum,'x':x,'y':y}\n        # append\n        # gauges = gauges.append(nr,ignore_index=True)\n        gauges = pd.concat([gauges, nr],ignore_index=True)\n    # print(gauges.columns)\n    # print(gauges)\n\n    # add just USGS gauges\n    for row in justUsgsGauges.itertuples(): \n        stationid,name,organization,stype,datum_elevation,vdatum,latitude,longitude,hdatum,x,y = (row.site_no,row.station_nm,row.agency_cd,row.site_tp_cd,row.alt_va,row.alt_datum_cd,row.dec_lat_va,row.dec_long_va,row.dec_coord_datum_cd,row.x,row.y)\n        stationurl = 'https://waterdata.usgs.gov/nwis/inventory/?site_no='+str(stationid)\n        graphurl = f'http://waterdata.usgs.gov/nwisweb/graph?site_no={stationid}&amp;parm_cd=00065&amp;period=7'\n        to_navd88 = np.nan\n\n        # nothing from AHPS\n        action_stage,flood_stage,moderate_stage,major_stage = (np.nan,np.nan,np.nan,np.nan)\n\n        # create a new row\n        nr = {'stationid':stationid,'name':name,'organization':organization,'stype':stype,'stationurl':stationurl,'graphurl':graphurl,\n            'action_stage':action_stage,'flood_stage':flood_stage,'moderate_stage':moderate_stage,'major_stage':major_stage,\n            'datum_elevation':datum_elevation,'vdatum':vdatum,'to_navd88':to_navd88,\n            'latitude':latitude,'longitude':longitude,'hdatum':hdatum,'x':x,'y':y}\n        # append\n        # gauges = gauges.append(nr,ignore_index=True)\n        gauges = pd.concat([gauges, nr],ignore_index=True)\n    # print(gauges)\n\n    # add just AHPS gauges\n    for row in justAhpsGauges.itertuples(): \n        # nothing from USGS gauges\n\n        # get values from the AHPS gauge\n        r = ahpsGauges.loc[ahpsGauges.GaugeLID==row.GaugeLID]\n        # turn the row DF into a list of objects\n        r = r.to_records(index=False)[0]\n\n        # decide gauge type\n        if (' Lake' in r.Location) or (' Reservoir' in r.Location):\n            stype = 'LK'\n        else:\n            stype = 'ST'\n\n        # add datum elevation from AHPS station HTML page\n        stationurl = f'http://water.weather.gov/ahps2/hydrograph.php?wfo={r.WFO}&amp;gage={r.GaugeLID}'\n        datumName, datumEle = GetAhpsGaugeDatumElevation(stationurl)\n        datum_elevation, vdatum = (datumEle,datumName)\n        to_navd88 = np.nan\n\n        # other attributes\n        stationid,name,organization,latitude,longitude,hdatum,x,y = (r.GaugeLID,r.Location+', '+r.State,'AHPS',r.Latitude, r.Longitude,r.HDatum,r.x,r.y)\n        action_stage,flood_stage,moderate_stage,major_stage = (r.Action, r.Flood, r.Moderate, r.Major)\n        graphurl = f'http://water.weather.gov/resources/hydrographs/{r.GaugeLID.lower()}_hg.png'\n\n        # create a new row\n        nr = {'stationid':stationid,'name':name,'organization':organization,'stype':stype,'stationurl':stationurl,'graphurl':graphurl,\n            'action_stage':action_stage,'flood_stage':flood_stage,'moderate_stage':moderate_stage,'major_stage':major_stage,\n            'datum_elevation':datum_elevation,'vdatum':vdatum,'to_navd88':to_navd88,\n            'latitude':latitude,'longitude':longitude,'hdatum':hdatum,'x':x,'y':y}\n        # append\n        # gauges = gauges.append(nr,ignore_index=True)\n        gauges = pd.concat([gauges, nr],ignore_index=True)\n\n    return gauges\n</code></pre>"},{"location":"gauge/#fldpln.gauge.NGVD29ToNAVD88OrthoHeightAdjustment","title":"<code>NGVD29ToNAVD88OrthoHeightAdjustment(lat, lon, inDatum, outDatum, inVertDatum='NGVD29', outVertDatum='NAVD88', orthoHt=0.0)</code>","text":"<p>Calculate vertical datum shift between NGVD29 and NAVD88 using NGS web service.  NGS Latitude-longitude-height Service: https://www.ngs.noaa.gov/web_services/ncat/lat-long-height-service.shtml</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>latitude of the gauge.</p> required <code>lon</code> <code>float</code> <p>longitude of the gauge.</p> required <code>inDatum</code> <code>str</code> <p>input datum name.</p> required <code>outDatum</code> <code>str</code> <p>output datum name.</p> required <code>inVertDatum</code> <code>str</code> <p>input vertical datum name, default to NGVD29.</p> <code>'NGVD29'</code> <code>outVertDatum</code> <code>str</code> <p>output vertical datum name, default to NAVD88.</p> <code>'NAVD88'</code> <code>orthoHt</code> <code>float</code> <p>orthometric height, default to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>float</code> <p>vertical datum shift from NGVD29 to NAVD88.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def NGVD29ToNAVD88OrthoHeightAdjustment(lat,lon,inDatum,outDatum,inVertDatum='NGVD29',outVertDatum='NAVD88',orthoHt=0.0):\n    \"\"\" Calculate vertical datum shift between NGVD29 and NAVD88 using NGS web service. \n        NGS Latitude-longitude-height Service: https://www.ngs.noaa.gov/web_services/ncat/lat-long-height-service.shtml\n\n        Args:\n            lat (float): latitude of the gauge.\n            lon (float): longitude of the gauge.\n            inDatum (str): input datum name.\n            outDatum (str): output datum name.\n            inVertDatum (str): input vertical datum name, default to NGVD29.\n            outVertDatum (str): output vertical datum name, default to NAVD88.\n            orthoHt (float): orthometric height, default to 0.0.\n\n        Return:\n            float: vertical datum shift from NGVD29 to NAVD88.\n    \"\"\"\n\n    # ngsUrl = f'https://geodesy.noaa.gov/api/ncat/llh?lat={lat}&amp;lon={lon}&amp;inDatum={inDatum}&amp;outDatum={outDatum}&amp;inVertDatum={inVertDatum}&amp;outVertDatum={outVertDatum}&amp;orthoHt={orthoHt}'\n    ngsUrl = 'https://geodesy.noaa.gov/api/ncat/llh'\n    payload = {'lat':lat,'lon': lon, 'inDatum':inDatum,'outDatum':outDatum, 'inVertDatum':inVertDatum,'outVertDatum':outVertDatum,'orthoHt':orthoHt}\n\n    # request the html page\n    response = requests.get(ngsUrl, params=payload)\n    result = response.json()\n    destOrthoht = result['destOrthoht']\n\n    # convert meter to US Survey ft\n    m2usft = 3.2808333333\n    destOrthoht = float(destOrthoht) * m2usft\n\n    return destOrthoht\n</code></pre>"},{"location":"gauge/#fldpln.gauge.PrepareAhpsGaugeDatum","title":"<code>PrepareAhpsGaugeDatum(scratchFolder, libFolder, prjFileName, datumFile)</code>","text":"<p>Prepare AHPS gauge datum.</p> <p>Parameters:</p> Name Type Description Default <code>scratchFolder</code> <code>str</code> <p>scratch folder to store downloaded files.</p> required <code>libFolder</code> <code>str</code> <p>library folder to store files.</p> required <code>prjFileName</code> <code>str</code> <p>file name of projection information.</p> required <code>datumFile</code> <code>str</code> <p>file name of gauge datum information.</p> required <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame of AHPS gauges.</p> Source code in <code>fldpln/gauge.py</code> <pre><code>def PrepareAhpsGaugeDatum(scratchFolder,libFolder,prjFileName,datumFile):\n    \"\"\" Prepare AHPS gauge datum.\n\n        Args:\n            scratchFolder (str): scratch folder to store downloaded files.\n            libFolder (str): library folder to store files.\n            prjFileName (str): file name of projection information.\n            datumFile (str): file name of gauge datum information.\n\n        Return:\n            data frame: a data frame of AHPS gauges.\n    \"\"\"\n\n    from osgeo import ogr # ogr cannot be installed using pip. So we put it only in the functions that use it and DO NOT include it in requirements.txt. It's user's responsibility to install it.\n\n    print('Downloading AHPS gauge shapefile ...')\n\n    # Clean out existing obs file\n    if os.path.isfile(os.path.join(scratchFolder,'ahps_download.tgz')):\n        os.remove(os.path.join(scratchFolder,'ahps_download.tgz'))\n\n    # delete the shapefile if existing\n    shpFullName = os.path.join(scratchFolder,'national_shapefile_obs.shp')\n    shpDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n    if os.path.exists(shpFullName):\n        shpDriver.DeleteDataSource(shpFullName)\n\n    # download current gauge observations\n    ahpsUrl = r\"https://water.weather.gov/ahps/download.php?data=tgz_obs\"\n    ahpsDownload = requests.get(ahpsUrl)\n    # save the download in a file\n    zippedFile = os.path.join(scratchFolder, \"ahps_download.tgz\")\n    with open(zippedFile, \"wb\") as f:\n        f.write(ahpsDownload.content)\n\n    # unzipping the tgz and tar\n    with tarfile.open(zippedFile, 'r:gz') as tf:\n        tf.extractall(path=scratchFolder)\n\n    # read AHPS shapefile\n    gauges = gpd.read_file(shpFullName)\n\n    print('Project the shapefile to library coordinate system ...')\n    # read in spatial reference of the library\n    srFile = os.path.join(libFolder,prjFileName)\n    with open(srFile, 'r') as srf:\n        srText = srf.read()\n    # set geodataframe CRS\n    libSr = CRS.from_wkt(srText)\n\n    # project gauges to library coordinate system\n    gauges = gauges.to_crs(libSr)\n    # save the projected shapefile\n    # gauges.to_file(shpFullName)\n\n    # add gauge's coordinates as fields\n    gauges['X'] = gauges['geometry'].x\n    gauges['Y'] = gauges['geometry'].y\n\n    # select cols\n    cols = ['GaugeLID','Status','Location','Latitude','Longitude','Waterbody','State','WFO','HDatum','URL','Units','Action','Flood','Moderate','Major','X','Y']\n    gauges = gauges[cols]\n    # Add datum col\n    gauges['Datum'] = np.nan\n\n    print('Save gauge datum file ...')\n    # Save the file\n    # datumFile = os.path.join(scratchFolder,datumFileName)\n    gauges.to_csv(datumFile, index=False)\n\n    return gauges\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install fldpln, run this command in your terminal:</p> <pre><code>pip install fldpln\n</code></pre> <p>This is the preferred method to install fldpln, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install fldpln from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/xingongli/fldpln\n</code></pre>"},{"location":"mapping/","title":"mapping module","text":"<p>Module for mapping tile-based library.</p>"},{"location":"mapping/#fldpln.mapping.CreateFolders","title":"<code>CreateFolders(outFolder, scratchFolderName='scratch', outMapFolderName='maps', removeExist=True)</code>","text":"<p>Create folders for storing temporary files and output maps.</p> <p>Parameters:</p> Name Type Description Default <code>outFolder</code> <code>str</code> <p>output folder</p> required <code>scratchFolderName</code> <code>str</code> <p>name of the folder for storing temporary files</p> <code>'scratch'</code> <code>outMapFolderName</code> <code>str</code> <p>name of the folder for storing output maps, default is 'maps'</p> <code>'maps'</code> <code>removeExist</code> <code>str</code> <p>bool whether to remove existing folders, default is True</p> <code>True</code> Source code in <code>fldpln/mapping.py</code> <pre><code>def CreateFolders(outFolder,scratchFolderName='scratch',outMapFolderName='maps',removeExist=True):\n    \"\"\" Create folders for storing temporary files and output maps.\n\n        Args:\n            outFolder (str): output folder\n            scratchFolderName (str): name of the folder for storing temporary files\n            outMapFolderName (str): name of the folder for storing output maps, default is 'maps'\n            removeExist (str): bool whether to remove existing folders, default is True\n\n        Return: \n            tuple: folder for output maps, folder for temporary files.\n    \"\"\"\n    # create output folder if it doesn't exist\n    os.makedirs(outFolder, exist_ok=True)\n\n    # scratch folder\n    scratchFolder = os.path.join(outFolder, scratchFolderName)\n    # map output folder\n    outMapFolder = os.path.join(outFolder, outMapFolderName)\n\n    # Create the folders for storing temp and output files\n    if removeExist:\n        if os.path.isdir(scratchFolder): shutil.rmtree(scratchFolder)\n        if os.path.isdir(outMapFolder): shutil.rmtree(outMapFolder)\n\n    os.makedirs(scratchFolder, exist_ok=True)\n    os.makedirs(outMapFolder, exist_ok=True)\n\n    return outMapFolder,scratchFolder\n</code></pre>"},{"location":"mapping/#fldpln.mapping.DownloadTiledLibrary","title":"<code>DownloadTiledLibrary(libUrl, libName, localLibFolder)</code>","text":"<p>Download and unzip tiled libraries.</p> <p>Parameters:</p> Name Type Description Default <code>libUrl</code> <code>str</code> <p>the url of the library</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>localLibFolder</code> <code>str</code> <p>the folder where the library will be saved</p> required <p>Returns:</p> Type Description <code>None</code> <p>no return.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def DownloadTiledLibrary(libUrl,libName,localLibFolder ):\n    \"\"\" Download and unzip tiled libraries.\n\n        Args:\n            libUrl (str): the url of the library\n            libName (str): the name of the library\n            localLibFolder (str): the folder where the library will be saved\n\n        Return:\n            None: no return.\n    \"\"\"\n\n    # create local folder if not existing\n    os.makedirs(localLibFolder,exist_ok=True)\n\n    # If you need to redownload for whatever reason\n    if os.path.exists(os.path.join(localLibFolder, libName)):\n        shutil.rmtree(os.path.join(localLibFolder, libName))\n    os.mkdir(os.path.join(localLibFolder, libName))\n\n    # Download base library\n    print(f'Downloading library {libName} ...')\n    urllib.request.urlretrieve(libUrl+'/'+libName+'.zip',os.path.join(localLibFolder, libName+'.zip'))\n\n    # unzip library\n    print(f'Unzip library {libName} ...')\n    with zipfile.ZipFile(os.path.join(localLibFolder,libName+'.zip'),'r') as zip_ref:\n        zip_ref.extractall(os.path.join(localLibFolder, libName))\n\n    # clean up folder\n    os.remove(os.path.join(localLibFolder, libName+'.zip'))\n    print(\"Done for \"+libName)\n    return\n</code></pre>"},{"location":"mapping/#fldpln.mapping.EstimateFspDofFromGauge","title":"<code>EstimateFspDofFromGauge(libFolder, libName, gaugeFspDf, minGaugeDof=0.0328084, weightingType='V')</code>","text":"<p>Estimate/interpolate FSP DOF (Depth of Flow, i.e., FSP stage) from observed gauge DOFs using  distance-(horizontal) or elevation-based (vertical) linear interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are located</p> required <code>libName</code> <code>str</code> <p>the name of the library that the gauges will be snapped to</p> required <code>gaugeFspDf</code> <code>data frame</code> <p>a data frame of gauge FSPs (i.e., FSPs to which gauges are snapped). It should have at least 4 columns ['stage_elevation','lib_name','FspX','FspY'].</p> required <code>minGaugeDof</code> <code>float</code> <p>min DOF a gauge should have, by default is 1 cm = 0.0328083989501312 foot</p> <code>0.0328084</code> <code>weightingType</code> <code>str</code> <p>'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'</p> <code>'V'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with interpolated FSP DOFs.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def EstimateFspDofFromGauge(libFolder,libName,gaugeFspDf,minGaugeDof=0.0328084,weightingType='V'):\n    \"\"\" Estimate/interpolate FSP DOF (Depth of Flow, i.e., FSP stage) from observed gauge DOFs using \n        distance-(horizontal) or elevation-based (vertical) linear interpolation.\n\n        Args:\n            libFolder (str): the folder where the libraries are located\n            libName (str): the name of the library that the gauges will be snapped to\n            gaugeFspDf (data frame): a data frame of gauge FSPs (i.e., FSPs to which gauges are snapped). It should have at least 4 columns ['stage_elevation','lib_name','FspX','FspY'].\n            minGaugeDof (float): min DOF a gauge should have, by default is 1 cm = 0.0328083989501312 foot\n            weightingType (str): 'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'\n\n        Return:\n            data frame: a data frame with interpolated FSP DOFs.\n    \"\"\"\n\n    # select the gauge FSPs in the library\n    gaugeFspDf = gaugeFspDf[gaugeFspDf['lib_name']==libName]\n\n    # read in FSP and stream order network files\n    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n    strOrdFile = os.path.join(libFolder, libName, strOrdNetFileName)\n    fspDf = pd.read_csv(fspFile) \n    strOrdDf = pd.read_csv(strOrdFile) \n\n    # Get gauge stream order for interpolation by stream orders\n    gaugeFspDf = pd.merge(gaugeFspDf,fspDf,how='inner',on=['FspX','FspY'])[['FspX','FspY','StrOrd','DsDist','SegId','FilledElev','stage_elevation']]\n    # print(gaugeFspDf)\n\n    # calculate gauge FSP's DOF\n    gaugeFspDf['Dof'] = gaugeFspDf['stage_elevation'] - gaugeFspDf['FilledElev']\n    # reset gauge FSP DOF, if it's &lt; 0 or nodata, to minGaugeDof\n    gaugeFspDf.loc[(gaugeFspDf['Dof']&lt;=minGaugeDof)|gaugeFspDf['Dof'].isna(),['Dof']] = minGaugeDof\n    # print(gaugeFspDf)\n\n    # create an empty DF to store interpolated FSP DOFs\n    fspDof = pd.DataFrame(columns=['FspId','FspX','FspY','DsDist','FilledElev','Dof'])\n    # interpolate DoF for each stream order from low to high (low order means high priority!)\n    # get the stream orders with gauges on them\n    strOrds = gaugeFspDf['StrOrd'].drop_duplicates().sort_values().tolist()\n    if len(strOrds)==0:\n        print('No stream found for the gauges!')\n        return None\n\n    for ord in strOrds:\n        # gauges on the stream order\n        gaugeOrd = gaugeFspDf[gaugeFspDf['StrOrd']==ord].sort_values('DsDist')\n\n        #\n        # create the upstream ending gauge assuming a level water elevation at the gauge\n        #\n        # get upstream gauge's segment IDs\n        t = gaugeOrd.tail(1)[['SegId','FilledElev','Dof','FspX','FspY']].values.flatten().tolist()\n        upSegId, gElev, dof, gx,gy = t \n        # get segment FSPs and find the level-off FSP\n        t = fspDf[fspDf['SegId']==upSegId].copy() # tell pandas we want a copy to avoid \"SettingWithCopyWarning\"\n        t['Dof'] = gElev+dof-t['FilledElev']\n        t = t[t['Dof']&gt;=0].sort_values('DsDist').tail(1)[['FspX','FspY','DsDist','FilledElev','Dof']]\n        # check if the ending gauge is the gauge itself\n        tx,ty = t.iat[0,0], t.iat[0,1]\n        # same as: tx,ty = t[['FspX','FspY']].values.flatten().tolist()\n        if (gx==tx) and (gy==ty):\n            # the gauge is the ending gauge.\n            usEndGauge = pd.DataFrame() # an empty DF\n        else:\n            usEndGauge = t\n        # print(usEndGauge)\n\n        #\n        # create downstream ending gauge\n        #\n        # get the downstream order and junction FSP's coordinates\n        t = strOrdDf[strOrdDf['StrOrd']==ord][['DsStrOrd','JunctionFspX', 'JunctionFspY']].values.flatten().tolist()\n        dsStrOrd,fspx,fspy = t \n\n        # whether there is a junction/confluence gauge\n        if dsStrOrd !=0:\n            # there is a downstream order. see whether the junction's DOF has an interpolated DOF\n            juncDf = fspDof[(fspDof['FspX']==fspx) &amp;(fspDof['FspY']==fspy)]\n            if len(juncDf) != 0:\n                # Junction has DOF. Create downstream ending gauge with the junction FSP\n                dsEndGauge = juncDf[['FspX','FspY','DsDist','FilledElev','Dof']]\n            else:\n                juncDf = None\n\n        # No downstream order or junction FSP\n        if (dsStrOrd == 0) or (juncDf is None):\n            # Create downstream ending gauge with the last FSP in the segment\n            # get the downstream and upstream segment IDs\n            dsSegId, gx,gy = gaugeOrd.head(1)[['SegId','FspX','FspY']].values.flatten().tolist()\n            # get the first FSP on the downstream segment\n            t = fspDf[fspDf['SegId']==dsSegId].tail(1)[['FspX','FspY','DsDist','FilledElev']]\n            # check if the ending gauge is the gauge itself\n            tx,ty = t.iat[0,0], t.iat[0,1]\n            # same as: tx,ty = t[['FspX','FspY']].values.flatten().tolist()\n            if (gx==tx) and (gy==ty):\n                # the gauge is the ending gauge.\n                dsEndGauge = pd.DataFrame() # an empty DF\n            else:\n                dsEndGauge = t\n                dsEndGauge['Dof']=0\n        # print(dsEndGauge)\n\n        # put all the gauges together\n        gaugeOrd = gaugeOrd[['FspX','FspY','DsDist','FilledElev','Dof']]\n        # gaugeOrd = gaugeOrd.append(usEndGauge)\n        # gaugeOrd = gaugeOrd.append(dsEndGauge)\n        gaugeOrd = pd.concat([gaugeOrd,usEndGauge])\n        gaugeOrd = pd.concat([gaugeOrd,dsEndGauge])\n        # print(f\"Gauges with DOFs on stream order ({ord}): \\n\",gaugeOrd)\n\n        # calculate the min and max downstream distance\n        minDist = gaugeOrd['DsDist'].min()\n        maxDist = gaugeOrd['DsDist'].max()\n\n        # select the FSPs on the stream order\n        fspOrd = fspDf[fspDf['StrOrd']==ord][['FspId','FspX','FspY','DsDist','FilledElev']]\n        fspOrd = fspOrd[(fspOrd['DsDist']&gt;=minDist) &amp; (fspOrd['DsDist']&lt;=maxDist)]\n        fx, fe = fspOrd['DsDist'].to_numpy(),fspOrd['FilledElev'].to_numpy()\n\n        # prepare gauges\n        gaugeOrd = gaugeOrd.sort_values('DsDist')\n        gx, ge, gy = gaugeOrd['DsDist'].to_numpy(), gaugeOrd['FilledElev'].to_numpy(), gaugeOrd['Dof'].astype(np.float64).to_numpy()\n\n        # interpolate\n        fspOrd['Dof'] = InterpDofWithGauges(fx,fe,gx,ge,gy,weightingType)\n\n        # append the interpolated SFPs\n        # fspDof = fspDof.append(fspOrd,ignore_index=True)\n        fspDof = pd.concat([fspDof,fspOrd],ignore_index=True)\n        # print(fspDof)\n\n    # select columns\n    fspDof = fspDof[['FspId','Dof']]\n    # fspDof = fspDof[['FspId','DsDist','FilledElev','Dof']] # keep more columns for checking the interpolation\n    # print(fspDof)\n\n    # return interpolated DOF for the FSPs\n    return fspDof\n</code></pre>"},{"location":"mapping/#fldpln.mapping.EstimateFspDofFromGaugeBlob","title":"<code>EstimateFspDofFromGaugeBlob(libBlobSerClient, libName, gaugeDf, gaugeElevField, minGaugeDof=0.0328084)</code>","text":"<p>Estimate FSP Depth of Flow (DoF), i.e., FSP stage from gauges on Microsoft Planetary Computer using Azure Blob Storage.</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a BlobServiceClient object</p> required <code>libName</code> <code>str</code> <p>the name of the library that the gauges will be snapped to</p> required <code>gaugeDf</code> <code>data frame</code> <p>a data frame of gauges. It should have at least 4 columns ['FspX','FspY','FspFilledElev','Dist']</p> required <code>gaugeElevField</code> <code>str</code> <p>the field in gaugeDf that stores gauge's water surface elevation</p> required <code>minGaugeDof</code> <code>float</code> <p>min DOF a gauge should have, default is 1 cm = 0.0328083989501312 foot. Negative DOF may occur as incorrect gauge datum</p> <code>0.0328084</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with interpolated FSP DOFs.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def EstimateFspDofFromGaugeBlob(libBlobSerClient,libName,gaugeDf,gaugeElevField,minGaugeDof=0.0328084):\n    \"\"\" Estimate FSP Depth of Flow (DoF), i.e., FSP stage from gauges on Microsoft Planetary Computer using Azure Blob Storage.\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a BlobServiceClient object\n            libName (str): the name of the library that the gauges will be snapped to\n            gaugeDf (data frame): a data frame of gauges. It should have at least 4 columns ['FspX','FspY','FspFilledElev','Dist']\n            gaugeElevField (str): the field in gaugeDf that stores gauge's water surface elevation\n            minGaugeDof (float): min DOF a gauge should have, default is 1 cm = 0.0328083989501312 foot. Negative DOF may occur as incorrect gauge datum\n\n        Return:\n            data frame: a data frame with interpolated FSP DOFs.\n    \"\"\"\n\n    # calculate FSP DOF\n    gaugeDf['Dof'] = gaugeDf[gaugeElevField] - gaugeDf['FspFilledElev']\n    # reset DOF, if it's &lt; 0, to minGaugeDof\n    gaugeDf.loc[gaugeDf['Dof']&lt;=minGaugeDof,['Dof']] = minGaugeDof\n    print(gaugeDf)\n\n    #\n    # read in FSP and stream order network files \n    #\n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n\n    # read fsp info csv files\n    blob_client = container_client.get_blob_client(fspInfoFileName)\n    # create a SAS token\n    sas_token = azure.storage.blob.generate_blob_sas(\n        container_client.account_name,\n        container_client.container_name,\n        blob_client.blob_name,\n        account_key=container_client.credential.account_key,\n        permission=[\"read\"],\n    )\n    # construct the URL\n    url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n    # read the blob\n    fspDf = pd.read_csv(url)\n\n    # read stream order text file\n    blob_client = container_client.get_blob_client(strOrdNetFileName)\n    # create a SAS token\n    sas_token = azure.storage.blob.generate_blob_sas(\n        container_client.account_name,\n        container_client.container_name,\n        blob_client.blob_name,\n        account_key=container_client.credential.account_key,\n        permission=[\"read\"],\n    )\n    # construct the URL\n    url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n    # read the blob\n    strOrdDf = pd.read_csv(url)\n\n    # Get gauge stream order for interpolation by stream orders\n    gaugeDf = pd.merge(gaugeDf,fspDf,how='inner',on=['FspX','FspY'])[['FspX','FspY','Dof','StrOrd','DsDist','SegId','FilledElev']]\n    # print(gaugeDf)\n\n    # create an empty DF to store interpolated FSP DOFs\n    fspDof = pd.DataFrame(columns=['FspId','FspX','FspY','DsDist','Dof'])\n    # interpolate DoF for each stream order from low to high (low order means high priority!)\n    # get the stream orders with gauges on them\n    strOrds = gaugeDf['StrOrd'].drop_duplicates().sort_values().tolist()\n    if len(strOrds)==0:\n        print('No stream found for the gauges!')\n        return None\n\n    for ord in strOrds:\n        # gauges on the stream order\n        gaugeOrd = gaugeDf[gaugeDf['StrOrd']==ord].sort_values('DsDist')\n\n        #\n        # create the upstream ending gauge assuming a level water elevation at the gauge\n        #\n        # get upstream gauge's segment IDs\n        t = gaugeOrd.tail(1)[['SegId','FilledElev','Dof','FspX','FspY']].values.flatten().tolist()\n        upSegId, gElev, dof, gx,gy = t \n        # get segment FSPs and find the level-off FSP\n        t = fspDf[fspDf['SegId']==upSegId].copy() # tell pandas we want a copy to avoid \"SettingWithCopyWarning\"\n        t['Dof'] = gElev+dof-t['FilledElev']\n        t = t[t['Dof']&gt;=0].sort_values('DsDist').tail(1)[['FspX','FspY','DsDist','Dof']]\n        # check if the ending gauge is the gauge itself\n        tx,ty = t.iat[0,0], t.iat[0,1]\n        # same as: tx,ty = t[['FspX','FspY']].values.flatten().tolist()\n        if (gx==tx) and (gy==ty):\n            # the gauge is the ending gauge.\n            usEndGauge = pd.DataFrame() # an empty DF\n        else:\n            usEndGauge = t\n        # print(usEndGauge)\n\n        #\n        # create downstream ending gauge\n        #\n        # get the downstream order and junction FSP's coordinates\n        t = strOrdDf[strOrdDf['StrOrd']==ord][['DsStrOrd','JunctionFspX', 'JunctionFspY']].values.flatten().tolist()\n        dsStrOrd,fspx,fspy = t \n\n        # whether there is a junction/confluence gauge\n        if dsStrOrd !=0:\n            # there is a downstream order. see whether the junction's DOF has an interpolated DOF\n            juncDf = fspDof[(fspDof['FspX']==fspx) &amp;(fspDof['FspY']==fspy)]\n            if len(juncDf) != 0:\n                # Junction has DOF. Create downstream ending gauge with the junction FSP\n                dsEndGauge = juncDf[['FspX','FspY','DsDist','Dof']]\n            else:\n                juncDf = None\n\n        # No downstream order or junction FSP\n        if (dsStrOrd == 0) or (juncDf is None):\n            # Create downstream ending gauge with the last FSP in the segment\n            # get the downstream and upstream segment IDs\n            dsSegId, gx,gy = gaugeOrd.head(1)[['SegId','FspX','FspY']].values.flatten().tolist()\n            # get the first FSP on the downstream segment\n            t = fspDf[fspDf['SegId']==dsSegId].tail(1)[['FspX','FspY','DsDist']]\n            # check if the ending gauge is the gauge itself\n            tx,ty = t.iat[0,0], t.iat[0,1]\n            # same as: tx,ty = t[['FspX','FspY']].values.flatten().tolist()\n            if (gx==tx) and (gy==ty):\n                # the gauge is the ending gauge.\n                dsEndGauge = pd.DataFrame() # an empty DF\n            else:\n                dsEndGauge = t\n                dsEndGauge['Dof']=0\n        # print(dsEndGauge) \n\n        # put all the gauges together\n        gaugeOrd = gaugeOrd[['FspX','FspY','DsDist','Dof']]\n        # gaugeOrd = gaugeOrd.append(usEndGauge)\n        # gaugeOrd = gaugeOrd.append(dsEndGauge)\n        gaugeOrd = pd.concat([gaugeOrd,usEndGauge])\n        gaugeOrd = pd.concat([gaugeOrd,dsEndGauge])\n        # print(f\"Gauges with DOFs on stream order ({ord}): \\n\",gaugeOrd)\n\n        # calculate the min and max downstream distance\n        minDist = gaugeOrd['DsDist'].min()\n        maxDist = gaugeOrd['DsDist'].max()\n\n        # select the FSPs on the stream order\n        fspOrd = fspDf[fspDf['StrOrd']==ord][['FspId','FspX','FspY','DsDist']]\n        fspOrd = fspOrd[(fspOrd['DsDist']&gt;=minDist) &amp; (fspOrd['DsDist']&lt;=maxDist)]\n\n        # interpolate DOF for the FSPs with the gauges\n        gaugeOrd = gaugeOrd.sort_values('DsDist') # for using np.interp(), x must be ascending!\n        fspOrd['Dof'] = np.interp(fspOrd['DsDist'], gaugeOrd['DsDist'], gaugeOrd['Dof'].astype(np.float64))\n        # print(fspOrd)\n\n        # append the interpolated SFPs\n        # fspDof = fspDof.append(fspOrd,ignore_index=True)\n        fspDof = pd.concat([fspDof,fspOrd],ignore_index=True)\n        # print(fspDof)\n\n    # select and rename columns\n    fspDof = fspDof[['FspId','Dof']]\n    # print(fspDof)\n\n    # return interpolated DOF for the FSPs\n    return fspDof\n</code></pre>"},{"location":"mapping/#fldpln.mapping.GetTileTifs","title":"<code>GetTileTifs(tifFiles)</code>","text":"<p>Get tile Geotif files</p> <p>Parameters:</p> Name Type Description Default <code>tifFiles</code> <code>list</code> <p>a list of tile Geotif files</p> required <p>Returns:</p> Type Description <code>list</code> <p>a list of tile Geotif files</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def GetTileTifs(tifFiles):\n    \"\"\" Get tile Geotif files\n\n        Args:\n            tifFiles (list): a list of tile Geotif files\n\n        Return:\n            list: a list of tile Geotif files\n    \"\"\"\n\n    tileTifs = []\n    for tif in tifFiles:\n        if not(tif is None):\n            tileTifs.append(tif)\n    return tileTifs\n</code></pre>"},{"location":"mapping/#fldpln.mapping.InterpBetweenTwoGauges","title":"<code>InterpBetweenTwoGauges(fx, fe, gx1, ge1, gy1, gx2, ge2, gy2, weightingType='V')</code>","text":"<p>Interpolate FSP DOF (Depth of Flow, i.e., FSP stage) based on FSP's elevation (fe) or distance (fx) between two gauges.</p> <p>Parameters:</p> Name Type Description Default <code>fx</code> <code>float or vector of float</code> <p>FSP's distance from downstream outlet.</p> required <code>fe</code> <code>float or vector of float</code> <p>FSP's elevation. Can be a vector</p> required <code>gx1</code> <code>float</code> <p>gauge1's distance from downstream outlet</p> required <code>ge1</code> <code>float</code> <p>gauge1's elevation</p> required <code>gy1</code> <code>float</code> <p>gauge1's DOF</p> required <code>gx2</code> <code>float</code> <p>gauge2's distance from downstream outlet</p> required <code>ge2</code> <code>float</code> <p>gauge2's elevation</p> required <code>gy2</code> <code>float</code> <p>gauge2's DOF</p> required <code>weightingType</code> <code>str</code> <p>'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'</p> <code>'V'</code> <p>Returns:</p> Type Description <code>float or vector of float</code> <p>interpolated DOF at fx</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def InterpBetweenTwoGauges(fx, fe, gx1, ge1, gy1, gx2, ge2, gy2, weightingType='V'):\n    \"\"\" Interpolate FSP DOF (Depth of Flow, i.e., FSP stage) based on FSP's elevation (fe) or distance (fx) between two gauges.\n\n        Args:\n            fx (float or vector of float): FSP's distance from downstream outlet.\n            fe (float or vector of float): FSP's elevation. Can be a vector\n            gx1 (float): gauge1's distance from downstream outlet\n            ge1 (float): gauge1's elevation\n            gy1 (float): gauge1's DOF\n            gx2 (float): gauge2's distance from downstream outlet\n            ge2 (float): gauge2's elevation\n            gy2 (float): gauge2's DOF\n            weightingType (str): 'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'\n\n        Return:\n            float or vector of float: interpolated DOF at fx\n\"\"\"\n    if weightingType == 'H':\n        # distance-based (hirizontal) linear interpolation\n        fy = gy1+(fx-gx1)/(gx2-gx1)*(gy2-gy1)\n    else:\n        # elevation-based (vertical) linear interpolation\n        # when two gauges have the same elevation, using distance-based (horizontal) interpolation    \n        if (ge1 == ge2):\n            fy = gy1+(fx-gx1)/(gx2-gx1)*(gy2-gy1)\n        else:\n            # elevation-based (vertical) interpolation\n            fy = gy1+(fe-ge1)/(ge2-ge1)*(gy2-gy1)\n\n    return fy\n</code></pre>"},{"location":"mapping/#fldpln.mapping.InterpDofWithGauges","title":"<code>InterpDofWithGauges(fx, fe, gx, ge, gy, weightingType='V')</code>","text":"<p>Interpolate FSP DOF (Depth of Flow, i.e., FSP stage) using the DOFs observed at a list of gauges.</p> <p>Parameters:</p> Name Type Description Default <code>fx</code> <code>vector of float</code> <p>FSP's distance from downstream outlet.</p> required <code>fe</code> <code>vector of float</code> <p>FSP's elevation.</p> required <code>gx</code> <code>vector of float</code> <p>gauge's distance from downstream outlet.</p> required <code>ge</code> <code>vector of float</code> <p>gauge's elevation.</p> required <code>gy</code> <code>vector of float</code> <p>gauge's DOF.</p> required <code>weightingType</code> <code>str</code> <p>'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'</p> <code>'V'</code> <p>Returns:</p> Type Description <code>vector of float</code> <p>interpolated DOF at fx    </p> Source code in <code>fldpln/mapping.py</code> <pre><code>def InterpDofWithGauges(fx,fe,gx,ge,gy,weightingType='V'):\n    \"\"\" Interpolate FSP DOF (Depth of Flow, i.e., FSP stage) using the DOFs observed at a list of gauges.\n\n        Args:\n            fx (vector of float): FSP's distance from downstream outlet.\n            fe (vector of float): FSP's elevation.\n            gx (vector of float): gauge's distance from downstream outlet.\n            ge (vector of float): gauge's elevation.\n            gy (vector of float): gauge's DOF.\n            weightingType (str): 'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'\n\n        Return:\n            vector of float: interpolated DOF at fx    \n    \"\"\"\n    # initialize the return vector as NAN\n    fy = np.empty(fx.size)\n    fy[:] = np.nan\n\n    # interpolate by gauge pairs\n    for i in range(gy.size-1):\n        # get a pair of gauges\n        gx1,ge1,gy1=gx[i],ge[i],gy[i]\n        gx2,ge2,gy2=gx[i+1],ge[i+1],gy[i+1]\n\n        # prepare the FSPs for interpolation\n        ind = np.where((fx&gt;=gx1) &amp; (fx&lt;=gx2))\n        fxi, fei=fx[ind], fe[ind]\n        fy[ind] = InterpBetweenTwoGauges(fxi, fei, gx1, ge1, gy1, gx2, ge2, gy2, weightingType)\n    return fy\n</code></pre>"},{"location":"mapping/#fldpln.mapping.InterpolateFspDofFromGauge","title":"<code>InterpolateFspDofFromGauge(libFolder, libName, gaugeFspDf, minGaugeDof=0.0328084, weightingType='V')</code>","text":"<p>Interpolate FSP DOF (Depth of Flow, i.e., FSP stage) from observed gauge DOFs using distance-(horizontal) or  elevation-based (vertical) linear interpolation. Different from EstimateFspDofFromGauge(),  this function assumes gauge FSPs already have their DOF calculated!</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are located</p> required <code>libName</code> <code>str</code> <p>the name of the library that the gauges will be snapped to</p> required <code>gaugeFspDf</code> <code>data frame</code> <p>a data frame of gauge FSPs (i.e., FSPs to which gauges are snapped). It should have at least 4 columns ['lib_name','FspX','FspY','Dof'].</p> required <code>minGaugeDof</code> <code>float</code> <p>min DOF a gauge should have, by default is 1 cm = 0.0328083989501312 foot</p> <code>0.0328084</code> <code>weightingType</code> <code>str</code> <p>'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'</p> <code>'V'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with interpolated FSP DOFs.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf,minGaugeDof=0.0328084,weightingType='V'):\n    \"\"\" Interpolate FSP DOF (Depth of Flow, i.e., FSP stage) from observed gauge DOFs using distance-(horizontal) or \n        elevation-based (vertical) linear interpolation. Different from EstimateFspDofFromGauge(), \n        this function assumes gauge FSPs already have their DOF calculated!\n\n        Args:\n            libFolder (str): the folder where the libraries are located\n            libName (str): the name of the library that the gauges will be snapped to\n            gaugeFspDf (data frame): a data frame of gauge FSPs (i.e., FSPs to which gauges are snapped). It should have at least 4 columns ['lib_name','FspX','FspY','Dof'].\n            minGaugeDof (float): min DOF a gauge should have, by default is 1 cm = 0.0328083989501312 foot\n            weightingType (str): 'V' for vertical distance-based or 'H' for horizontal distance-based, default is 'V'\n\n        Return:\n            data frame: a data frame with interpolated FSP DOFs.\n    \"\"\"\n\n    # select the gauge FSPs in the library\n    gaugeFspDf = gaugeFspDf[gaugeFspDf['lib_name']==libName]\n\n    # read in FSP and stream order network files\n    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n    strOrdFile = os.path.join(libFolder, libName, strOrdNetFileName)\n    fspDf = pd.read_csv(fspFile) \n    strOrdDf = pd.read_csv(strOrdFile) \n\n    # reset gauge FSP DOF, if it's &lt; 0 or nodata, to minGaugeDof\n    gaugeFspDf.loc[(gaugeFspDf['Dof']&lt;=minGaugeDof)|gaugeFspDf['Dof'].isna(),['Dof']] = minGaugeDof\n    # print(gaugeFspDf)\n\n    # create an empty DF to store interpolated FSP DOFs\n    fspDof = pd.DataFrame(columns=['FspId','FspX','FspY','DsDist','FilledElev','Dof'])\n    # interpolate DoF for each stream order from low to high (low order means high priority!)\n    # get the stream orders with gauges on them\n    strOrds = gaugeFspDf['StrOrd'].drop_duplicates().sort_values().tolist()\n    if len(strOrds)==0:\n        print('No stream order found for the gauges!')\n        return None\n\n    for ord in strOrds:\n        # gauges on the stream order\n        gaugeOrd = gaugeFspDf[gaugeFspDf['StrOrd']==ord].sort_values('DsDist')\n\n        #\n        # create the upstream ending gauge assuming a level water elevation at the gauge\n        #\n        # get upstream gauge's segment IDs\n        t = gaugeOrd.tail(1)[['SegId','FilledElev','Dof','FspX','FspY']].values.flatten().tolist()\n        upSegId, gElev, dof, gx,gy = t \n        # get segment FSPs and find the level-off FSP\n        t = fspDf[fspDf['SegId']==upSegId].copy() # tell pandas we want a copy to avoid \"SettingWithCopyWarning\"\n        t['Dof'] = gElev+dof-t['FilledElev']\n        t = t[t['Dof']&gt;=0].sort_values('DsDist').tail(1)[['FspX','FspY','DsDist','FilledElev','Dof']]\n        # check if the ending gauge is the gauge itself\n        tx,ty = t.iat[0,0], t.iat[0,1]\n        # same as: tx,ty = t[['FspX','FspY']].values.flatten().tolist()\n        if (gx==tx) and (gy==ty):\n            # the gauge is the ending gauge.\n            usEndGauge = pd.DataFrame() # an empty DF\n        else:\n            usEndGauge = t\n        # print(usEndGauge)\n\n        #\n        # create downstream ending gauge\n        #\n        # get the downstream order and junction FSP's coordinates\n        t = strOrdDf[strOrdDf['StrOrd']==ord][['DsStrOrd','JunctionFspX', 'JunctionFspY']].values.flatten().tolist()\n        dsStrOrd,fspx,fspy = t \n\n        # whether there is a junction/confluence gauge\n        if dsStrOrd !=0:\n            # there is a downstream order. see whether the junction's DOF has an interpolated DOF\n            juncDf = fspDof[(fspDof['FspX']==fspx) &amp;(fspDof['FspY']==fspy)]\n            if len(juncDf) != 0:\n                # Junction has DOF. Create downstream ending gauge with the junction FSP\n                dsEndGauge = juncDf[['FspX','FspY','DsDist','FilledElev','Dof']]\n            else:\n                juncDf = None\n\n        # No downstream order or junction FSP\n        if (dsStrOrd == 0) or (juncDf is None):\n            # Create downstream ending gauge with the last FSP in the segment\n            # get the downstream and upstream segment IDs\n            dsSegId, gx,gy = gaugeOrd.head(1)[['SegId','FspX','FspY']].values.flatten().tolist()\n            # get the first FSP on the downstream segment\n            t = fspDf[fspDf['SegId']==dsSegId].tail(1)[['FspX','FspY','DsDist','FilledElev']]\n            # check if the ending gauge is the gauge itself\n            tx,ty = t.iat[0,0], t.iat[0,1]\n            # same as: tx,ty = t[['FspX','FspY']].values.flatten().tolist()\n            if (gx==tx) and (gy==ty):\n                # the gauge is the ending gauge.\n                dsEndGauge = pd.DataFrame() # an empty DF\n            else:\n                dsEndGauge = t\n                dsEndGauge['Dof']=0\n        # print(dsEndGauge)\n\n        # put all the gauges together\n        gaugeOrd = gaugeOrd[['FspX','FspY','DsDist','FilledElev','Dof']]\n        # gaugeOrd = gaugeOrd.append(usEndGauge)\n        gaugeOrd = pd.concat([gaugeOrd,usEndGauge])\n        # gaugeOrd = gaugeOrd.append(dsEndGauge)\n        gaugeOrd = pd.concat([gaugeOrd,dsEndGauge])\n        # print(f\"Gauges with DOFs on stream order ({ord}): \\n\",gaugeOrd)\n\n        # calculate the min and max downstream distance\n        minDist = gaugeOrd['DsDist'].min()\n        maxDist = gaugeOrd['DsDist'].max()\n\n        # select the FSPs on the stream order\n        fspOrd = fspDf[fspDf['StrOrd']==ord][['FspId','FspX','FspY','DsDist','FilledElev']]\n        fspOrd = fspOrd[(fspOrd['DsDist']&gt;=minDist) &amp; (fspOrd['DsDist']&lt;=maxDist)]\n        fx, fe = fspOrd['DsDist'].to_numpy(),fspOrd['FilledElev'].to_numpy()\n\n        # prepare gauges\n        gaugeOrd = gaugeOrd.sort_values('DsDist')\n        gx, ge, gy = gaugeOrd['DsDist'].to_numpy(), gaugeOrd['FilledElev'].to_numpy(), gaugeOrd['Dof'].astype(np.float64).to_numpy()\n\n        # interpolate\n        fspOrd['Dof'] = InterpDofWithGauges(fx,fe,gx,ge,gy,weightingType)\n\n        # append the interpolated SFPs\n        # fspDof = fspDof.append(fspOrd,ignore_index=True)\n        fspDof = pd.concat([fspDof,fspOrd],ignore_index=True)\n        # print(fspDof)\n\n    # select columns\n    fspDof = fspDof[['FspId','Dof']]\n    # fspDof = fspDof[['FspId','DsDist','FilledElev','Dof']] # keep more columns for checking the interpolation\n    # print(fspDof)\n\n    # return interpolated DOF for the FSPs\n    return fspDof\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MapFloodDepthWithTiles","title":"<code>MapFloodDepthWithTiles(libFolder, libName, fileFormat, outMapFolder, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Map flood depth with tiled library based on FSP DOF and AOI extent</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are stored</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>fileFormat</code> <code>str</code> <p>the file format of the tile, 'snappy' or 'mat'</p> required <code>outMapFolder</code> <code>str</code> <p>the folder where the mapped tiles will be saved</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'.  If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.  If it's a float, it's a constant stage for all the FSPs.  If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>a list of mapped tile names as GeoTif files.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MapFloodDepthWithTiles(libFolder,libName,fileFormat,outMapFolder,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Map flood depth with tiled library based on FSP DOF and AOI extent\n\n        Args:\n            libFolder (str): the folder where the libraries are stored\n            libName (str): the name of the library\n            fileFormat (str): the file format of the tile, 'snappy' or 'mat'\n            outMapFolder (str): the folder where the mapped tiles will be saved\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'. \n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. \n                If it's a float, it's a constant stage for all the FSPs. \n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None\n\n        Return:\n            list: a list of mapped tile names as GeoTif files.\n    \"\"\"\n\n    # create the folder for generating tile maps\n    os.makedirs(outMapFolder,exist_ok=True)\n\n    #\n    # Read lib meta data file\n    #\n    metaDataFile = os.path.join(libFolder, libName, metaDataFileName)\n    with open(metaDataFile,'r') as jf:\n        md = json.load(jf)\n    cellSize = md['CellSize']\n    srText = md['SpatialReference']\n    libSr = rasterio.crs.CRS.from_wkt(srText)\n\n    #\n    # decide the tiles to map\n    #\n    tileIds,fppExtents = Tiles2Map(libFolder,libName,fspDof,aoiExtent=None)\n    print('Tiles need to be mapped:',tileIds)\n\n    #\n    # map the selected tiles\n    #\n    if tileIds is None:\n        tileTifs = None\n    else:\n        tileTifs = []\n        for tid,fppExtent in zip(tileIds,fppExtents):\n            tif=MapOneTile(libFolder,libName,tid,fppExtent,cellSize,libSr,fileFormat,outMapFolder,fspDof,aoiExtent)\n            if not(tif is None):\n                tileTifs.append(tif)\n        if not tileTifs: # empty list\n            tileTifs = None\n\n    return tileTifs\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MapFloodDepthWithTilesAsDag","title":"<code>MapFloodDepthWithTilesAsDag(libFolder, libName, fileFormat, outMapFolder, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Map flood depth with tiled library based on FSP DOF and AOI extent as a Directed Acyclic Graph (DAG)</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are stored</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>fileFormat</code> <code>str</code> <p>the file format of the tile, 'snappy' or 'mat'</p> required <code>outMapFolder</code> <code>str</code> <p>the folder where the mapped tiles will be saved</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'.  If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.  If it's a float, it's a constant stage for all the FSPs.  If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>a Directed Acyclic Graph (DAG) and the root node name.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MapFloodDepthWithTilesAsDag(libFolder,libName,fileFormat,outMapFolder,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Map flood depth with tiled library based on FSP DOF and AOI extent as a Directed Acyclic Graph (DAG)\n\n        Args:\n            libFolder (str): the folder where the libraries are stored\n            libName (str): the name of the library\n            fileFormat (str): the file format of the tile, 'snappy' or 'mat'\n            outMapFolder (str): the folder where the mapped tiles will be saved\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'. \n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. \n                If it's a float, it's a constant stage for all the FSPs. \n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None\n\n        Return:\n            tuple: a Directed Acyclic Graph (DAG) and the root node name.\n    \"\"\"\n\n    # create the folder for generating tile maps\n    os.makedirs(outMapFolder,exist_ok=True)\n\n    #\n    # Read lib meta data file\n    #\n    metaDataFile = os.path.join(libFolder, libName, metaDataFileName)\n    with open(metaDataFile,'r') as jf:\n        md = json.load(jf)\n    cellSize = md['CellSize']\n    srText = md['SpatialReference']\n    libSr = rasterio.crs.CRS.from_wkt(srText)\n\n    #\n    # decide the tiles to map\n    #\n    tileIds,fppExtents = Tiles2Map(libFolder,libName,fspDof,aoiExtent=None)\n    print('Tiles need to be mapped:',tileIds)\n\n    #\n    # map the selected tiles\n    #\n    if tileIds is None:\n        dag = None\n        dagRootName = None\n    else:\n        dag = {}\n        for tid,fppExtent in zip(tileIds,fppExtents):\n            # tif=MapOneTileBlob(libBlobSerClient,libName,tid,fppExtent,cellSize,libSr,fileFormat,mapContainerClient,fspDof,aoiExtent)\n            dag[f'MapOneTile_{tid}'] = (MapOneTile,libFolder,libName,tid,fppExtent,cellSize,libSr,fileFormat,outMapFolder,fspDof,aoiExtent)\n\n        # MosaicGtifsBlob(mapContClient,tileTifs,outName,keepTileMaps)  \n        dagRootName = 'MapTiles'\n        # dag[dagRootName] = (MosaicGtifsBlob,mapContClient,list(dag.keys()),outName,False)\n        dag[dagRootName] = (GetTileTifs,list(dag.keys()))\n\n    return dag,dagRootName\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MapFloodDepthWithTilesBlob","title":"<code>MapFloodDepthWithTilesBlob(libBlobSerClient, libName, fileFormat, mapContainerClient, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Map flood depth with tiled library based on FSP DOF and AOI extent on Microsoft Planetary Computer using data in Azure Blob Storage.</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a BlobServiceClient object</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>fileFormat</code> <code>str</code> <p>the file format of the tile, 'snappy' or 'mat'</p> required <code>mapContainerClient</code> <code>ContainerClient</code> <p>a ContainerClient object for the container to store the mapped tiles</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'.  If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.  If it's a float, it's a constant stage for all the FSPs.  If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>a list of mapped tile names as GeoTif files</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MapFloodDepthWithTilesBlob(libBlobSerClient,libName,fileFormat,mapContainerClient,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Map flood depth with tiled library based on FSP DOF and AOI extent on Microsoft Planetary Computer using data in Azure Blob Storage.\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a BlobServiceClient object\n            libName (str): the name of the library\n            fileFormat (str): the file format of the tile, 'snappy' or 'mat'\n            mapContainerClient (ContainerClient): a ContainerClient object for the container to store the mapped tiles\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'. \n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. \n                If it's a float, it's a constant stage for all the FSPs. \n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None\n\n        Return:\n            list: a list of mapped tile names as GeoTif files\n    \"\"\"\n\n    #\n    # Read lib meta data file\n    #    \n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n    # get blob client\n    blob_client = container_client.get_blob_client(metaDataFileName)\n    # read the blob into memory\n    streamdownloader = blob_client.download_blob()\n    md = json.loads(streamdownloader.readall())\n    cellSize = md['CellSize']\n    srText = md['SpatialReference']\n    libSr = rasterio.crs.CRS.from_wkt(srText)\n\n    #\n    # decide the tiles to map\n    #\n    tileIds,fppExtents = Tiles2MapBlob(libBlobSerClient,libName,fspDof,aoiExtent=None)\n    print('Tiles need to be mapped:',tileIds)\n\n    #\n    # map the selected tiles\n    #\n    if tileIds is None:\n        tileTifs = None\n    else:\n        tileTifs = []\n        for tid,fppExtent in zip(tileIds,fppExtents):\n            tif=MapOneTileBlob(libBlobSerClient,libName,tid,fppExtent,cellSize,libSr,fileFormat,mapContainerClient,fspDof,aoiExtent)\n            if not(tif is None):\n                tileTifs.append(tif)\n    print('Actual tiles mapped:',tileTifs)\n\n    return tileTifs\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MapFloodDepthWithTilesBlobAsDag","title":"<code>MapFloodDepthWithTilesBlobAsDag(libBlobSerClient, libName, fileFormat, mapContainerClient, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Map flood depth with tiled library based on FSP DOF and AOI extent on Microsoft Planetary Computer using data in Azure Blob Storage  as a Directed Acyclic Graph (DAG).</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a BlobServiceClient object</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>fileFormat</code> <code>str</code> <p>the file format of the tile, 'snappy' or 'mat'</p> required <code>mapContainerClient</code> <code>ContainerClient</code> <p>a ContainerClient object for the container to store the mapped tiles</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'.  If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.  If it's a float, it's a constant stage for all the FSPs.  If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>a Directed Acyclic Graph (DAG) and the root node name</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MapFloodDepthWithTilesBlobAsDag(libBlobSerClient,libName,fileFormat,mapContainerClient,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Map flood depth with tiled library based on FSP DOF and AOI extent on Microsoft Planetary Computer using data in Azure Blob Storage \n        as a Directed Acyclic Graph (DAG).\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a BlobServiceClient object\n            libName (str): the name of the library\n            fileFormat (str): the file format of the tile, 'snappy' or 'mat'\n            mapContainerClient (ContainerClient): a ContainerClient object for the container to store the mapped tiles\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'. \n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. \n                If it's a float, it's a constant stage for all the FSPs. \n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None\n\n        Return:\n            tuple: a Directed Acyclic Graph (DAG) and the root node name\n    \"\"\"\n\n    #\n    # Read lib meta data file\n    #    \n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n    # get blob client\n    blob_client = container_client.get_blob_client(metaDataFileName)\n    # read the blob into memory\n    streamdownloader = blob_client.download_blob()\n    md = json.loads(streamdownloader.readall())\n    cellSize = md['CellSize']\n    srText = md['SpatialReference']\n    libSr = rasterio.crs.CRS.from_wkt(srText)\n\n    #\n    # decide the tiles to map\n    #\n    tileIds,fppExtents = Tiles2MapBlob(libBlobSerClient,libName,fspDof,aoiExtent=None)\n    print('Tiles need to be mapped:',tileIds)\n\n    #\n    # map the selected tiles\n    #\n    if tileIds is None:\n        dag = None\n        dagRootName = None\n    else:\n        dag = {}\n        for tid,fppExtent in zip(tileIds,fppExtents):\n            # tif=MapOneTileBlob(libBlobSerClient,libName,tid,fppExtent,cellSize,libSr,fileFormat,mapContainerClient,fspDof,aoiExtent)\n            dag[f'MapOneTileBlob_{tid}'] = (MapOneTileBlob,libBlobSerClient,libName,tid,fppExtent,cellSize,libSr,fileFormat,mapContainerClient,fspDof,aoiExtent)\n\n        # MosaicGtifsBlob(mapContClient,tileTifs,outName,keepTileMaps)  \n        dagRootName = 'MapTiles'\n        # dag[dagRootName] = (MosaicGtifsBlob,mapContClient,list(dag.keys()),outName,False)\n        dag[dagRootName] = (GetTileTifs,list(dag.keys()))\n\n    return dag,dagRootName\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MapOneTile","title":"<code>MapOneTile(libFolder, libName, tid, fppExtent, cellSize, libSr, fileFormat, outMapFolder, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Map one tile as a GeoTif file based on FSP DOF and AOI extent</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are stored</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>tid</code> <code>int</code> <p>the tile ID</p> required <code>fppExtent</code> <code>list</code> <p>the extent of the FPPs in the tile [minX,maxX,minY,maxY]</p> required <code>cellSize</code> <code>float</code> <p>the cell size of the raster</p> required <code>libSr</code> <code>str</code> <p>the spatial reference of the library</p> required <code>fileFormat</code> <code>str</code> <p>the file format of the tile, 'snappy' or 'mat'</p> required <code>outMapFolder</code> <code>str</code> <p>the folder where the mapped tiles will be saved</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'. If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. If it's a float, it's a constant stage for all the FSPs. If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None </p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>the name of the mapped tile as a GeoTif file</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MapOneTile(libFolder,libName,tid,fppExtent,cellSize,libSr,fileFormat,outMapFolder,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Map one tile as a GeoTif file based on FSP DOF and AOI extent\n\n        Args:\n            libFolder (str): the folder where the libraries are stored\n            libName (str): the name of the library\n            tid (int): the tile ID\n            fppExtent (list): the extent of the FPPs in the tile [minX,maxX,minY,maxY]\n            cellSize (float): the cell size of the raster\n            libSr (str): the spatial reference of the library\n            fileFormat (str): the file format of the tile, 'snappy' or 'mat'\n            outMapFolder (str): the folder where the mapped tiles will be saved\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'.\n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.\n                If it's a float, it's a constant stage for all the FSPs.\n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None \n\n        Return:\n            str: the name of the mapped tile as a GeoTif file\n    \"\"\"\n\n    # print('Mapping tile: ', tid)\n    # print('Read tile file ...')\n    if fileFormat == 'snappy':\n        # tileName = os.path.join(libFolder, libName, tileFileMainName+'_'+str(tid)+'.gzip') # for gzip\n        tileName = os.path.join(libFolder, libName, tileFileMainName+'_'+str(tid)+'.snz') # for snappy\n        tdf = pd.read_parquet(tileName) # the original column datatypes are kept when read into a DF!\n    elif fileFormat == 'mat':\n        # Tiles can also be saved as .mat \n        tileName = os.path.join(libFolder, libName,  tileFileMainName+'_'+str(tid)+'.mat')\n        # tileName = os.path.join('~/fldpln/libraries', libName,  tileFileMainName+'_'+str(tid)+'.mat')\n        # read from mat file\n        matFile = sio.loadmat(tileName)\n        df1 = pd.DataFrame(matFile['FspFpps'], columns=relColumnNames[0:3])\n        df2 = pd.DataFrame(matFile['DtfFilledDepth'], columns=relColumnNames[-2::])\n        tdf = pd.concat([df1, df2], axis=1)\n    else:\n        print('Unsupported file format!')\n        return None\n\n    # Turn FSP-FPP relations to a 2D array\n    dtfArray, noData, mapMinX, mapMaxY = TileFspFppRelations2Array(tdf, fppExtent, cellSize, fspDof, aoiExtent)\n\n    # map the tile\n    if not (dtfArray is None): # needs to be mapped\n        # Create and save map as a GeoTif file\n        # print('Saving map as a TIF raster ...')\n\n        # output file name\n        rasterName = os.path.join(outMapFolder,libName+'_tile_'+str(tid)+'.tif')\n\n        # create GeoTIFF profile\n        # create an Affine transformation from upper left corner coordinates and pixel sizes\n        transform = rasterio.transform.from_origin(mapMinX, mapMaxY, cellSize, cellSize)\n        profile = dict(\n            driver=\"GTiff\",\n            height = dtfArray.shape[0], \n            width = dtfArray.shape[1],\n            count=1,\n            dtype=str(dtfArray.dtype),\n            crs=libSr,\n            transform=transform,\n            nodata=noData\n        )\n\n        # write to COG file\n        with MemoryFile() as memfile:\n            # write the array to a memory file\n            with memfile.open(**profile) as mem:\n                # Populate the input file with numpy array\n                mem.write(dtfArray,1)\n            # open the memory file reading\n            with memfile.open(mode='r') as mem:\n                dst_profile = cog_profiles.get(\"deflate\")\n                cog_translate(\n                    mem,\n                    rasterName,\n                    dst_profile,\n                    in_memory=True,\n                    quiet=True,\n                )\n        return rasterName\n\n        # # code to save tile as regular GeoTIFF file\n        # with rasterio.open(rasterName, 'w', **profile) as tifRaster:\n        #     tifRaster.write(dtfArray, 1)\n        # return rasterName\n    else:\n        return None\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MapOneTileBlob","title":"<code>MapOneTileBlob(libBlobSerClient, libName, tid, fppExtent, cellSize, libSr, fileFormat, mapContainerClient, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Map one tile as a GeoTif file based on FSP DOF and AOI extent on Microsoft Planetary Computer using Azure Blob Storage.</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a BlobServiceClient object</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>tid</code> <code>int</code> <p>the tile ID</p> required <code>fppExtent</code> <code>list</code> <p>the extent of the FPPs in the tile [minX,maxX,minY,maxY]</p> required <code>cellSize</code> <code>float</code> <p>the cell size of the raster</p> required <code>libSr</code> <code>str</code> <p>the spatial reference of the library</p> required <code>fileFormat</code> <code>str</code> <p>the file format of the tile, 'snappy' or 'mat'</p> required <code>mapContainerClient</code> <code>ContainerClient</code> <p>a ContainerClient object for the container to store the mapped tiles</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'. If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. If it's a float, it's a constant stage for all the FSPs. If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY], default is None</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>the name of the mapped tile as a GeoTif file</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MapOneTileBlob(libBlobSerClient,libName,tid,fppExtent,cellSize,libSr,fileFormat,mapContainerClient,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Map one tile as a GeoTif file based on FSP DOF and AOI extent on Microsoft Planetary Computer using Azure Blob Storage.\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a BlobServiceClient object\n            libName (str): the name of the library\n            tid (int): the tile ID\n            fppExtent (list): the extent of the FPPs in the tile [minX,maxX,minY,maxY]\n            cellSize (float): the cell size of the raster\n            libSr (str): the spatial reference of the library\n            fileFormat (str): the file format of the tile, 'snappy' or 'mat'\n            mapContainerClient (ContainerClient): a ContainerClient object for the container to store the mapped tiles\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'.\n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.\n                If it's a float, it's a constant stage for all the FSPs.\n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY], default is None\n\n        Return:\n            str: the name of the mapped tile as a GeoTif file\n    \"\"\"\n\n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n\n    # print('Mapping tile: ', tid)\n    # print('Read tile file ...')\n    if fileFormat == 'snappy':\n        tileName = tileFileMainName+'_'+str(tid)+'.snz' # for snappy\n        # get blob client\n        blob_client = container_client.get_blob_client(tileName)\n        # create a SAS token\n        sas_token = azure.storage.blob.generate_blob_sas(\n            container_client.account_name,\n            container_client.container_name,\n            blob_client.blob_name,\n            account_key=container_client.credential.account_key,\n            permission=[\"read\"],\n        )\n        # construct the URL\n        url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n        # read the blob\n        tdf = pd.read_parquet(url)\n\n    elif fileFormat == 'mat':\n        print('Not supported yet!')\n        # # Tiles can also be saved as .mat \n        # tileName = os.path.join(libFolder, libName,  tileFileMainName+'_'+str(tid)+'.mat')\n        # # tileName = os.path.join('~/fldpln/libraries', libName,  tileFileMainName+'_'+str(tid)+'.mat')\n        # # read from mat file\n        # matFile = sio.loadmat(tileName)\n        # df1 = pd.DataFrame(matFile['FspFpps'], columns=relColumnNames[0:3])\n        # df2 = pd.DataFrame(matFile['DtfFilledDepth'], columns=relColumnNames[-2::])\n        # tdf = pd.concat([df1, df2], axis=1)\n    else:\n        print('Unsupported file format!')\n        return None\n\n    # Turn FSP-FPP relations to a 2D array\n    dtfArray, noData, mapMinX, mapMaxY = TileFspFppRelations2Array(tdf, fppExtent, cellSize, fspDof, aoiExtent)\n\n    # map the tile\n    if not (dtfArray is None): # needs to be mapped\n        # Create and save map as a GeoTif file\n        # print('Saving map as a TIF raster ...')\n\n        # output file name\n        rasterName = libName+'_tile_'+str(tid)+'.tif'\n\n        # create GeoTIFF profile\n        # create an Affine transformation from upper left corner coordinates and pixel sizes\n        transform = rasterio.transform.from_origin(mapMinX, mapMaxY, cellSize, cellSize)\n        profile = dict(\n            driver=\"GTiff\",\n            height = dtfArray.shape[0], \n            width = dtfArray.shape[1],\n            count=1,\n            dtype=str(dtfArray.dtype),\n            crs=libSr,\n            transform=transform,\n            nodata=noData\n        )\n\n        # write the array to blob storage as a COG file\n        with MemoryFile() as memfile:\n            # write the array to a memory file\n            with memfile.open(**profile) as mem:\n                # Populate the input file with numpy array\n                mem.write(dtfArray,1)\n            # open the memory file reading\n            with memfile.open(mode='r') as mem:\n                dst_profile = cog_profiles.get(\"deflate\")\n                # translate the memfile into a COG memfile\n                with MemoryFile() as mem_dst:\n                    # Important, we pass `mem_dst.name` as output dataset path\n                    cog_translate(mem, mem_dst.name, dst_profile, in_memory=True,quiet=True)\n                    # upload the mem file to blob storage\n                    blob_client = mapContainerClient.get_blob_client(rasterName)\n                    blob_client.upload_blob(mem_dst, overwrite=True)\n        return rasterName\n\n#         # code to save tile as a regular GeoTIFF file\n#         rasterName = libName+'_tile_'+str(tid)+'.tif'\n#         # create an Affine transformation from upper left corner coordinates and pixel sizes\n#         transform = rasterio.transform.from_origin(mapMinX, mapMaxY, cellSize, cellSize)\n\n#         # Write data to an in-memory io.BytesIO buffer\n#         # open an in-memory buffer\n#         with io.BytesIO() as buffer:\n#             # write Geotif to the buffer\n#             with rasterio.open(buffer, 'w', driver='GTiff',\n#                                     height = dtfArray.shape[0], width = dtfArray.shape[1],\n#                                     count=1, dtype=str(dtfArray.dtype),\n#                                     crs=libSr,\n#                                     transform=transform,\n#                                     nodata=noData) as tifRaster:\n#                 tifRaster.write(dtfArray, 1)\n#             # upload the in-memory tif to blob storage\n#             buffer.seek(0)\n#             blob_client = mapContainerClient.get_blob_client(rasterName)\n#             blob_client.upload_blob(buffer, overwrite=True)\n\n#         return rasterName\n    else:\n        return None\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MosaicGtifs","title":"<code>MosaicGtifs(outMapFolder, gtifs, mosaicTifName, keepTifs=False)</code>","text":"<p>Mosaic a list of GeoTifs into one GeoTif file using rasterio.merge module. See https://medium.com/spatial-data-science/how-to-mosaic-merge-raster-data-in-python-fb18e44f3c8. This func may cause memory overflow as the merge() first creates the mosaiced array in memory!</p> <p>Parameters:</p> Name Type Description Default <code>outMapFolder</code> <code>str</code> <p>the folder where the mosaiced tif will be saved</p> required <code>gtifs</code> <code>list</code> <p>a list of tile GeoTifs to be mosaiced</p> required <code>mosaicTifName</code> <code>str</code> <p>the name of the mosaiced GeoTif file</p> required <code>keepTifs</code> <code>bool</code> <p>whether to keep the tile GeoTifs, default is False</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the name of the mosaiced GeoTif.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MosaicGtifs(outMapFolder,gtifs,mosaicTifName, keepTifs=False):\n    \"\"\" Mosaic a list of GeoTifs into one GeoTif file using rasterio.merge module.\n        See https://medium.com/spatial-data-science/how-to-mosaic-merge-raster-data-in-python-fb18e44f3c8.\n        This func may cause memory overflow as the merge() first creates the mosaiced array in memory!\n\n        Args:\n            outMapFolder (str): the folder where the mosaiced tif will be saved\n            gtifs (list): a list of tile GeoTifs to be mosaiced\n            mosaicTifName (str): the name of the mosaiced GeoTif file\n            keepTifs (bool): whether to keep the tile GeoTifs, default is False\n\n        Return:\n            str: the name of the mosaiced GeoTif.\n    \"\"\"\n\n    # open all the Gtifs\n    ras2Mosaic = []\n    for gtif in gtifs: # assuming no None item in the list\n        gtifFullName = os.path.join(outMapFolder,gtif)\n        ras = rasterio.open(gtifFullName)\n        ras2Mosaic.append(ras)\n\n    # create array representing all source rasters mosaicked together\n    mosaicedArray, output = merge(ras2Mosaic)\n\n    # close all the tifs\n    for ras in ras2Mosaic:\n        ras.close()\n\n    # Prepareto write the array into one tif file\n    outMeta = ras2Mosaic[0].meta.copy()\n    outMeta.update(\n        {\"driver\": \"GTiff\",\n            \"height\": mosaicedArray.shape[1],\n            \"width\": mosaicedArray.shape[2],\n            \"transform\": output,\n        }\n    )\n\n    # write to COG file\n    mosaicTifFullName = os.path.join(outMapFolder,mosaicTifName)\n    with MemoryFile() as memfile:\n        # write the array to a memory file\n        with memfile.open(**outMeta) as mem:\n            # Populate the input file with numpy array\n            mem.write(mosaicedArray)\n        # open the memory file reading\n        with memfile.open(mode='r') as mem:\n            dst_profile = cog_profiles.get(\"deflate\")\n            cog_translate(\n                mem,\n                mosaicTifFullName,\n                dst_profile,\n                in_memory=True,\n                quiet=True,\n            )\n\n    # # save the mosaiced array to a regular GeoTIFF file\n    # mosaicTifFullName = os.path.join(outMapFolder,mosaicTifName)\n    # with rasterio.open(mosaicTifFullName, 'w', **outMeta) as m:\n    #     m.write(mosaicedArray)\n\n    # delete tile maps\n    if not keepTifs: \n        # print('Delete tile maps ...')\n        for tm in gtifs:\n            os.remove(tm)\n\n    return mosaicTifFullName\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MosaicGtifsBlob","title":"<code>MosaicGtifsBlob(mapContClient, gtifs, outGtif, keepTifs=False)</code>","text":"<p>Mosaic a list of GeoTifs into one GeoTif file using rasterio.merge module on Microsoft Planetary Computer using data in Azure Blob Storage. See https://medium.com/spatial-data-science/how-to-mosaic-merge-raster-data-in-python-fb18e44f3c8. This func may cause memory overflow as the merge() first creates the mosaiced array in memory!</p> <p>Parameters:</p> Name Type Description Default <code>mapContClient</code> <code>ContainerClient</code> <p>a ContainerClient object for the container to store the mosaiced tif</p> required <code>gtifs</code> <code>list</code> <p>a list of tile GeoTifs to be mosaiced</p> required <code>outGtif</code> <code>str</code> <p>the name of the mosaiced GeoTif file</p> required <code>keepTifs</code> <code>bool</code> <p>whether to keep the tile GeoTifs, default is False</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>no return.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MosaicGtifsBlob(mapContClient, gtifs, outGtif, keepTifs=False):\n    \"\"\" Mosaic a list of GeoTifs into one GeoTif file using rasterio.merge module on Microsoft Planetary Computer using data in Azure Blob Storage.\n        See https://medium.com/spatial-data-science/how-to-mosaic-merge-raster-data-in-python-fb18e44f3c8.\n        This func may cause memory overflow as the merge() first creates the mosaiced array in memory!\n\n        Args:\n            mapContClient (ContainerClient): a ContainerClient object for the container to store the mosaiced tif\n            gtifs (list): a list of tile GeoTifs to be mosaiced\n            outGtif (str): the name of the mosaiced GeoTif file\n            keepTifs (bool): whether to keep the tile GeoTifs, default is False\n\n        Return:\n            None: no return.\n    \"\"\"\n\n    # open all the Gtifs\n    ras2Mosaic = []\n    for gtif in gtifs: # assuming no None tif file names in the list\n        blob_client = mapContClient.get_blob_client(gtif)\n        # create a SAS token\n        sas_token = azure.storage.blob.generate_blob_sas(\n            mapContClient.account_name,\n            mapContClient.container_name,\n            blob_client.blob_name,\n            account_key=mapContClient.credential.account_key,\n            permission=[\"read\"],\n        )\n        # Note that the container or the blob must have a access level of Anonymous access\n        url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n        ras = rasterio.open(url)\n        ras2Mosaic.append(ras)\n\n    # create array representing all source rasters mosaicked together\n    mosaicedArray, output = merge(ras2Mosaic)\n\n    # close all the tifs\n    for ras in ras2Mosaic:\n        ras.close()\n\n    # Prepareto write the array into one tif file\n    outMeta = ras2Mosaic[0].meta.copy()\n    outMeta.update(\n        {\"driver\": \"GTiff\",\n            \"height\": mosaicedArray.shape[1],\n            \"width\": mosaicedArray.shape[2],\n            \"transform\": output,\n        }\n    )\n\n    # write the array to blob storage as a COG file\n    with MemoryFile() as memfile:\n        # write the array to a memory file\n        with memfile.open(**outMeta) as mem:\n            # Populate the input file with numpy array\n            mem.write(mosaicedArray) # or mem.write(mosaicedArray,1)\n        # open the memory file reading\n        with memfile.open(mode='r') as mem:\n            dst_profile = cog_profiles.get(\"deflate\")\n            # translate the memfile into a COG memfile\n            with MemoryFile() as mem_dst:\n                # Important, we pass `mem_dst.name` as output dataset path\n                cog_translate(mem, mem_dst.name, dst_profile, in_memory=True, quiet=True)\n                # upload the mem file to blob storage\n                blob_client = mapContClient.get_blob_client(outGtif)\n                blob_client.upload_blob(mem_dst, overwrite=True)  \n\n    # #\n    # # original code to save the mosaiced array as a regular GeoTIFF file\n    # #\n    # # Write data to an in-memory io.BytesIO buffer\n    # # open an in-memory buffer\n    # with io.BytesIO() as buffer:\n    #     # write Geotif to the buffer\n    #     with rasterio.open(buffer, 'w', **outMeta) as tifRaster:\n    #         tifRaster.write(mosaicedArray)\n    #     # upload the in-memory tif to blob storage\n    #     buffer.seek(0)\n    #     blob_client = mapContClient.get_blob_client(outGtif)\n    #     blob_client.upload_blob(buffer, overwrite=True)\n\n    # delete tile maps\n    if not keepTifs: \n        # print('Delete tile maps ...')\n        for gtif in gtifs:\n            if not(gtif is None):\n                blob_client = mapContClient.get_blob_client(gtif)\n                blob_client.delete_blob()\n\n    return\n</code></pre>"},{"location":"mapping/#fldpln.mapping.MosaicGtifsUsingVirtualRaster","title":"<code>MosaicGtifsUsingVirtualRaster(gtifs, outGtif)</code>","text":"<p>Mosaic a list of GeoTifs into one GeoTif file using GDAL virtual raster. Easiest way of mosaic very large Gtif. Based on the video at https://www.youtube.com/watch?v=sBBMKbAj8XE</p> <p>Parameters:</p> Name Type Description Default <code>gtifs</code> <code>list</code> <p>a list of tile GeoTifs to be mosaiced</p> required <code>outGtif</code> <code>str</code> <p>the name of the mosaiced GeoTif file</p> required <p>Returns:</p> Type Description <code>None</code> <p>no return</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def MosaicGtifsUsingVirtualRaster(gtifs, outGtif):\n    \"\"\" Mosaic a list of GeoTifs into one GeoTif file using GDAL virtual raster.\n        Easiest way of mosaic very large Gtif. Based on the video at https://www.youtube.com/watch?v=sBBMKbAj8XE\n\n        Args:\n            gtifs (list): a list of tile GeoTifs to be mosaiced\n            outGtif (str): the name of the mosaiced GeoTif file\n\n        Return:\n            None: no return\n    \"\"\"\n\n    from osgeo import gdal # since gdal cannot be installed using pip, we leave the import here and it's up to the user to install gdal.\n\n    # Create a XML-based virtual raster file for mosaicing\n    vrtFolder = os.path.dirname(outGtif)\n    vrtName = os.path.basename(outGtif) + '.vrt'\n    vrtFullName = os.path.join(vrtFolder,vrtName)\n    vrt = gdal.BuildVRT(vrtFullName, gtifs)\n\n    # access meta data from the first gtif\n    ras = rasterio.open(gtifs[0])\n    rasMeta = ras.meta\n    # get cell size in x and y\n    tran = rasMeta['transform'] \n    xCellSize, yCellSize = tran.a, tran.e\n\n    # mosaic the gtifs\n    gdal.Translate(outGtif, vrt, xRes = xCellSize, yRes = yCellSize)\n\n    # clean up\n    vrt = None\n    # delete the VRT file\n    if os.path.isfile(vrtFullName):\n        os.remove(vrtFullName)\n    return\n</code></pre>"},{"location":"mapping/#fldpln.mapping.NearestPoint","title":"<code>NearestPoint(p1df, x1FieldName, y1FieldName, p2df, x2FieldName, y2FieldName, distFieldName='dist', otherColumns=None)</code>","text":"<p>Join two sets of points by nearest distance. The returned data frame will have, in addition to p1df fields, a new distance field (i.e., distFieldName),  plus other fields (i.e., otherColumns) copied from p2df.</p> <p>Parameters:</p> Name Type Description Default <code>p1df</code> <code>data frame</code> <p>the first set of points as a pandas DataFrame</p> required <code>x1FieldName</code> <code>str</code> <p>the field name of y coordinates in p1df</p> required <code>y1FieldName</code> <code>str</code> <p>the field name of y coordinates in p1df</p> required <code>p2df</code> <code>data frame</code> <p>the second set of points as a pandas DataFrame</p> required <code>x2FieldName</code> <code>str</code> <p>the field name of x coordinates in p2df</p> required <code>y2FieldName</code> <code>str</code> <p>the field name y coordinates in p2df</p> required <code>distFieldName</code> <code>str</code> <p>the name of the distance field in the returned data frame, default is 'dist'</p> <code>'dist'</code> <code>otherColumns</code> <code>list</code> <p>the names of other fields to be copied from p2df to the returned data frame, default is None.</p> <code>None</code> Source code in <code>fldpln/mapping.py</code> <pre><code>def NearestPoint(p1df, x1FieldName, y1FieldName, p2df, x2FieldName, y2FieldName, distFieldName='dist',otherColumns=None):\n    \"\"\" Join two sets of points by nearest distance. The returned data frame will have, in addition to p1df fields, a new distance field (i.e., distFieldName), \n        plus other fields (i.e., otherColumns) copied from p2df.\n\n        Args:\n            p1df (data frame): the first set of points as a pandas DataFrame\n            x1FieldName (str): the field name of y coordinates in p1df\n            y1FieldName (str): the field name of y coordinates in p1df\n            p2df (data frame): the second set of points as a pandas DataFrame\n            x2FieldName (str): the field name of x coordinates in p2df\n            y2FieldName (str): the field name y coordinates in p2df\n            distFieldName (str): the name of the distance field in the returned data frame, default is 'dist'\n            otherColumns (list): the names of other fields to be copied from p2df to the returned data frame, default is None.\n\n        Return: \n            data frame: a data frame with the nearest points from p2df for each point in p1df.\n    \"\"\"\n    # # make sure the input DFs have unique index so that deltaX, deltaY and p2df['Dist'] = deltaX*deltaX + deltaY*deltaY can work\n    p1df.reset_index(inplace=True)\n    p2df.reset_index(inplace=True)\n\n    # Create a temp DF for storing the nearest points from p2df\n    if otherColumns is None:\n        cols = [distFieldName]\n    else:\n        cols = [distFieldName] + otherColumns\n    nearestP2Df =pd.DataFrame(columns=cols)\n\n    # find the nearest point from p2df for each point in p1df\n    for row in p1df.itertuples(): # itertuples() is the fastest way of iterating a df\n        # find the nearest pt2\n        idx,x1,y1 = (getattr(row,'Index'),getattr(row,x1FieldName),getattr(row,y1FieldName))\n        deltaX = p2df[x2FieldName]-x1\n        deltaY = p2df[y2FieldName]-y1\n        # calculate distance to all the points in p2df\n        distDf = deltaX*deltaX + deltaY*deltaY\n\n        # get the nearest point's index and dist\n        s = distDf.sort_values().head(1)\n        idxp2, dist = (s.index[0],s.values[0])\n        # update dist\n        dist = sqrt(dist)\n\n        # get additional fields from p2df by index\n        values = [dist] + p2df.loc[idxp2,otherColumns].values.flatten().tolist()\n\n        # add the nearest point to the nearest point DF\n        t = pd.DataFrame([values], index=[idx], columns=cols)\n        # nearestP2Df = nearestP2Df.append(t,ignore_index=False)\n        nearestP2Df = pd.concat([nearestP2Df,t],ignore_index=False)\n\n    # merge p1df points with their nearest points using their index\n    p1df = p1df.merge(nearestP2Df, how = 'left', left_index=True, right_index=True)\n\n    return p1df\n</code></pre>"},{"location":"mapping/#fldpln.mapping.NearestPointInPlace","title":"<code>NearestPointInPlace(p1df, x1FieldName, y1FieldName, p2df, x2FieldName, y2FieldName, distFieldName='dist', otherColumns=None)</code>","text":"<p>Join two sets of points by nearest distance. p1df will have a new distance field (i.e., distFieldName), plus other fields (i.e., otherColumns) copied from p2df. Note that this function changes p1df. This is the only difference between this function and NearestPoint()! This is the only difference between this function and NearestPoint()!</p> <p>Parameters:</p> Name Type Description Default <code>p1df</code> <code>data frame</code> <p>the first set of points as a pandas DataFrame</p> required <code>x1FieldName</code> <code>str</code> <p>the field name of y coordinates in p1df</p> required <code>y1FieldName</code> <code>str</code> <p>the field name of y coordinates in p1df</p> required <code>p2df</code> <code>data frame</code> <p>the second set of points as a pandas DataFrame</p> required <code>x2FieldName</code> <code>str</code> <p>the field name of x coordinates in p2df</p> required <code>y2FieldName</code> <code>str</code> <p>the field name y coordinates in p2df</p> required <code>distFieldName</code> <code>str</code> <p>the name of the distance field in the returned data frame, default is 'dist'</p> <code>'dist'</code> <code>otherColumns</code> <code>list</code> <p>the names of other fields to be copied from p2df to the returned data frame, default is None</p> <code>None</code> <p>Returns:</p> Type Description <code>data frame</code> <p>p1df with the nearest points from p2df.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def NearestPointInPlace(p1df, x1FieldName, y1FieldName, p2df, x2FieldName, y2FieldName, distFieldName='dist', otherColumns=None):\n    \"\"\" Join two sets of points by nearest distance. p1df will have a new distance field (i.e., distFieldName), plus other fields (i.e., otherColumns) copied from p2df.\n        Note that this function changes p1df. This is the only difference between this function and NearestPoint()! This is the only difference between this function and NearestPoint()!\n\n        Args:\n            p1df (data frame): the first set of points as a pandas DataFrame\n            x1FieldName (str): the field name of y coordinates in p1df\n            y1FieldName (str): the field name of y coordinates in p1df\n            p2df (data frame): the second set of points as a pandas DataFrame\n            x2FieldName (str): the field name of x coordinates in p2df\n            y2FieldName (str): the field name y coordinates in p2df\n            distFieldName (str): the name of the distance field in the returned data frame, default is 'dist'\n            otherColumns (list): the names of other fields to be copied from p2df to the returned data frame, default is None\n\n        Return:\n            data frame: p1df with the nearest points from p2df.\n    \"\"\"\n\n    # # make sure the input DFs have unique index so that deltaX, deltaY and p2df['Dist'] = deltaX*deltaX + deltaY*deltaY can work\n    p1df.reset_index(inplace=True)\n    p2df.reset_index(inplace=True)\n\n    # Create a temp DF for storing the nearest points from p2df\n    if otherColumns is None:\n        cols = [distFieldName]\n    else:\n        cols = [distFieldName] + otherColumns\n\n    # find the nearest point from p2df for each point in p1df\n    # find the nearest pt2 for each pt1\n    for idx in p1df.index: \n        # find the nearest point in pt2df\n        x1,y1 = (p1df.at[idx,x1FieldName],p1df.at[idx,y1FieldName])\n        deltaX = p2df[x2FieldName]-x1\n        deltaY = p2df[y2FieldName]-y1\n        # calculate distance to all the points in p2df\n        distDf = deltaX*deltaX + deltaY*deltaY\n\n        # get the nearest point's index and dist\n        s = distDf.sort_values().head(1)\n        idxp2, dist = (s.index[0],s.values[0])\n        # update dist\n        dist = sqrt(dist)\n\n        # get additional fields from p2df by index\n        nearestPt = p2df.loc[idxp2,otherColumns].values.flatten().tolist()\n\n        # add the dist and other fields to p1df\n        p1df.at[idx,cols] = [dist] + nearestPt\n\n    return p1df\n</code></pre>"},{"location":"mapping/#fldpln.mapping.SnapGauges2Fsps","title":"<code>SnapGauges2Fsps(libFolder, libNames, gauges, snapDist=350, gaugeXField='X', gaugeYField='Y', fspColumns=['FspId', 'FspX', 'FspY', 'FilledElev'])</code>","text":"<p>Snap gauges to library FSPs. The function will return a data frame with the nearest FSPs for each gauge in each library.  Note that multiple FSPs from different libraries might be snapped to the same gauge!</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are located</p> required <code>libNames</code> <code>list</code> <p>a list of library names that the gauges will be snapped to</p> required <code>gauges</code> <code>str or data frame</code> <p>a text file or a pandas DF of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system</p> required <code>snapDist</code> <code>float</code> <p>the distance to snap gauges to FSPs, default is 350</p> <code>350</code> <code>gaugeXField</code> <code>str</code> <p>the field name of x coordinates in gauges, default is 'X'</p> <code>'X'</code> <code>gaugeYField</code> <code>str</code> <p>the field name of y coordinates in gauges, default is 'Y'</p> <code>'Y'</code> <code>fspColumns</code> <code>list</code> <p>the names of FSP columns to be returned, default is ['FspId','FspX','FspY','FilledElev']</p> <code>['FspId', 'FspX', 'FspY', 'FilledElev']</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with the nearest FSPs for each gauge in each library.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def SnapGauges2Fsps(libFolder,libNames,gauges,snapDist=350,gaugeXField='X',gaugeYField='Y',fspColumns=['FspId','FspX','FspY','FilledElev']):\n    \"\"\" Snap gauges to library FSPs. The function will return a data frame with the nearest FSPs for each gauge in each library. \n        Note that multiple FSPs from different libraries might be snapped to the same gauge!\n\n        Args:\n            libFolder (str): the folder where the libraries are located\n            libNames (list): a list of library names that the gauges will be snapped to\n            gauges (str or data frame): a text file or a pandas DF of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system\n            snapDist (float): the distance to snap gauges to FSPs, default is 350\n            gaugeXField (str): the field name of x coordinates in gauges, default is 'X'\n            gaugeYField (str): the field name of y coordinates in gauges, default is 'Y'\n            fspColumns (list): the names of FSP columns to be returned, default is ['FspId','FspX','FspY','FilledElev']\n\n        Return:\n            data frame: a data frame with the nearest FSPs for each gauge in each library.\n    \"\"\"\n\n    if isinstance(gauges,(str)):\n        # assume gauges are a text file\n        allGaugesDf = pd.read_csv(gauges,index_col=False)\n\n    if isinstance(gauges, pd.DataFrame):\n        # gauges is a DF\n        allGaugesDf = gauges\n\n    # snape gauges to all the FSPs in each library\n    # initialization\n    snappedGauges = pd.DataFrame()\n    # snap library by library\n    for libName in libNames:\n        # read fsp info csv files\n        fspDf = pd.read_csv(os.path.join(libFolder, libName, fspInfoFileName),index_col=False)[fspColumns]\n\n        # Find FSP extent + snap distance\n        fspMinX = fspDf['FspX'].min()-snapDist # half cell size?\n        fspMaxX = fspDf['FspX'].max()+snapDist\n        fspMinY = fspDf['FspY'].min()-snapDist\n        fspMaxY = fspDf['FspY'].max()+snapDist\n\n        # select the gauges within the FSP extent of the library\n        gaugesDf = allGaugesDf[(allGaugesDf[gaugeXField]&gt;=fspMinX) &amp; (allGaugesDf[gaugeXField]&lt;=fspMaxX) &amp; (allGaugesDf[gaugeYField]&gt;=fspMinY) &amp;(allGaugesDf[gaugeYField]&lt;=fspMaxY)]\n\n        # find the nearest FSP for each gauge\n        distFieldName = 'd2NearestFsp'\n        gaugesDf = NearestPoint(gaugesDf,gaugeXField,gaugeYField,fspDf,'FspX','FspY',distFieldName,fspColumns)\n\n        # select gauges within snap distance\n        gaugesDf = gaugesDf[(gaugesDf[distFieldName]&lt;=snapDist)]\n        # add library name \n        gaugesDf['lib_name'] = libName\n\n        if snappedGauges.empty:\n            snappedGauges = gaugesDf\n        else:\n            snappedGauges = pd.concat([snappedGauges,gaugesDf])\n\n    return snappedGauges\n</code></pre>"},{"location":"mapping/#fldpln.mapping.SnapGauges2FspsBlob","title":"<code>SnapGauges2FspsBlob(libBlobSerClient, libName, gaugesDf, snapDist=350, gaugeIdField='GaugeLID', gaugeXField='X', gaugeYField='Y')</code>","text":"<p>Snap gauges to library FSPs on Microsoft Planetary Computer (MPC) using Azure Blob Storage. The function has NOT been checked yet!</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a blob service client</p> required <code>libName</code> <code>str</code> <p>the name of the library that the gauges will be snapped to</p> required <code>gaugesDf</code> <code>data frame</code> <p>a pandas DF of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system</p> required <code>snapDist</code> <code>float</code> <p>the distance to snap gauges to FSPs, default is 350</p> <code>350</code> <code>gaugeIdField</code> <code>str</code> <p>the field name of gauge IDs in gauges, default is 'GaugeLID'</p> <code>'GaugeLID'</code> <code>gaugeXField</code> <code>str</code> <p>the field name of x coordinates in gauges, default is 'X'</p> <code>'X'</code> <code>gaugeYField</code> <code>str</code> <p>the field name of y coordinates in gauges, default is 'Y'</p> <code>'Y'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with the nearest FSPs for each gauge in the library.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def SnapGauges2FspsBlob(libBlobSerClient,libName,gaugesDf,snapDist=350,gaugeIdField='GaugeLID',gaugeXField='X',gaugeYField='Y'):\n    \"\"\" Snap gauges to library FSPs on Microsoft Planetary Computer (MPC) using Azure Blob Storage. The function has NOT been checked yet!\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a blob service client\n            libName (str): the name of the library that the gauges will be snapped to\n            gaugesDf (data frame): a pandas DF of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system\n            snapDist (float): the distance to snap gauges to FSPs, default is 350\n            gaugeIdField (str): the field name of gauge IDs in gauges, default is 'GaugeLID'\n            gaugeXField (str): the field name of x coordinates in gauges, default is 'X'\n            gaugeYField (str): the field name of y coordinates in gauges, default is 'Y'\n\n        Return:\n            data frame: a data frame with the nearest FSPs for each gauge in the library.\n    \"\"\"\n# gauges -- a pandas DF. It must have the columns of 'X' and 'Y' in FSP's coordinate system\n\n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n\n    # read fsp info csv files\n    blob_client = container_client.get_blob_client(fspInfoFileName)\n    # create a SAS token\n    sas_token = azure.storage.blob.generate_blob_sas(\n        container_client.account_name,\n        container_client.container_name,\n        blob_client.blob_name,\n        account_key=container_client.credential.account_key,\n        permission=[\"read\"],\n    )\n    # construct the URL\n    url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n    # read the blob\n    fspDf = pd.read_csv(url,index_col=False)[['FspX','FspY','FilledElev']]\n\n    # Find FSP's border extent + snap distance\n    fspMinX = fspDf['FspX'].min()-snapDist # half cell size?\n    fspMaxX = fspDf['FspX'].max()+snapDist\n    fspMinY = fspDf['FspY'].min()-snapDist\n    fspMaxY = fspDf['FspY'].max()+snapDist\n\n    # select the gauges within the extent\n    gaugesDf = gaugesDf[(gaugesDf[gaugeXField]&gt;=fspMinX) &amp; (gaugesDf[gaugeXField]&lt;=fspMaxX) &amp; (gaugesDf[gaugeYField]&gt;=fspMinY) &amp;(gaugesDf[gaugeYField]&lt;=fspMaxY)]\n\n    # Create a temp DF for nearest FSPs\n    cols = [gaugeIdField,'FspX','FspY','FspFilledElev','Dist']\n    nearestFspDf =pd.DataFrame(columns=cols)\n\n    # find the nearest FSP for each gauge\n    for row in gaugesDf.itertuples(index=False): # itertuples() is the fastest way of iterating a df\n        # find the nearest FSP\n        glid,x,y = (getattr(row,gaugeIdField),getattr(row,gaugeXField),getattr(row,gaugeYField))\n        deltaX = fspDf['FspX']-x\n        deltaY = fspDf['FspY']-y\n        fspDf['Dist'] = deltaX*deltaX + deltaY*deltaY\n        nearestFsp = fspDf.sort_values('Dist').head(1)\n        fspX,fspY,elev,dist = nearestFsp[['FspX','FspY','FilledElev','Dist']].values.flatten().tolist()\n        dist = sqrt(dist)\n        # add the nearest FSP to the gauge DF\n        t = pd.DataFrame([[glid,fspX,fspY,elev,dist]],columns=cols)\n        # nearestFspDf = nearestFspDf.append(t,ignore_index=True)\n        nearestFspDf = pd.concat([nearestFspDf,t],ignore_index=True)\n\n    # merge gauge and their nearest FSPs\n    gaugesDf = gaugesDf.merge(nearestFspDf, how = 'left', on=gaugeIdField)\n    # select gauges within snap distance\n    snappedGauges = gaugesDf[(gaugesDf['Dist']&lt;=snapDist)]\n\n    return snappedGauges\n</code></pre>"},{"location":"mapping/#fldpln.mapping.SnapGaugesToFsps","title":"<code>SnapGaugesToFsps(libFolder, libName, gauges, snapDist=250, gaugeIdField='GaugeLID', gaugeXField='X', gaugeYField='Y')</code>","text":"<p>Snap gauges to library FSPs. The function will return a data frame with the snapped gauges only.</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are located</p> required <code>libName</code> <code>str</code> <p>the name of the library that the gauges will be snapped to</p> required <code>gauges</code> <code>str or data frame</code> <p>a text file or a pandas data frame of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system</p> required <code>snapDist</code> <code>float</code> <p>the distance to snap gauges to FSPs, default is 250</p> <code>250</code> <code>gaugeIdField</code> <code>str</code> <p>the field name of gauge IDs in gauges, default is 'GaugeLID'</p> <code>'GaugeLID'</code> <code>gaugeXField</code> <code>str</code> <p>the field name of x coordinates in gauges, default is 'X'</p> <code>'X'</code> <code>gaugeYField</code> <code>str</code> <p>the field name of y coordinates in gauges, default is 'Y'</p> <code>'Y'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with the snapped gauges only.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def SnapGaugesToFsps(libFolder,libName,gauges,snapDist=250,gaugeIdField='GaugeLID',gaugeXField='X',gaugeYField='Y'):\n    \"\"\" Snap gauges to library FSPs. The function will return a data frame with the snapped gauges only.\n\n        Args:\n            libFolder (str): the folder where the libraries are located\n            libName (str): the name of the library that the gauges will be snapped to\n            gauges (str or data frame): a text file or a pandas data frame of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system\n            snapDist (float): the distance to snap gauges to FSPs, default is 250\n            gaugeIdField (str): the field name of gauge IDs in gauges, default is 'GaugeLID'\n            gaugeXField (str): the field name of x coordinates in gauges, default is 'X'\n            gaugeYField (str): the field name of y coordinates in gauges, default is 'Y'\n\n        Return:\n            data frame: a data frame with the snapped gauges only.\n    \"\"\"\n\n    if isinstance(gauges,(str)):\n        # assume gauges are a text file\n        gaugesDf = pd.read_csv(gauges,index_col=False)\n\n    if isinstance(gauges, pd.DataFrame):\n        # gauges is a DF\n        gaugesDf = gauges\n\n    # read fsp info csv files\n    fspDf = pd.read_csv(os.path.join(libFolder, libName, fspInfoFileName),index_col=False)[['FspX','FspY','FilledElev']]\n\n    # Find FSP's border extent + snap distance\n    fspMinX = fspDf['FspX'].min()-snapDist # half cell size?\n    fspMaxX = fspDf['FspX'].max()+snapDist\n    fspMinY = fspDf['FspY'].min()-snapDist\n    fspMaxY = fspDf['FspY'].max()+snapDist\n\n    # select the gauges within the extent\n    gaugesDf = gaugesDf[(gaugesDf[gaugeXField]&gt;=fspMinX) &amp; (gaugesDf[gaugeXField]&lt;=fspMaxX) &amp; (gaugesDf[gaugeYField]&gt;=fspMinY) &amp;(gaugesDf[gaugeYField]&lt;=fspMaxY)]\n\n    # Create a temp DF for nearest FSPs\n    cols = [gaugeIdField,'FspX','FspY','FspFilledElev','Dist']\n    nearestFspDf =pd.DataFrame(columns=cols)\n\n    # find the nearest FSP for each gauge\n    for row in gaugesDf.itertuples(index=False): # itertuples() is the fastest way of iterating a df\n        # find the nearest FSP\n        glid,x,y = (getattr(row,gaugeIdField),getattr(row,gaugeXField),getattr(row,gaugeYField))\n        deltaX = fspDf['FspX']-x\n        deltaY = fspDf['FspY']-y\n        fspDf['Dist'] = deltaX*deltaX + deltaY*deltaY\n        nearestFsp = fspDf.sort_values('Dist').head(1)\n        fspX,fspY,elev,dist = nearestFsp[['FspX','FspY','FilledElev','Dist']].values.flatten().tolist()\n        dist = sqrt(dist)\n        # add the nearest FSP to the gauge DF\n        t = pd.DataFrame([[glid,fspX,fspY,elev,dist]],columns=cols)\n        # nearestFspDf = nearestFspDf.append(t,ignore_index=True)\n        nearestFspDf = pd.concat([nearestFspDf,t],ignore_index=True)\n\n    # merge gauge and their nearest FSPs\n    gaugesDf = gaugesDf.merge(nearestFspDf, how = 'left', on=gaugeIdField)\n    # select gauges within snap distance\n    snappedGauges = gaugesDf[(gaugesDf['Dist']&lt;=snapDist)]\n\n    return snappedGauges\n</code></pre>"},{"location":"mapping/#fldpln.mapping.SnapGaugesToFspsBlob","title":"<code>SnapGaugesToFspsBlob(libBlobSerClient, libName, gaugesDf, snapDist=250, gaugeIdField='GaugeLID', gaugeXField='X', gaugeYField='Y')</code>","text":"<p>Snap gauges to library FSPs on Microsoft Planetary Computer (MPC) using Azure Blob Storage. The function is has NOT been checked yet!</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a blob service client</p> required <code>libName</code> <code>str</code> <p>the name of the library that the gauges will be snapped to</p> required <code>gaugesDf</code> <code>data frame</code> <p>a pandas DF of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system</p> required <code>snapDist</code> <code>float</code> <p>the distance to snap gauges to FSPs, default is 250</p> <code>250</code> <code>gaugeIdField</code> <code>str</code> <p>the field name of gauge IDs in gauges, default is 'GaugeLID'</p> <code>'GaugeLID'</code> <code>gaugeXField</code> <code>str</code> <p>the field name of x coordinates in gauges, default is 'X'</p> <code>'X'</code> <code>gaugeYField</code> <code>str</code> <p>the field name of y coordinates in gauges, default is 'Y'</p> <code>'Y'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>a data frame with the snapped gauges only.</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def SnapGaugesToFspsBlob(libBlobSerClient,libName,gaugesDf,snapDist=250,gaugeIdField='GaugeLID',gaugeXField='X',gaugeYField='Y'):\n    \"\"\" Snap gauges to library FSPs on Microsoft Planetary Computer (MPC) using Azure Blob Storage. The function is has NOT been checked yet!\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a blob service client\n            libName (str): the name of the library that the gauges will be snapped to\n            gaugesDf (data frame): a pandas DF of gauges. It must have the columns of 'X' and 'Y' in FSP's coordinate system\n            snapDist (float): the distance to snap gauges to FSPs, default is 250\n            gaugeIdField (str): the field name of gauge IDs in gauges, default is 'GaugeLID'\n            gaugeXField (str): the field name of x coordinates in gauges, default is 'X'\n            gaugeYField (str): the field name of y coordinates in gauges, default is 'Y'\n\n        Return:\n            data frame: a data frame with the snapped gauges only.\n    \"\"\"\n\n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n\n    # read fsp info csv files\n    blob_client = container_client.get_blob_client(fspInfoFileName)\n    # create a SAS token\n    sas_token = azure.storage.blob.generate_blob_sas(\n        container_client.account_name,\n        container_client.container_name,\n        blob_client.blob_name,\n        account_key=container_client.credential.account_key,\n        permission=[\"read\"],\n    )\n    # construct the URL\n    url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n    # read the blob\n    fspDf = pd.read_csv(url,index_col=False)[['FspX','FspY','FilledElev']]\n\n    # Find FSP's border extent + snap distance\n    fspMinX = fspDf['FspX'].min()-snapDist # half cell size?\n    fspMaxX = fspDf['FspX'].max()+snapDist\n    fspMinY = fspDf['FspY'].min()-snapDist\n    fspMaxY = fspDf['FspY'].max()+snapDist\n\n    # select the gauges within the extent\n    gaugesDf = gaugesDf[(gaugesDf[gaugeXField]&gt;=fspMinX) &amp; (gaugesDf[gaugeXField]&lt;=fspMaxX) &amp; (gaugesDf[gaugeYField]&gt;=fspMinY) &amp;(gaugesDf[gaugeYField]&lt;=fspMaxY)]\n\n    # Create a temp DF for nearest FSPs\n    cols = [gaugeIdField,'FspX','FspY','FspFilledElev','Dist']\n    nearestFspDf =pd.DataFrame(columns=cols)\n\n    # find the nearest FSP for each gauge\n    for row in gaugesDf.itertuples(index=False): # itertuples() is the fastest way of iterating a df\n        # find the nearest FSP\n        glid,x,y = (getattr(row,gaugeIdField),getattr(row,gaugeXField),getattr(row,gaugeYField))\n        deltaX = fspDf['FspX']-x\n        deltaY = fspDf['FspY']-y\n        fspDf['Dist'] = deltaX*deltaX + deltaY*deltaY\n        nearestFsp = fspDf.sort_values('Dist').head(1)\n        fspX,fspY,elev,dist = nearestFsp[['FspX','FspY','FilledElev','Dist']].values.flatten().tolist()\n        dist = sqrt(dist)\n        # add the nearest FSP to the gauge DF\n        t = pd.DataFrame([[glid,fspX,fspY,elev,dist]],columns=cols)\n        # nearestFspDf = nearestFspDf.append(t,ignore_index=True)\n        nearestFspDf = pd.concat([nearestFspDf,t],ignore_index=True)\n\n    # merge gauge and their nearest FSPs\n    gaugesDf = gaugesDf.merge(nearestFspDf, how = 'left', on=gaugeIdField)\n    # select gauges within snap distance\n    snappedGauges = gaugesDf[(gaugesDf['Dist']&lt;=snapDist)]\n\n    return snappedGauges\n</code></pre>"},{"location":"mapping/#fldpln.mapping.TileFspFppRelations2Array","title":"<code>TileFspFppRelations2Array(fspFppRels, fppExtent, cellSize, fspDof='MinDtf', aoiExtent=None, noData=-9999)</code>","text":"<p>Turn a dataframe of FSP-FPP relations to a 2D array of flood depth.  The minimum bounding extent of the FPPs in the relations is always used when create the map for the tile!</p> <p>Parameters:</p> Name Type Description Default <code>fspFppRels</code> <code>data frame</code> <p>a dataframe of FSP-FPP relations which have the columns of [\"FspId\", \"FppCol\", \"FppRow\", \"Dtf\", \"FilledDepth\"] from a tile</p> required <code>fppExtent</code> <code>list</code> <p>a list of [minX, maxX, minY, maxY], FPP's external extent of the tile and is also used to locate FPP's columns and rows in map coordinate</p> required <code>cellSize</code> <code>float</code> <p>the cell size of the raster</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'. If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. If it's a float, it's a constant stage for all the FSPs. If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None</p> <code>None</code> <code>noData</code> <code>int</code> <p>the no data value, default is -9999</p> <code>-9999</code> <p>Returns:</p> Type Description <code>tuple</code> <p>a tuple of the np array as the map, the no data value, the minimum X value, and the minimum Y value</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def TileFspFppRelations2Array(fspFppRels, fppExtent, cellSize, fspDof='MinDtf', aoiExtent=None, noData=-9999):\n    \"\"\" Turn a dataframe of FSP-FPP relations to a 2D array of flood depth. \n        The minimum bounding extent of the FPPs in the relations is always used when create the map for the tile!\n\n        Args:\n            fspFppRels (data frame): a dataframe of FSP-FPP relations which have the columns of [\"FspId\", \"FppCol\", \"FppRow\", \"Dtf\", \"FilledDepth\"] from a tile\n            fppExtent (list): a list of [minX, maxX, minY, maxY], FPP's external extent of the tile and is also used to locate FPP's columns and rows in map coordinate\n            cellSize (float): the cell size of the raster\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'.\n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.\n                If it's a float, it's a constant stage for all the FSPs.\n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None\n            noData (int): the no data value, default is -9999\n\n        Return:\n            tuple: a tuple of the np array as the map, the no data value, the minimum X value, and the minimum Y value\n    \"\"\"\n\n    tdf = fspFppRels\n    # print('Number of FSP-FPP relations:', len(tdf))\n    if len(tdf)==0:\n        # no FPP needs to be mapping\n        return None, None, None, None\n\n    # Limit the FPPs in the tile to the AOI extent when provided\n    if not (aoiExtent is None):\n        # Note that the aoiExtent intersects with the tile, otherwise the tile won't be selected for mapping!\n        fppMinX,fppMaxX,fppMinY,fppMaxY = fppExtent\n        aoiMinX,aoiMaxX,aoiMinY,aoiMaxY = aoiExtent\n        # calculate new FPP extent within the fppExtent\n        newFppMinX, newFppMinY= max(fppMinX,aoiMinX), max(fppMinY,aoiMinY)\n        newFppMaxX, newFppMaxY= min(fppMaxX,aoiMaxX), min(fppMaxY,aoiMaxY)\n        # calculate FPP col &amp; row extent for the new FPP extent\n        minFppCol,maxFppCol = int(round((newFppMinX-fppMinX)/cellSize)), int(round((newFppMaxX-fppMinX)/cellSize))-1\n        minFppRow,maxFppRow = int(round((fppMaxY-newFppMaxY)/cellSize)), int(round((fppMaxY-newFppMinY)/cellSize))-1\n        # select the FPPs within the the new fppExtent based on FppCol &amp; FppRow\n        tdf = tdf[(tdf['FppCol']&gt;=minFppCol) &amp; (tdf['FppCol']&lt;=maxFppCol) &amp; (tdf['FppRow']&gt;=minFppRow) &amp; (tdf['FppRow']&lt;=maxFppRow)].copy() # tell pandas we want a copy to avoid \"SettingWithCopyWarning\" in line 459, 465 \n\n        if len(tdf)==0:\n            # no FPP needs to be mapping\n            return None, None, None, None\n\n    #\n    # Calculate pixel values at each FPP based on the types of fspDof: 'MinDtf', 'NumOfFsps', 'Depression', a constant DOF, and a list of DOF\n    # Pixel value at each FPP is saved in the 'Dtf' column.\n    #\n    if isinstance(fspDof,(str)) and fspDof == 'MinDtf':\n        # no FSP DOF is provide, map the minimum DTF at FPPs\n        # print('Map the minimum DTF ...')\n        # calculate the minimum DTF at each FPP\n        tdf = tdf.groupby(['FppCol', 'FppRow'],as_index=False).agg({'Dtf': 'min'}) #,MaxDtf = ('Dtf', max))\n        # print(tdf)\n\n    elif isinstance(fspDof,(str)) and fspDof == 'NumOfFsps':\n        # no FSP DOF is provide, map the number of FSPs associated with each FPP (neighborhood size of each FPP)\n        # print('Map the number of FSPs associated with each FPP ...')\n        tdf = tdf.groupby(['FppCol', 'FppRow'],as_index=False).agg({'Dtf':'count'}) #size() # count the # of FSPs associated with each FPP\n        # tdf.rename(columns={'size':'Depth'},inplace=True) # inplace changing column name\n\n    elif isinstance(fspDof,(str)) and fspDof == 'Depression':\n        # print('Map depression depth ...')\n        # assign 'Dtf' to filled drpression\n        tdf['Dtf'] = tdf['FilledDepth']\n        tdf = tdf.groupby(['FppCol', 'FppRow'],as_index=False).agg({'Dtf':'first'})\n\n    elif isinstance(fspDof, numbers.Number): # constant stage for all the FSPs\n        # all the FSPs in the tile have the same DOF\n        # print(f\"Map a constant FSP DOF of {fspDof} ...\")\n        tdf['Dtf'] = float(fspDof) - tdf['Dtf']\n        tdf = tdf[tdf['Dtf']&gt;0]\n\n        tdf = tdf.groupby(['FppCol', 'FppRow'],as_index=False).agg({'Dtf':'max','FilledDepth':'first'})\n        # add the depth of filled drpression\n        tdf['Dtf'] = tdf['Dtf'] + tdf['FilledDepth']\n\n        # code used to find negative flood depth for library 'midkan'\n        # t = tdf['Dtf']&lt;0\n        # if t.any():\n        #     print('Negative DTF!')\n\n    # Map the tile with a FSP DOF df\n    elif isinstance(fspDof, pd.DataFrame):\n        # print('Map with a list of FSPs with DOFs ...')\n\n        # Only keep those relations whose DTF is less than or equal to the max interpolated DOF. \n        # This significantly saves memory and time when merge the relations with the DOFs!\n        maxDof = fspDof['Dof'].max()\n        tdf = tdf[tdf['Dtf'] &lt; maxDof] # tdf.drop(tdf[tdf['Dtf']&lt;=0].index, inplace=True) # saves memory than tdf = tdf[tdf['Dtf'] &gt; 0]?\n        # print('Number of relations to be mapped: ',len(tdf))\n\n        # set FSP DOF column data types to speed merge\n        fspDof = fspDof.astype(dtype={\"FspId\":np.int32,\"Dof\":np.float32},copy=False)\n\n        # create index to speed up merge\n        # tdf.astype(np.float32,copy=False).set_index(keys=['FspX','FspY'],inplace=True)\n        # fspDof.astype(np.float32,copy=False).set_index(keys=['FspX','FspY'],inplace=True)\n        # tdf = pd.merge(tdf, fspDof, how='inner', left_index=True,right_index=True)\n\n        # map the FPPs whose FSPs' DOF &gt; the MinDOF\n        tdf = pd.merge(tdf, fspDof, how='inner', on=['FspId']) #.astype(np.float32,copy=False)\n\n        # calculate DTF\n        tdf['Dtf'] = tdf['Dof'] - tdf['Dtf']\n        tdf = tdf[tdf['Dtf'] &gt; 0] # tdf.drop(tdf[tdf['Dtf']&lt;=0].index, inplace=True) # saves memory than tdf = tdf[tdf['Dtf'] &gt; 0]?\n\n        tdf = tdf.groupby(['FppCol', 'FppRow'],as_index=False).agg({'Dtf':'max','FilledDepth':'first'}) #Depth = ('Dtf', max),FilledDepth=('FilledDepth',first))\n        # print(tdf)\n        # add the depth of filled drpression\n        tdf['Dtf'] = tdf['Dtf'] + tdf['FilledDepth']\n        # drop 'FilledDepth'\n    else:\n        print(f'Unsupported fspDof type {fspDof}!')\n        return None, None, None, None\n    #\n    # Turn relations into 2D array\n    #\n    if len(tdf)==0:\n        # no FPP needs to be mapping\n        return None, None, None, None\n\n    # drop off not-used columns in the DF\n    tdf = tdf[['FppCol','FppRow','Dtf']]\n    # tdf.drop(columns=['FilledDepth'],axis=1,inplace=True)\n\n    # Determine the minimum map extent to speed up the mapping\n    # original map extent is the FPP's extent\n    mapMinX,mapMaxX,mapMinY,mapMaxY = fppExtent\n\n    # further reduce map extent if FPP extent is reduced\n    if (not (aoiExtent is None)) or isinstance(fspDof,(int, float)) or isinstance(fspDof, pd.DataFrame):\n        # further reduce the map extent with the FPPs \n        mapMinCol,mapMaxCol = tdf['FppCol'].min(),tdf['FppCol'].max()\n        mapMinRow,mapMaxRow = tdf['FppRow'].min(),tdf['FppRow'].max()\n        # shift FPP's cols and rows\n        tdf['FppCol'] = tdf['FppCol']-mapMinCol\n        tdf['FppRow'] = tdf['FppRow']-mapMinRow\n        # calculate map's new extent\n        mapMaxX = mapMinX + (mapMaxCol+1)*cellSize # this line MUST before the next line as the next line changes mapMinX!\n        mapMinX = mapMinX + mapMinCol*cellSize\n        mapMinY = mapMaxY - (mapMaxRow+1)*cellSize # this line MUST before the next line as the next line changes mapMaxY!\n        mapMaxY = mapMaxY - mapMinRow*cellSize\n\n    # print('Map extent (minX, maxX, minY, maxY) :',(mapMinX, mapMaxX, mapMinY, mapMaxY))\n    # Calculate map rows and columns\n    tCols = int(round((mapMaxX-mapMinX)/cellSize))\n    tRows = int(round((mapMaxY-mapMinY)/cellSize))\n    # print(f'Turn FSP-FPP relations to a 2D array of {tRows, tCols} ...')\n\n    # Initialize the array for saving as a raster\n    dtfArray =  np.full(shape=(tRows,tCols),fill_value=noData,dtype=np.float32)\n\n    # # update the array with FPP's DTF\n    for (idx,idy,dtf) in tdf.itertuples(index=False): # itertuples() is the fastest way of iterating a df\n        # idx,idy,dtf = (getattr(row,'FppCol'),getattr(row,'FppRow'),getattr(row,'Dtf')) \n        dtfArray[idy,idx] = dtf\n\n    return dtfArray, noData, mapMinX, mapMaxY\n</code></pre>"},{"location":"mapping/#fldpln.mapping.Tiles2Map","title":"<code>Tiles2Map(libFolder, libName, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Decide the tiles need to be mapped for the library.</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>the folder where the libraries are stored</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'. If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. If it's a float, it's a constant stage for all the FSPs. If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>a list of tile IDs, a list of tile FPP extents</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def Tiles2Map(libFolder,libName,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Decide the tiles need to be mapped for the library.\n\n        Args:\n            libFolder (str): the folder where the libraries are stored\n            libName (str): the name of the library\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'.\n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.\n                If it's a float, it's a constant stage for all the FSPs.\n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None.\n\n        Return:\n            tuple: a list of tile IDs, a list of tile FPP extents\n    \"\"\"\n\n    #\n    # Read in the fsp-tile index and tile index files for selecting tiles for mapping\n    #\n    # read in fsp-tile index file to select the tiles for mapping\n    fspIdxFile = os.path.join(libFolder, libName, fspTileIndexFileName)\n    fspIdxDf = pd.read_csv(fspIdxFile)\n    # print(fspIdxDf)\n\n    # tile index file stores tile and FPP extents for the tile\n    tileIdxFile = os.path.join(libFolder, libName, tileIndexFileName)\n    tileIdxDf = pd.read_csv(tileIdxFile)\n    # print(tileIdxDf)\n\n    #\n    # Select the tiles for mapping based on the FSPs and fsp-tile index\n    #\n    # print('Select the tiles need to be mapped ...')\n    # for 'MinDtf', 'NumOfFsps' and 'Depression'\n    if (isinstance(fspDof,(str)) and fspDof == 'MinDtf') or (isinstance(fspDof,(str)) and fspDof == 'NumOfFsps') or (isinstance(fspDof,(str)) and fspDof == 'Depression'):\n        # map all the tiles\n        # fspTiles = fspIdxDf['TileId'].drop_duplicates().sort_values().tolist()\n        # use tileIdxDf is faster!\n        fspTiles = tileIdxDf['TileId'].sort_values().tolist()\n        # same as: fspTiles = tileIdxDf['TileId'].sort_values().to_list()\n\n    # for constant FSP DOF\n    elif isinstance(fspDof, numbers.Number): # constant stage for all the FSPs\n        # find which tiles need to be mapped\n        # select those tiles whose FSP's DOF &gt; MinDtf\n        fspTiles = fspIdxDf[float(fspDof)&gt;fspIdxDf['MinDtf']]\n        # print(fspTiles)\n        # find the tiles need to be mapped\n        # fspTiles = fspTiles['TileId'].unique()\n        fspTiles = fspTiles['TileId'].drop_duplicates().sort_values().tolist()\n\n    # for a dataframe of FSPs\n    elif isinstance(fspDof, pd.DataFrame):\n        # find which tiles need to be mapped\n        fspTiles = pd.merge(fspIdxDf, fspDof, how='inner', on=['FspId'])\n        # print(fspTiles)\n        # select those where DOF &gt; minDtf\n        fspTiles = fspTiles[fspTiles['Dof']&gt;fspTiles['MinDtf']]\n        # print(fspTiles)\n        # find the tiles need to be mapped\n        # fspTiles = fspTiles['TileId'].unique()\n        fspTiles = fspTiles['TileId'].drop_duplicates().sort_values().tolist()\n    else:\n        print(f'Unsupported fspDof type {fspDof}!')\n        return\n\n    # tiles selected based on the FSPs\n    # print('Tiles selected based on FSPs: ', fspTiles)\n\n    # further limit the tiles to those that intersect with the AOI extent\n    if aoiExtent is None:\n        tiles = fspTiles\n    else:\n        # select the tiles intersecting AOI extent based on the FPPs' extent of the tile\n        aoiMinX,aoiMaxX,aoiMinY,aoiMaxY = aoiExtent\n        aoiTiles = tileIdxDf[~((tileIdxDf['FppMinX']&gt;aoiMaxX) | (tileIdxDf['FppMaxX']&lt;aoiMinX))] # rectangles are NOT on left or right of each other\n        aoiTiles = aoiTiles[~((aoiTiles['FppMinY']&gt;aoiMaxY) | (aoiTiles['FppMaxY']&lt;aoiMinY))] # rectangles are NOT on top or bottom of each other\n        aoiTiles = aoiTiles['TileId'].drop_duplicates().sort_values().tolist()\n        print('Tiles selected based on AOI extent: ', aoiTiles)\n        # intersect the lists\n        tiles = list(set(fspTiles) &amp; set(aoiTiles))\n\n    # tiles selected\n    if len(tiles) == 0:\n        # print('No tile needs to be mapped!')\n        return None,None\n    else:\n        # get each tile's fppExtent\n        fppExtents=[]\n        for tid in tiles:\n            fppExtent = tileIdxDf[tileIdxDf['TileId']==tid].reset_index().loc[0,['FppMinX','FppMaxX','FppMinY','FppMaxY']].values.tolist()\n            fppExtents.append(fppExtent)\n        return tiles,fppExtents\n</code></pre>"},{"location":"mapping/#fldpln.mapping.Tiles2MapBlob","title":"<code>Tiles2MapBlob(libBlobSerClient, libName, fspDof='MinDtf', aoiExtent=None)</code>","text":"<p>Decide the tiles need to be mapped for the library on Microsoft Planetary Computer using Azure Blob Storage.</p> <p>Parameters:</p> Name Type Description Default <code>libBlobSerClient</code> <code>BlobServiceClient</code> <p>a BlobServiceClient object</p> required <code>libName</code> <code>str</code> <p>the name of the library</p> required <code>fspDof</code> <code>str, float, or data frame</code> <p>the FSP DOF for mapping flood depth. default is 'MinDtf'. If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'. If it's a float, it's a constant stage for all the FSPs. If it's a data frame, it's a data frame of FSPs with DOF.</p> <code>'MinDtf'</code> <code>aoiExtent</code> <code>list</code> <p>the extent of the area of interest [minX,maxX,minY,maxY]. default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>a list of tile IDs, a list of tile FPP extents</p> Source code in <code>fldpln/mapping.py</code> <pre><code>def Tiles2MapBlob(libBlobSerClient,libName,fspDof='MinDtf',aoiExtent=None):\n    \"\"\" Decide the tiles need to be mapped for the library on Microsoft Planetary Computer using Azure Blob Storage.\n\n        Args:\n            libBlobSerClient (BlobServiceClient): a BlobServiceClient object\n            libName (str): the name of the library\n            fspDof (str, float, or data frame): the FSP DOF for mapping flood depth. default is 'MinDtf'.\n                If it's a string, it can be 'MinDtf', 'NumOfFsps', or 'Depression'.\n                If it's a float, it's a constant stage for all the FSPs.\n                If it's a data frame, it's a data frame of FSPs with DOF.\n            aoiExtent (list): the extent of the area of interest [minX,maxX,minY,maxY]. default is None.\n\n        Return:\n            tuple: a list of tile IDs, a list of tile FPP extents\n    \"\"\"\n\n    #\n    # Read in the fsp-tile index and tile index files for selecting tiles for ampping\n    #\n    # create a container client, assuming the container already exists\n    container_client = libBlobSerClient.get_container_client(container=libName)\n\n    # read in fsp-tile index file to select the tiles for mapping \n    # get blob client\n    blob_client = container_client.get_blob_client(fspTileIndexFileName)\n    # create a SAS token\n    sas_token = azure.storage.blob.generate_blob_sas(\n        container_client.account_name,\n        container_client.container_name,\n        blob_client.blob_name,\n        account_key=container_client.credential.account_key,\n        permission=[\"read\"],\n    )\n    # construct the URL\n    url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n    # read the blob\n    fspIdxDf = pd.read_csv(url)\n    # print(fspIdxDf)\n\n    # tile index file stores tile and FPP extents for the tile\n    # get blob client\n    blob_client = container_client.get_blob_client(tileIndexFileName)\n    # create a SAS token\n    sas_token = azure.storage.blob.generate_blob_sas(\n        container_client.account_name,\n        container_client.container_name,\n        blob_client.blob_name,\n        account_key=container_client.credential.account_key,\n        permission=[\"read\"],\n    )\n    # construct the URL\n    url = blob_client.url + \"?\" + urllib.parse.quote_plus(sas_token)\n    # read the blob\n    tileIdxDf = pd.read_csv(url)\n    # print(tileIdxDf)\n\n    #\n    # Select the tiles for mapping based on the FSPs and fsp-tile index\n    #\n    # print('Select the tiles need to be mapped ...')\n    # for 'MinDtf', 'NumOfFsps' and 'Depression'\n    if (isinstance(fspDof,(str)) and fspDof == 'MinDtf') or (isinstance(fspDof,(str)) and fspDof == 'NumOfFsps') or (isinstance(fspDof,(str)) and fspDof == 'Depression'):\n        # map all the tiles\n        # fspTiles = fspIdxDf['TileId'].drop_duplicates().sort_values().tolist()\n        # use tileIdxDf is faster!\n        fspTiles = tileIdxDf['TileId'].sort_values().tolist()\n        # same as: fspTiles = tileIdxDf['TileId'].sort_values().to_list()\n\n    # for constant FSP DOF\n    elif isinstance(fspDof, numbers.Number): # constant stage for all the FSPs\n        # find which tiles need to be mapped\n        # select those tiles whose FSP's DOF &gt; MinDtf\n        fspTiles = fspIdxDf[float(fspDof)&gt;fspIdxDf['MinDtf']]\n        # print(fspTiles)\n        # find the tiles need to be mapped\n        # fspTiles = fspTiles['TileId'].unique()\n        fspTiles = fspTiles['TileId'].drop_duplicates().sort_values().tolist()\n\n    # for a dataframe of FSPs\n    elif isinstance(fspDof, pd.DataFrame):\n        # find which tiles need to be mapped\n        fspTiles = pd.merge(fspIdxDf, fspDof, how='inner', on=['FspId'])\n        # print(fspTiles)\n        # select those where DOF &gt; minDtf\n        fspTiles = fspTiles[fspTiles['Dof']&gt;fspTiles['MinDtf']]\n        # print(fspTiles)\n        # find the tiles need to be mapped\n        # fspTiles = fspTiles['TileId'].unique()\n        fspTiles = fspTiles['TileId'].drop_duplicates().sort_values().tolist()\n    else:\n        print(f'Unsupported fspDof type {fspDof}!')\n        return\n\n    # tiles selected based on the FSPs\n    # print('Tiles selected based on FSPs: ', fspTiles)\n\n    # further limit the tiles to those that interset with the AOI extent\n    if aoiExtent is None:\n        tiles = fspTiles\n    else:\n        # select the tiles intersecting AOI extent based on the FPPs' extent of the tile\n        aoiMinX,aoiMaxX,aoiMinY,aoiMaxY = aoiExtent\n        aoiTiles = tileIdxDf[~((tileIdxDf['FppMinX']&gt;aoiMaxX) | (tileIdxDf['FppMaxX']&lt;aoiMinX))] # rectangles are NOT on left or right of each other\n        aoiTiles = aoiTiles[~((aoiTiles['FppMinY']&gt;aoiMaxY) | (aoiTiles['FppMaxY']&lt;aoiMinY))] # rectangles are NOT on top or bottom of each other\n        aoiTiles = aoiTiles['TileId'].drop_duplicates().sort_values().tolist()\n        print('Tiles selected based on AOI extent: ', aoiTiles)\n        # intersect the lists\n        tiles = list(set(fspTiles) &amp; set(aoiTiles))\n\n    # tiles selected\n    if len(tiles) == 0:\n        # print('No tile needs to be mapped!')\n        return None,None\n    else:\n        # get each tile's fppExtent\n        fppExtents=[]\n        for tid in tiles:\n            fppExtent = tileIdxDf[tileIdxDf['TileId']==tid].reset_index().loc[0,['FppMinX','FppMaxX','FppMinY','FppMaxY']].values.tolist()\n            fppExtents.append(fppExtent)\n        return tiles,fppExtents\n</code></pre>"},{"location":"model/","title":"model module","text":"<p>The FLDPLN model module. This module implements the FLDPLN singleton class which exposes the MATLAB functions in the fldpln_py package/library created by MATLAB.  In essence, the FLDPLN class hides the conversion from Python variables to MATLAB data types in those functions. </p>"},{"location":"model/#fldpln.model.FLDPLN","title":"<code> FLDPLN        </code>","text":"<p>A singleton class for using the fldpln_py Python package generated in MATLAB.</p> Source code in <code>fldpln/model.py</code> <pre><code>class FLDPLN:\n    \"\"\" A singleton class for using the fldpln_py Python package generated in MATLAB.\n    \"\"\"\n\n    # implement the FLDPLN class as a Singleton class, i.e., only one instance for the class\n    # see https://www.geeksforgeeks.org/singleton-pattern-in-python-a-complete-guide/\n    def __new__(cls):\n        # import FLDPLN python package created by MATLAB\n        import fldpln_py\n\n        if not hasattr(cls, 'instance'):\n            # first (and ONLY instance)\n            cls.instance = super(FLDPLN, cls).__new__(cls) \n\n            # initialize the fldpln_py library when the only instance is created\n            print('Initialize FLDPLN model python package ...')\n            try:\n                cls.instance.fldpln_py_handle = fldpln_py.initialize()\n                print('Done!')\n\n            except Exception as e:\n                print('Error initializing fldpln_py package\\\\n:{}'.format(e))\n                exit(1)\n\n        return cls.instance\n\n    # Calling destructor to terminate the reference to the library\n    def __del__(self):\n\n        print('Terminate FLDPLN model python package.')\n        self.fldpln_py_handle.terminate()\n\n    # Initialize the library\n    # def Initialize(self):\n    #     # Initialize MATLAB fldpln_py library and retrieve a handle to it\n    #     print('Initialize the fldpln_py library ...')\n    #     try:\n    #         self.fldpln_py_handle = fldpln_py.initialize()\n\n    #     except Exception as e:\n    #         print('Error initializing fldpln_py package\\\\n:{}'.format(e))\n    #         exit(1)\n\n    # Generate stream segments for building library\n    def GenerateSegments(self,fdrf, facf, strfac, segfac, seglen, segdir):\n        \"\"\" Generate stream segments for building segment-based FLDPLN library\n            Args:\n                fdrf (str): Flow direction BIL file path\n                facf (str): Flow accumulation BIL file path\n                strfac (int): Stream flow accumulation threshold (in sq. miles) for identifying stream networks\n                segfac (int): Stream flow accumulation threshold (in sq. miles) used for creating segments along stream networks\n                seglen (int): Segment length threshold (in miles) used for creating segments along stream networks\n                segdir (str): Output directory for segment files\n\n            Returns:\n                None: No return value\n        \"\"\"\n\n        import matlab\n\n        try:\n            # generate stream segments\n            fdrfIn = fdrf\n            facfIn = facf\n            strfacIn = matlab.double([strfac], size=(1, 1))\n            segfacIn = matlab.double([segfac], size=(1, 1))\n            seglenIn = matlab.double([seglen], size=(1, 1))\n            segdirIn = segdir\n\n            print('Generate segments ...')\n            self.fldpln_py_handle.rp_generate_segments(fdrfIn, facfIn, strfacIn, segfacIn, seglenIn, segdirIn, nargout=0)\n            print('Done!')\n\n        except Exception as e:\n            print('Error occurred during program execution\\\\n:{}'.format(e))\n\n    # Write segment and FSP as CSV files\n    def WriteSegmentFspCsvFiles(self,bildir, segdir, seg_list, outdir=None, fileType='csv'):\n        \"\"\" Write segment and FSP as CSV files for viewing or creating segment shapefile\n            Args:\n                bildir (str): BIL file directory\n                segdir (str): Segment file directory\n                seg_list (list): List of integer segment IDs to be exported. If empty, all segments will be exported\n                outdir (str): Output directory for the CSV files. If None, the output will be saved in the segment file directory\n                fileType (str): FSP output file type. Choose from {'mat', 'csv'}. default is 'csv'\n\n            Returns:\n                None: No return value\n        \"\"\"\n\n        import matlab\n\n        try:\n            # write FSP and segment info CSV files\n            bildirIn = bildir\n            segdirIn = segdir\n\n            # handle segment list\n            segLstLen = len(seg_list)\n            if segLstLen == 0: # empty list, all segments will be exported\n                seg_listIn = matlab.double([], size=(0, 0)) \n            else:\n                seg_listIn = matlab.double(seg_list, size=(segLstLen, 1))\n\n            # prepare output folder\n            if outdir is None:\n                outdirIn = segdirIn\n            else:\n                outdirIn = outdir\n\n            print('Write FSP and segment files ...')\n            self.fldpln_py_handle.ut_write_fsp_segment_csv_files(bildirIn, segdirIn, seg_listIn, outdirIn, fileType, nargout=0)\n            print('Done!')\n\n        except Exception as e:\n            print('Error occurred during program execution\\\\n:{}'.format(e))\n\n    # Create FLDPLN segment-based library\n    def CreateSegmentLibrary(self,bildir, segdir, filmskfile, segshpfile, fldmn, fldmx, dh, mxht, libdir, mtype, para):\n        \"\"\" Create segment-based FLDPLN library\n            Args:\n                bildir (str): BIL file directory\n                segdir (str): Segment file directory\n                filmskfile (str): Spatial mask BIL file path used to limit the modeling. If no mask, set to ''\n                segshpfile (str): A shapefile that contains the segments to be used in the library. If all segments are used, set to ''\n                segshpfile (dict): Dictionary containing the shapefile information\n                    file (str): Shapefile path that contains the select subset of segments, set to '' if all segments are used\n                    segid_field (str): Field name in the shapefile that contains the segment ID\n                    seg_fldmx_field (str): Field name in the shapefile that contains the fldmx value. Set to '' if all segments use the same fldmx\n                fldmn (float): Minimum flood stage assumed, typically set to 1 centimeter or 0.0328084 foot depends on DEM's vertical unit\n                fldmx (float): Maximum stage modeled\n                dh (float): Vertical step size in DEM's vertical unit\n                mxht (float): max dem+flood height to cease flooding. Usually set 0 for no cap height\n                libdir (str): Output directory for the segment-based library\n                mtype (str): FLDPLN model type. Choose from {'hd', 'ram0', 'ram'}\n                para (dict): Dictionary containing parallelization settings when running the model.\n                    type (str): Parallelization type. Choose from {'none', 'parfor', 'parfeval'}\n                    numcores (int): Number of cores to use. Set to 0 to use all available cores\n                    worker_type (str): Type of workers, 'Processes' or 'Threads'. Default is 'Processes' as MATLAB Runtime does not support 'Threads'\n\n            Returns:\n                None: No return value\n        \"\"\"\n\n        import matlab\n\n        try:\n            bildirIn = bildir\n            segdirIn = segdir\n            filmskfileIn = filmskfile\n            segshpfileIn = segshpfile\n            fldmnIn = matlab.double([fldmn], size=(1, 1))\n            fldmxIn = matlab.double([fldmx], size=(1, 1))\n            dhIn = matlab.double([dh], size=(1, 1))\n            mxhtIn = matlab.double([mxht], size=(1, 1)) # default 0\n            libdirIn = libdir\n\n            # model types: hd, ram0, ram\n            mtypeIn = mtype\n\n            # parallelization settings\n            paraIn = para\n            if 'numcores' in para.keys():\n                # convert user input of core number to MATLAB data type, and keep other string parameters\n                # paraIn = {\"type\": para['type'], \"numcores\": matlab.double([para['numcores']], size=(1, 1))}\n                paraIn[\"numcores\"] = matlab.double([para['numcores']], size=(1, 1))\n\n            # create library\n            print('Create segment-based library ...')\n            self.fldpln_py_handle.rp_create_segment_library_v8(bildirIn, segdirIn, filmskfileIn, segshpfileIn, fldmnIn, fldmxIn, dhIn, mxhtIn, libdirIn, mtypeIn, paraIn, nargout=0)\n            print('Done!')\n\n        except Exception as e:\n            print('Error occurred during program execution\\\\n:{}'.format(e))\n\n    # Reformat segment-based library for tiling and mapping\n    def FormatSegmentLibrary(self,bildir, segdir, libdir, dirout):\n        \"\"\" Reformat segment-based library for tiling and mapping\n\n            Args:\n                bildir (str): BIL file directory\n                segdir (str): Segment file directory\n                libdir (str): Raw segment-based library directory\n                dirout (str): Output directory for the reformatted library\n\n            Returns:\n                None: No return value\n        \"\"\"\n\n        try:\n            bildirIn = bildir\n            segdirIn = segdir\n            libdirIn = libdir\n            diroutIn = dirout\n\n            print('Format segment-based library ...')\n            self.fldpln_py_handle.rp_format_segment_library(bildirIn, segdirIn, libdirIn, diroutIn, nargout=0)\n            print('Done!')\n\n        except Exception as e:\n            print('Error occurred during program execution\\\\n:{}'.format(e))\n\n    def GenerateStreamOrder(self, bildir, segdir, segshp):\n        \"\"\" Generate stream order for the segments\n\n            Args:\n                bildir (str): BIL file directory\n                segdir (str): Segment file directory\n                segshp (str): Selected segment shapefile\n\n            Returns:\n                None: No return value\n        \"\"\"\n\n        try:\n            bildirIn = bildir\n            segdirIn = segdir\n            segshpIn = segshp\n\n            print('Generate stream order ...')\n            self.fldpln_py_handle.rp_generate_stream_order(bildirIn, segdirIn, segshpIn, nargout=0)\n            print('Done!')\n\n        except Exception as e:\n            print('Error occurred during program execution\\\\n:{}'.format(e))\n\n\n    # # Terminate the library\n    # def Terminate(self):\n    #     print('Terminate the fldpln_py library.')\n    #     self.fldpln_py_handle.terminate()\n</code></pre>"},{"location":"model/#fldpln.model.FLDPLN.CreateSegmentLibrary","title":"<code>CreateSegmentLibrary(self, bildir, segdir, filmskfile, segshpfile, fldmn, fldmx, dh, mxht, libdir, mtype, para)</code>","text":"<p>Create segment-based FLDPLN library</p> <p>Parameters:</p> Name Type Description Default <code>bildir</code> <code>str</code> <p>BIL file directory</p> required <code>segdir</code> <code>str</code> <p>Segment file directory</p> required <code>filmskfile</code> <code>str</code> <p>Spatial mask BIL file path used to limit the modeling. If no mask, set to ''</p> required <code>segshpfile</code> <code>str</code> <p>A shapefile that contains the segments to be used in the library. If all segments are used, set to ''</p> required <code>segshpfile</code> <code>dict</code> <p>Dictionary containing the shapefile information file (str): Shapefile path that contains the select subset of segments, set to '' if all segments are used segid_field (str): Field name in the shapefile that contains the segment ID seg_fldmx_field (str): Field name in the shapefile that contains the fldmx value. Set to '' if all segments use the same fldmx</p> required <code>fldmn</code> <code>float</code> <p>Minimum flood stage assumed, typically set to 1 centimeter or 0.0328084 foot depends on DEM's vertical unit</p> required <code>fldmx</code> <code>float</code> <p>Maximum stage modeled</p> required <code>dh</code> <code>float</code> <p>Vertical step size in DEM's vertical unit</p> required <code>mxht</code> <code>float</code> <p>max dem+flood height to cease flooding. Usually set 0 for no cap height</p> required <code>libdir</code> <code>str</code> <p>Output directory for the segment-based library</p> required <code>mtype</code> <code>str</code> <p>FLDPLN model type. Choose from {'hd', 'ram0', 'ram'}</p> required <code>para</code> <code>dict</code> <p>Dictionary containing parallelization settings when running the model. type (str): Parallelization type. Choose from {'none', 'parfor', 'parfeval'} numcores (int): Number of cores to use. Set to 0 to use all available cores worker_type (str): Type of workers, 'Processes' or 'Threads'. Default is 'Processes' as MATLAB Runtime does not support 'Threads'</p> required <p>Returns:</p> Type Description <code>None</code> <p>No return value</p> Source code in <code>fldpln/model.py</code> <pre><code>def CreateSegmentLibrary(self,bildir, segdir, filmskfile, segshpfile, fldmn, fldmx, dh, mxht, libdir, mtype, para):\n    \"\"\" Create segment-based FLDPLN library\n        Args:\n            bildir (str): BIL file directory\n            segdir (str): Segment file directory\n            filmskfile (str): Spatial mask BIL file path used to limit the modeling. If no mask, set to ''\n            segshpfile (str): A shapefile that contains the segments to be used in the library. If all segments are used, set to ''\n            segshpfile (dict): Dictionary containing the shapefile information\n                file (str): Shapefile path that contains the select subset of segments, set to '' if all segments are used\n                segid_field (str): Field name in the shapefile that contains the segment ID\n                seg_fldmx_field (str): Field name in the shapefile that contains the fldmx value. Set to '' if all segments use the same fldmx\n            fldmn (float): Minimum flood stage assumed, typically set to 1 centimeter or 0.0328084 foot depends on DEM's vertical unit\n            fldmx (float): Maximum stage modeled\n            dh (float): Vertical step size in DEM's vertical unit\n            mxht (float): max dem+flood height to cease flooding. Usually set 0 for no cap height\n            libdir (str): Output directory for the segment-based library\n            mtype (str): FLDPLN model type. Choose from {'hd', 'ram0', 'ram'}\n            para (dict): Dictionary containing parallelization settings when running the model.\n                type (str): Parallelization type. Choose from {'none', 'parfor', 'parfeval'}\n                numcores (int): Number of cores to use. Set to 0 to use all available cores\n                worker_type (str): Type of workers, 'Processes' or 'Threads'. Default is 'Processes' as MATLAB Runtime does not support 'Threads'\n\n        Returns:\n            None: No return value\n    \"\"\"\n\n    import matlab\n\n    try:\n        bildirIn = bildir\n        segdirIn = segdir\n        filmskfileIn = filmskfile\n        segshpfileIn = segshpfile\n        fldmnIn = matlab.double([fldmn], size=(1, 1))\n        fldmxIn = matlab.double([fldmx], size=(1, 1))\n        dhIn = matlab.double([dh], size=(1, 1))\n        mxhtIn = matlab.double([mxht], size=(1, 1)) # default 0\n        libdirIn = libdir\n\n        # model types: hd, ram0, ram\n        mtypeIn = mtype\n\n        # parallelization settings\n        paraIn = para\n        if 'numcores' in para.keys():\n            # convert user input of core number to MATLAB data type, and keep other string parameters\n            # paraIn = {\"type\": para['type'], \"numcores\": matlab.double([para['numcores']], size=(1, 1))}\n            paraIn[\"numcores\"] = matlab.double([para['numcores']], size=(1, 1))\n\n        # create library\n        print('Create segment-based library ...')\n        self.fldpln_py_handle.rp_create_segment_library_v8(bildirIn, segdirIn, filmskfileIn, segshpfileIn, fldmnIn, fldmxIn, dhIn, mxhtIn, libdirIn, mtypeIn, paraIn, nargout=0)\n        print('Done!')\n\n    except Exception as e:\n        print('Error occurred during program execution\\\\n:{}'.format(e))\n</code></pre>"},{"location":"model/#fldpln.model.FLDPLN.FormatSegmentLibrary","title":"<code>FormatSegmentLibrary(self, bildir, segdir, libdir, dirout)</code>","text":"<p>Reformat segment-based library for tiling and mapping</p> <p>Parameters:</p> Name Type Description Default <code>bildir</code> <code>str</code> <p>BIL file directory</p> required <code>segdir</code> <code>str</code> <p>Segment file directory</p> required <code>libdir</code> <code>str</code> <p>Raw segment-based library directory</p> required <code>dirout</code> <code>str</code> <p>Output directory for the reformatted library</p> required <p>Returns:</p> Type Description <code>None</code> <p>No return value</p> Source code in <code>fldpln/model.py</code> <pre><code>def FormatSegmentLibrary(self,bildir, segdir, libdir, dirout):\n    \"\"\" Reformat segment-based library for tiling and mapping\n\n        Args:\n            bildir (str): BIL file directory\n            segdir (str): Segment file directory\n            libdir (str): Raw segment-based library directory\n            dirout (str): Output directory for the reformatted library\n\n        Returns:\n            None: No return value\n    \"\"\"\n\n    try:\n        bildirIn = bildir\n        segdirIn = segdir\n        libdirIn = libdir\n        diroutIn = dirout\n\n        print('Format segment-based library ...')\n        self.fldpln_py_handle.rp_format_segment_library(bildirIn, segdirIn, libdirIn, diroutIn, nargout=0)\n        print('Done!')\n\n    except Exception as e:\n        print('Error occurred during program execution\\\\n:{}'.format(e))\n</code></pre>"},{"location":"model/#fldpln.model.FLDPLN.GenerateSegments","title":"<code>GenerateSegments(self, fdrf, facf, strfac, segfac, seglen, segdir)</code>","text":"<p>Generate stream segments for building segment-based FLDPLN library</p> <p>Parameters:</p> Name Type Description Default <code>fdrf</code> <code>str</code> <p>Flow direction BIL file path</p> required <code>facf</code> <code>str</code> <p>Flow accumulation BIL file path</p> required <code>strfac</code> <code>int</code> <p>Stream flow accumulation threshold (in sq. miles) for identifying stream networks</p> required <code>segfac</code> <code>int</code> <p>Stream flow accumulation threshold (in sq. miles) used for creating segments along stream networks</p> required <code>seglen</code> <code>int</code> <p>Segment length threshold (in miles) used for creating segments along stream networks</p> required <code>segdir</code> <code>str</code> <p>Output directory for segment files</p> required <p>Returns:</p> Type Description <code>None</code> <p>No return value</p> Source code in <code>fldpln/model.py</code> <pre><code>def GenerateSegments(self,fdrf, facf, strfac, segfac, seglen, segdir):\n    \"\"\" Generate stream segments for building segment-based FLDPLN library\n        Args:\n            fdrf (str): Flow direction BIL file path\n            facf (str): Flow accumulation BIL file path\n            strfac (int): Stream flow accumulation threshold (in sq. miles) for identifying stream networks\n            segfac (int): Stream flow accumulation threshold (in sq. miles) used for creating segments along stream networks\n            seglen (int): Segment length threshold (in miles) used for creating segments along stream networks\n            segdir (str): Output directory for segment files\n\n        Returns:\n            None: No return value\n    \"\"\"\n\n    import matlab\n\n    try:\n        # generate stream segments\n        fdrfIn = fdrf\n        facfIn = facf\n        strfacIn = matlab.double([strfac], size=(1, 1))\n        segfacIn = matlab.double([segfac], size=(1, 1))\n        seglenIn = matlab.double([seglen], size=(1, 1))\n        segdirIn = segdir\n\n        print('Generate segments ...')\n        self.fldpln_py_handle.rp_generate_segments(fdrfIn, facfIn, strfacIn, segfacIn, seglenIn, segdirIn, nargout=0)\n        print('Done!')\n\n    except Exception as e:\n        print('Error occurred during program execution\\\\n:{}'.format(e))\n</code></pre>"},{"location":"model/#fldpln.model.FLDPLN.GenerateStreamOrder","title":"<code>GenerateStreamOrder(self, bildir, segdir, segshp)</code>","text":"<p>Generate stream order for the segments</p> <p>Parameters:</p> Name Type Description Default <code>bildir</code> <code>str</code> <p>BIL file directory</p> required <code>segdir</code> <code>str</code> <p>Segment file directory</p> required <code>segshp</code> <code>str</code> <p>Selected segment shapefile</p> required <p>Returns:</p> Type Description <code>None</code> <p>No return value</p> Source code in <code>fldpln/model.py</code> <pre><code>def GenerateStreamOrder(self, bildir, segdir, segshp):\n    \"\"\" Generate stream order for the segments\n\n        Args:\n            bildir (str): BIL file directory\n            segdir (str): Segment file directory\n            segshp (str): Selected segment shapefile\n\n        Returns:\n            None: No return value\n    \"\"\"\n\n    try:\n        bildirIn = bildir\n        segdirIn = segdir\n        segshpIn = segshp\n\n        print('Generate stream order ...')\n        self.fldpln_py_handle.rp_generate_stream_order(bildirIn, segdirIn, segshpIn, nargout=0)\n        print('Done!')\n\n    except Exception as e:\n        print('Error occurred during program execution\\\\n:{}'.format(e))\n</code></pre>"},{"location":"model/#fldpln.model.FLDPLN.WriteSegmentFspCsvFiles","title":"<code>WriteSegmentFspCsvFiles(self, bildir, segdir, seg_list, outdir=None, fileType='csv')</code>","text":"<p>Write segment and FSP as CSV files for viewing or creating segment shapefile</p> <p>Parameters:</p> Name Type Description Default <code>bildir</code> <code>str</code> <p>BIL file directory</p> required <code>segdir</code> <code>str</code> <p>Segment file directory</p> required <code>seg_list</code> <code>list</code> <p>List of integer segment IDs to be exported. If empty, all segments will be exported</p> required <code>outdir</code> <code>str</code> <p>Output directory for the CSV files. If None, the output will be saved in the segment file directory</p> <code>None</code> <code>fileType</code> <code>str</code> <p>FSP output file type. Choose from {'mat', 'csv'}. default is 'csv'</p> <code>'csv'</code> <p>Returns:</p> Type Description <code>None</code> <p>No return value</p> Source code in <code>fldpln/model.py</code> <pre><code>def WriteSegmentFspCsvFiles(self,bildir, segdir, seg_list, outdir=None, fileType='csv'):\n    \"\"\" Write segment and FSP as CSV files for viewing or creating segment shapefile\n        Args:\n            bildir (str): BIL file directory\n            segdir (str): Segment file directory\n            seg_list (list): List of integer segment IDs to be exported. If empty, all segments will be exported\n            outdir (str): Output directory for the CSV files. If None, the output will be saved in the segment file directory\n            fileType (str): FSP output file type. Choose from {'mat', 'csv'}. default is 'csv'\n\n        Returns:\n            None: No return value\n    \"\"\"\n\n    import matlab\n\n    try:\n        # write FSP and segment info CSV files\n        bildirIn = bildir\n        segdirIn = segdir\n\n        # handle segment list\n        segLstLen = len(seg_list)\n        if segLstLen == 0: # empty list, all segments will be exported\n            seg_listIn = matlab.double([], size=(0, 0)) \n        else:\n            seg_listIn = matlab.double(seg_list, size=(segLstLen, 1))\n\n        # prepare output folder\n        if outdir is None:\n            outdirIn = segdirIn\n        else:\n            outdirIn = outdir\n\n        print('Write FSP and segment files ...')\n        self.fldpln_py_handle.ut_write_fsp_segment_csv_files(bildirIn, segdirIn, seg_listIn, outdirIn, fileType, nargout=0)\n        print('Done!')\n\n    except Exception as e:\n        print('Error occurred during program execution\\\\n:{}'.format(e))\n</code></pre>"},{"location":"model/#fldpln.model.FLDPLN.__new__","title":"<code>__new__(cls)</code>  <code>special</code> <code>staticmethod</code>","text":"<p>Create and return a new object.  See help(type) for accurate signature.</p> Source code in <code>fldpln/model.py</code> <pre><code>def __new__(cls):\n    # import FLDPLN python package created by MATLAB\n    import fldpln_py\n\n    if not hasattr(cls, 'instance'):\n        # first (and ONLY instance)\n        cls.instance = super(FLDPLN, cls).__new__(cls) \n\n        # initialize the fldpln_py library when the only instance is created\n        print('Initialize FLDPLN model python package ...')\n        try:\n            cls.instance.fldpln_py_handle = fldpln_py.initialize()\n            print('Done!')\n\n        except Exception as e:\n            print('Error initializing fldpln_py package\\\\n:{}'.format(e))\n            exit(1)\n\n    return cls.instance\n</code></pre>"},{"location":"tile/","title":"tile module","text":"<p>Module to re-organize FLDPLN segment-based library into tile-based library for fast mapping.</p>"},{"location":"tile/#fldpln.tile.CalculateFspSegmentDownstreamDistance","title":"<code>CalculateFspSegmentDownstreamDistance(libFolder, libName)</code>","text":"<p>Cleanup segments (some segments don't exist in FSPs) and save library FSP and segment information as two csv files (fsp_info.csv &amp; segment_info.csv).  It reads in the SpatialReference.prj and save it in CellSizeSpatialReference.json. For stage interpolation, it also calculates FSP and segment  downstream distance (i.e., distance to library outlet(s)) which involves:</p> <pre><code>1. Calculate FSP's within-segment downstream distance\n2. Calculate segment length which is more accurate than \"CellCount\" * cell size\n3. Calculate segment's downstream distance (to watershed outlet) for speeding up \n4. Calculate FSP's downstream distance\n</code></pre> <p>Note that FSPs and segments are based on raster cell centers. Segment and its downstream segment has a gap (1 cell or sqrt(2) cell).</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>folder containing the library.</p> required <code>libName</code> <code>str</code> <p>library name.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>FSP data frame. segment data frame.</p> Source code in <code>fldpln/tile.py</code> <pre><code>def CalculateFspSegmentDownstreamDistance(libFolder,libName):\n    \"\"\" Cleanup segments (some segments don't exist in FSPs) and save library FSP and segment information as two csv files (fsp_info.csv &amp; segment_info.csv). \n        It reads in the SpatialReference.prj and save it in CellSizeSpatialReference.json. For stage interpolation, it also calculates FSP and segment \n        downstream distance (i.e., distance to library outlet(s)) which involves:\n\n            1. Calculate FSP's within-segment downstream distance\n            2. Calculate segment length which is more accurate than \"CellCount\" * cell size\n            3. Calculate segment's downstream distance (to watershed outlet) for speeding up \n            4. Calculate FSP's downstream distance\n\n        Note that FSPs and segments are based on raster cell centers. Segment and its downstream segment has a gap (1 cell or sqrt(2) cell).\n\n        Args:\n            libFolder (str): folder containing the library.\n            libName (str): library name.\n\n        Return:\n            tuple: FSP data frame. segment data frame.\n    \"\"\"\n\n    #\n    # read in fsp (flood source pixel) and segment network info Excel files\n    #\n    # fspInfoColumnNames = ['FspX','FspY','SegId','FilledElev'], columns 'DsDist' will be calculated by this function\n    # segInfoColumnNames = ['SegId','CellCount','DsSegId', 'StFac','EdFac'], columns 'Length','DsDist' will be added by this function\n    fspInfoFile = os.path.join(libFolder, libName, fspInfoFileName)\n    segInfoFile = os.path.join(libFolder, libName, segInfoFileName)\n\n\n    # read in FSP ID and coordinates\n    # need to set float_precision='round_trip' to prevent rounding while reading the text file! float_precision='high' DOESN'T work.\n    # For Verdigris 10-m library, FSP ID of 22246, its FspX of -1003.7918248322967 in fsp_info.csv was read into memory as -1003.7918248322968 without using float_precision='round_trip'\n    fspDf = pd.read_csv(fspInfoFile,float_precision='round_trip',index_col=False)\n    segDf = pd.read_csv(segInfoFile,float_precision='round_trip',index_col=False)\n\n    #\n    # Clean up the segment table.\n    # 1. Remove the segment if it's not in the FSP table\n    # 2. If the missing segment is the downstream segment of another segment, set it as 0. \n    # Those missing segments are usually because of they are close to or in waterbodies. \n    # By removing those segment, a library may have several separate watersheds/outlets!\n    #\n    # get the segment IDs\n    segIds = segDf['SegId'].to_list()\n    for sid in segIds:\n        fsps = fspDf[fspDf['SegId']==sid]\n        if len(fsps)==0:\n            # segment not found in the FSP table. delete the row\n            segDf = segDf.loc[segDf['SegId']!=sid]\n            # set downstream segment ID to 0\n            segDf.loc[segDf['DsSegId']==sid,'DsSegId'] = 0\n\n    #\n    # Calculate FSP within-segment distance, segment length, and segment dowstream distance, and FSP downstream distance\n    #\n    # add field for FSP within-segment distance\n    fspDf['DsDist'] = 0.0\n    # add field for segment length\n    segDf['Length'] = 0.0\n\n    # Calculate FSP within-segment DOWNSTREAM distance and segment length\n    for segIdx, row in segDf.iterrows():\n        segID = row['SegId']\n        # print(segID)\n\n        # select FSP on the segment\n        fsps = fspDf[fspDf['SegId']==segID][['FspX','FspY']]\n\n        # calculate fsp downstream within segment length\n        segDist = 0.0\n        if len(fsps)==0:\n            # this should not happen as we have already clean up the segment table!\n            print(f\"Segment {segID} is missing in {fspInfoFileName}!\")\n        else:  \n            first=True\n            for idx, row in fsps[::-1].iterrows():\n                # Note the idx in fsps is the index in fspDf!!!\n                # calculate distance\n                if first:\n                    fspx1, fspy1 = row['FspX'], row['FspY']\n                    dist=0.0\n                    first=False\n                else:\n                    fspx2, fspy2 = row['FspX'], row['FspY']\n                    dist=math.sqrt((fspx1-fspx2)**2+(fspy1-fspy2)**2)\n                    fspx1, fspy1 = fspx2, fspy2\n                segDist += dist\n                fspDf.at[idx,'DsDist']=segDist\n\n        # update segment distance in segDf\n        segDf.at[segIdx,'Length'] = segDist\n\n    # show the DFs\n    # print(fspDf[:1136])\n    # print(segDf)\n\n    #\n    # Calculate segment downstream length\n    #\n    # only for the segments that exist in the FSP table\n    # But this not necessary as segments don't exist in FSP table already removed\n    # Also this line will remove the segment which has just one FSP!\n    # segDf = segDf[segDf['Length']&gt;0]\n\n    # add field for segment downstream distance for speeding up calculating FSP downstream distance\n    segDf['DsDist'] = 0.0\n    for segIdx, row in segDf.iterrows():\n        segID = row['SegId']\n        dsSegID = row['DsSegId']\n\n        dsDist = 0.0\n        while dsSegID != 0:\n            # get downstream segment length and ID\n            tempDf = segDf[segDf['SegId']==dsSegID][['Length','DsSegId']]\n            length, segID_ds = tempDf.iat[0,0], tempDf.iat[0,1]\n            dsDist += length\n\n            # There is a GAP between two segments as they are consisted of FSP cell centers\n            # Calculate the GAP and add it to segment downstream dist\n            # last fsp in upstream segment\n            lastFspXy = fspDf[fspDf['SegId']==segID][['FspX','FspY']].tail(1)    \n            fspx1, fspy1 = lastFspXy.iat[0,0],lastFspXy.iat[0,1]\n            # first FSP in downstream segment\n            firstFspXy = fspDf[fspDf['SegId']==dsSegID][['FspX','FspY']].head(1)\n            fspx2, fspy2 = firstFspXy.iat[0,0], firstFspXy.iat[0,1]\n            dist=math.sqrt((fspx1-fspx2)**2+(fspy1-fspy2)**2)\n            dsDist += dist\n\n            # move to ownstream segment\n            segID = dsSegID\n            dsSegID = segID_ds\n\n        segDf.at[segIdx,'DsDist'] = dsDist\n    # print(segDf)\n\n    # Calculate FSP downstream distance\n    for idx, row in fspDf.iterrows():\n        segID = row['SegId']\n        inSegDist = row['DsDist']\n\n        # get segment downstream distance\n        tempDf = segDf[segDf['SegId']==segID][['DsDist']]\n        segDsDist = tempDf.iat[0,0]\n\n        # reset FSP downstream distance\n        fspDf.at[idx,'DsDist'] = inSegDist + segDsDist\n    # print(fspDf)\n\n    # save the updated info files\n    fspDf.to_csv(fspInfoFile,index=False,mode='w+')\n    segDf.to_csv(segInfoFile,index=False,mode='w+')\n\n    return fspDf, segDf\n</code></pre>"},{"location":"tile/#fldpln.tile.CalculateLibraryExtent","title":"<code>CalculateLibraryExtent(segLibFolder, cellSize)</code>","text":"<p>Calculate library external border extent. Also calculate segment extents (FPP cell center) and save them in a data frame.</p> <p>Parameters:</p> Name Type Description Default <code>segLibFolder</code> <code>str</code> <p>folder containing the segment-based library.</p> required <code>cellSize</code> <code>float</code> <p>cell size in meters.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>external border extent (minX, maxX,minY, maxY), data frame of segment extent of ['MinX','MaxX','MinY','MaxY','FileName'] defined by FPP cell center.</p> Source code in <code>fldpln/tile.py</code> <pre><code>def CalculateLibraryExtent(segLibFolder, cellSize):\n    \"\"\" Calculate library external border extent. Also calculate segment extents (FPP cell center) and save them in a data frame.\n        Args:\n            segLibFolder (str): folder containing the segment-based library.\n            cellSize (float): cell size in meters.\n        Return:\n            tuple: external border extent (minX, maxX,minY, maxY), data frame of segment extent of ['MinX','MaxX','MinY','MaxY','FileName'] defined by FPP cell center.\n    \"\"\"\n\n    # Get all the segment mat files in the library/watershed\n    segMatFileFullNames = glob.glob(os.path.join(segLibFolder, segMatFileMainName+'*.mat'))\n    hcs = cellSize/2\n\n    # initialize extent\n    minX,maxX,minY,maxY = (math.inf,-math.inf,math.inf,-math.inf)\n    relNum = 0 # number of FSP-FPP relation\n    segExts = pd.DataFrame(columns=['MinX','MaxX','MinY','MaxY','FileName']) # empty df storing each segment's FPP extent and corresponding mat file name\n    # update the extent by all the segments\n    print('Calculate library extent ...')\n    for mf in segMatFileFullNames:\n        # read mat file\n        matVar = ReadMatFile(mf,matRelVarName)\n        # converting array to a dataframe\n        df = pd.DataFrame(matVar, columns=matRelColumnNames)\n\n        relNum += len(df)\n        # print('Segment:', segId, 'Number of FSP-FPP relations:', len(df))\n\n        # Calculate segment external border extent\n        sminX = df['floodplain_pixel_x'].min()-hcs\n        sminY = df['floodplain_pixel_y'].min()-hcs\n        smaxX = df['floodplain_pixel_x'].max()+hcs\n        smaxY = df['floodplain_pixel_y'].max()+hcs\n        segExt= pd.DataFrame([[sminX,smaxX,sminY,smaxY,mf]],columns=['MinX','MaxX','MinY','MaxY','FileName'])\n        # add the segment extent to the df\n        # segExts = segExts.append(segExt)\n        segExts = pd.concat([segExts,segExt])\n\n        #update library extent\n        if sminX &lt; minX: minX = sminX\n        if sminY &lt; minY: minY = sminY\n        if smaxX &gt; maxX: maxX = smaxX\n        if smaxY &gt; maxY: maxY = smaxY\n\n    print('Library external border extent (minX, maxX, minY, maxY) :',(minX, maxX,minY, maxY))\n    # print('Library segment FPP extents:\\n', segExts)\n    print('Total number of FSP-FPP relations:', relNum)\n\n    return (minX, maxX,minY, maxY), segExts \n</code></pre>"},{"location":"tile/#fldpln.tile.CalculateTileBoundary","title":"<code>CalculateTileBoundary(minX, maxX, minY, maxY, tileSizeX, tileSizeY, padding=True)</code>","text":"<p>Calculate each tile's boundary as (minX, maxX,minY, maxY).</p> <p>Parameters:</p> Name Type Description Default <code>minX</code> <code>float</code> <p>min x of the external border (not the cell center) coordinates of the area needs to be tiled.</p> required <code>maxX</code> <code>float</code> <p>max x of the external border (not the cell center) coordinates of the area needs to be tiled.</p> required <code>minY</code> <code>float</code> <p>min y of the external border (not the cell center) coordinates of the area needs to be tiled.</p> required <code>maxY</code> <code>float</code> <p>max y of the external border (not the cell center) coordinates of the area needs to be tiled.</p> required <code>tileSizeX</code> <code>float</code> <p>tile external border size (not the cell center size) in x axis.</p> required <code>tileSizeY</code> <code>float</code> <p>tile external border size (not the cell center size) in y axis.</p> required <code>padding</code> <code>bool</code> <p>whether the tiles have the same size and not reduced to the border of the tiled area. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>list of tile boundaries of (minX, maxX,minY, maxY)</p> Source code in <code>fldpln/tile.py</code> <pre><code>def CalculateTileBoundary(minX, maxX, minY, maxY, tileSizeX, tileSizeY, padding=True):\n    \"\"\" Calculate each tile's boundary as (minX, maxX,minY, maxY).\n        Args:\n            minX (float): min x of the external border (not the cell center) coordinates of the area needs to be tiled.\n            maxX (float): max x of the external border (not the cell center) coordinates of the area needs to be tiled.\n            minY (float): min y of the external border (not the cell center) coordinates of the area needs to be tiled.\n            maxY (float): max y of the external border (not the cell center) coordinates of the area needs to be tiled.\n            tileSizeX (float): tile external border size (not the cell center size) in x axis.\n            tileSizeY (float): tile external border size (not the cell center size) in y axis.\n            padding (bool): whether the tiles have the same size and not reduced to the border of the tiled area. Default is True.\n        Return:\n            list: list of tile boundaries of (minX, maxX,minY, maxY)\n    \"\"\"\n\n    # helper function to generate tile boundary in one dimension\n    def TileInOneDimension(min, max, tileSize, padding=True):\n        # generate tile marker coordinates\n        bs = np.arange(min, max, tileSize)\n        # handle the last coordinate as np.arange() is inclusive\n        if padding: #all the tiles have the same length whether it's outside the border of the tiled area or not\n            bs = np.append(bs, bs[-1]+tileSize)\n        else: # the last tile stops at the border of the area\n            bs = np.append(bs, max)\n\n        # generate tile border coordinate pairs\n        tb = [(bs[t], bs[t+1]) for t in range(bs.size-1)]\n        return tb\n\n    xb = TileInOneDimension(minX,maxX,tileSizeX,padding)\n    yb = TileInOneDimension(minY,maxY,tileSizeY,padding)\n    tb = [(x, y) for x in xb for y in yb] # in ((minX, maxX),(minY, maxY))\n    # convert to (minX, maxX,minY, maxY)\n    tb = [(minX, maxX,minY, maxY) for ((minX,maxX),(minY,maxY)) in tb]\n    return tb\n</code></pre>"},{"location":"tile/#fldpln.tile.GenerateSegmentShapefilesFromFspSegmentInfoFiles","title":"<code>GenerateSegmentShapefilesFromFspSegmentInfoFiles(segInfoFile, fspInfoFile, crs, outShpFile)</code>","text":"<p>Generate segment shapefiles from FSP and segment info files.</p> <p>Parameters:</p> Name Type Description Default <code>segInfoFile</code> <code>str</code> <p>segment info file.</p> required <code>fspInfoFile</code> <code>str</code> <p>FSP info file.</p> required <code>crs</code> <code>str</code> <p>coordinate reference system.</p> required <code>outShpFile</code> <code>str</code> <p>output shapefile.</p> required Source code in <code>fldpln/tile.py</code> <pre><code>def GenerateSegmentShapefilesFromFspSegmentInfoFiles(segInfoFile, fspInfoFile, crs, outShpFile):\n    \"\"\" Generate segment shapefiles from FSP and segment info files.\n\n        Args:\n            segInfoFile (str): segment info file.\n            fspInfoFile (str): FSP info file.\n            crs (str): coordinate reference system.\n            outShpFile (str): output shapefile.\n\n        Return: \n            None\n     \"\"\"\n\n    # read in FSP ID and coordinates\n    # need to set float_precision='round_trip' to prevent rounding while reading the text file! float_precision='high' DOESN'T work.\n    # For Verdigris 10-m library, FSP ID of 22246, its FspX of -1003.7918248322967 in fsp_info.csv was read into memory as -1003.7918248322968 without using float_precision='round_trip'\n    # read column names\n    segColNames = pd.read_csv(segInfoFile, index_col=0, nrows=0).columns.tolist()\n    # read in all the segments\n    segDf = pd.read_csv(segInfoFile,float_precision='round_trip',index_col=False)\n\n    # read in FSP's SegId and its coordinates\n    fspDf = pd.read_csv(fspInfoFile,float_precision='round_trip',index_col=False)[['SegId','FspX','FspY']]\n\n    # Create segment geometry list using FSP coordinates on a segment\n    segGeometry = []\n    for row in segDf.itertuples(): # itertuples() is the fastest way of iterating a df\n        segID,dsSegID = (getattr(row,'SegId'),getattr(row,'DsSegId'))\n\n        # select FSP on the segment\n        fsps = fspDf[fspDf['SegId']==segID][['FspX','FspY']]\n\n        # There is a GAP between two segments as they are consisted of FSP cell centers\n        # Upstream sgements are EXTENDED to the first FSP of the downstream segment!\n        if dsSegID != 0:\n            # first FSP in downstream segment\n            firstFspXy = fspDf[fspDf['SegId']==dsSegID][['FspX','FspY']].head(1)\n            # append to the fsps\n            # fsps = fsps.append(firstFspXy)\n            fsps = pd.concat([fsps,firstFspXy])\n        # print(fsps)\n\n        # turn the FSPs into a LineString\n        # create Points, a GeometryArray, from fsps\n        points = gpd.points_from_xy(fsps['FspX'],fsps['FspY'])\n        segLineStr = LineString(points)\n        # print(segLineStr)\n\n        # Insert the line into the geometry list\n        segGeometry.append(segLineStr)\n\n    # create a geodataframe for writing shapefile\n    libSegs = gpd.GeoDataFrame(segDf, crs=crs, geometry=segGeometry)\n\n    # Write the data into that Shapefile\n    schema = gpd.io.file.infer_schema(libSegs)\n    for c in segColNames:\n        schema['properties'][c] = segColSchema[c]\n    libSegs.to_file(outShpFile, driver= \"ESRI Shapefile\",schema=schema) # existing shapefile will be replaced automatically!!!\n\n    return #libSegs\n</code></pre>"},{"location":"tile/#fldpln.tile.GetStreamOrdersForFspsSegments","title":"<code>GetStreamOrdersForFspsSegments(libFolder, strOrdShpFile, shpSegIdName, shpStrOrdColName)</code>","text":"<p>Get stream order for FSPs and segments from segment stream order shapefile and save them in fsp_info.csv and segment_info.csv files.  It also creates file stream_order_info.csv which stores the network info at the level of stream orders for FSP DOF interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>libFolder</code> <code>str</code> <p>library folder.</p> required <code>strOrdShpFile</code> <code>str</code> <p>stream order shapefile.</p> required <code>shpSegIdName</code> <code>str</code> <p>segment ID column name in the shapefile.</p> required <code>shpStrOrdColName</code> <code>str</code> <p>stream order column name in the shapefile.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>FSP data frame, segment data frame, stream order network data frame.</p> Source code in <code>fldpln/tile.py</code> <pre><code>def GetStreamOrdersForFspsSegments(libFolder,strOrdShpFile,shpSegIdName,shpStrOrdColName):\n    \"\"\" Get stream order for FSPs and segments from segment stream order shapefile and save them in fsp_info.csv and segment_info.csv files. \n        It also creates file stream_order_info.csv which stores the network info at the level of stream orders for FSP DOF interpolation.\n\n        Args:\n            libFolder (str): library folder.\n            strOrdShpFile (str): stream order shapefile.\n            shpSegIdName (str): segment ID column name in the shapefile.\n            shpStrOrdColName (str): stream order column name in the shapefile.\n\n        Return:\n            tuple: FSP data frame, segment data frame, stream order network data frame.\n    \"\"\"\n\n    # get stream order from the shapefile\n    shpDf = gpd.read_file(strOrdShpFile)\n\n    # select columns\n    streamOrderColumns = [shpSegIdName,shpStrOrdColName]\n    segOrdDf = shpDf[streamOrderColumns]\n    # rename shp order column to 'StrOrd'\n    segOrdDf = segOrdDf.rename(columns={shpSegIdName: 'SegId', shpStrOrdColName: strOrdColName})\n    # print(segOrdDf)\n\n    # read fsp and segment info csv files\n    fspCsvFile = os.path.join(libFolder,fspInfoFileName)\n    segCsvFile = os.path.join(libFolder,segInfoFileName)\n    # need to set float_precision='round_trip' to prevent rounding while reading the text file! float_precision='high' DOESN'T work.\n    # For Verdigris 10-m library, FSP ID of 22246, its FspX of -1003.7918248322967 in fsp_info.csv was read into memory as -1003.7918248322968 without using float_precision='round_trip'\n    fspDf = pd.read_csv(fspCsvFile,float_precision='round_trip',index_col=False)\n    segDf = pd.read_csv(segCsvFile,float_precision='round_trip',index_col=False)\n\n    # remove existing \"StrOrd\" column\n    if ('StrOrd' in fspDf.columns):\n        fspDf.drop(['StrOrd'], axis=1,inplace=True)\n    if ('StrOrd' in segDf.columns):\n        segDf.drop(['StrOrd'], axis=1,inplace=True)\n\n    # Get segment stream order by merging DFs based on segment ID\n    segDf = pd.merge(segDf, segOrdDf, how='left', on='SegId')\n    # print(segDf)\n\n    # Get FSP's stream order by merging\n    fspDf = pd.merge(fspDf, segOrdDf, how='left', on='SegId')\n    # print(segDf)\n\n    # save the DFs in CSVs\n    fspDf.to_csv(fspCsvFile,index=False,mode='w+')\n    segDf.to_csv(segCsvFile,index=False,mode='w+')\n\n    #\n    # Create another table storing stream order network information with the columns:\n    # [\u2018StrOrd\u2019, \u2018DsStrOrd\u2019, \u2018JunctionFspX\u2019, \u2018JunctionFspY\u2019] for use in interpolting FSP DOF\n    #\n    strOrdDf = pd.DataFrame(columns=strOrdNetColumnNames)\n    strOrds = segDf['StrOrd'].drop_duplicates().sort_values().to_list()\n    for so in strOrds:\n        # find the most downstream segment in the stream order\n        mostDsSeg = segDf[segDf['StrOrd']==so].sort_values('DsDist')[['DsSegId']].head(1)\n        # print(mostDsSeg)\n        # get its downstream segment ID\n        dsSegId = mostDsSeg.iat[0,0]\n        if dsSegId != 0:\n            # get downstream stream order\n            dsOrd = segDf[segDf['SegId']==dsSegId]['StrOrd'].iat[0]\n            # get the first FSP in the downstream segment\n            firstFspXy = fspDf[fspDf['SegId']==dsSegId][['FspX','FspY']].head(1)\n            fspx, fspy = firstFspXy.iat[0,0], firstFspXy.iat[0,1]\n        else:\n            # no downstream segment\n            dsOrd,fspx,fspy = 0,0,0\n\n        # add the connectivity information to the table\n        temp = pd.DataFrame([[so,dsOrd,fspx,fspy]],columns=strOrdNetColumnNames)    \n        # append to the index table\n        # strOrdDf = strOrdDf.append(temp,ignore_index=True)\n        strOrdDf = pd.concat([strOrdDf, temp],ignore_index=True)\n\n    # save the table as a CSV file\n    strOrdCsvFile = os.path.join(libFolder, strOrdNetFileName)\n    strOrdDf.to_csv(strOrdCsvFile,index=False)\n\n    return fspDf, segDf, strOrdDf\n</code></pre>"},{"location":"tile/#fldpln.tile.ReadMatFile","title":"<code>ReadMatFile(matFile, varName)</code>","text":"<p>Read matlab files with different versions. scipy.io DOES NOT support MATLAB files version 7.3 yet! Some of the libraries are in 7.3 while the others are not.</p> <p>Parameters:</p> Name Type Description Default <code>matFile</code> <code>str</code> <p>matlab file name.</p> required <code>varName</code> <code>str</code> <p>variable name in the matlab file.</p> required <p>Returns:</p> Type Description <code>data frame</code> <p>variable matrix in the matlab file.</p> Source code in <code>fldpln/tile.py</code> <pre><code>def ReadMatFile(matFile, varName):\n    \"\"\" Read matlab files with different versions. scipy.io DOES NOT support MATLAB files version 7.3 yet! Some of the libraries are in 7.3 while the others are not.\n        Args:\n            matFile (str): matlab file name.\n            varName (str): variable name in the matlab file.\n        Return:\n            data frame: variable matrix in the matlab file.\n    \"\"\"\n\n    try: \n        # load the segment FSP-FPP-DTF table in mat file as &lt; 7.3. \n        vars = sio.loadmat(matFile)\n        var = vars[varName]\n    except NotImplementedError:\n        vars = {}\n        f = h5py.File(matFile,'r')\n        for k, v in f.items():\n            vars[k] = np.array(v)\n        # get the variable and transpose it for dataframe!\n        var = vars[varName].transpose()\n    except:\n        ValueError('Could not read the mat file at all...')\n        var = None\n\n    return var\n</code></pre>"},{"location":"tile/#fldpln.tile.TileLibrary","title":"<code>TileLibrary(segLibFolder, cellSize, tiledLibFolder, tileSize, fileFormat)</code>","text":"<p>Tile a library. Turn segment-based FSP-FPP relations to tile-based. Note that 'snappy' format needs to install the 'fastparquet' python package</p> <p>Parameters:</p> Name Type Description Default <code>segLibFolder</code> <code>str</code> <p>folder containing the segment-based library.</p> required <code>cellSize</code> <code>float</code> <p>cell size in meters.</p> required <code>tiledLibFolder</code> <code>str</code> <p>folder for the tiled library.</p> required <code>tileSize</code> <code>int</code> <p>number of cells in a tile.</p> required <code>fileFormat</code> <code>str</code> <p>'snappy' or 'mat'.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>metadata of the tiled library</p> Source code in <code>fldpln/tile.py</code> <pre><code>def TileLibrary(segLibFolder, cellSize, tiledLibFolder, tileSize, fileFormat):\n    \"\"\" Tile a library. Turn segment-based FSP-FPP relations to tile-based. Note that 'snappy' format needs to install the 'fastparquet' python package\n\n        Args:\n            segLibFolder (str): folder containing the segment-based library.\n            cellSize (float): cell size in meters.\n            tiledLibFolder (str): folder for the tiled library.\n            tileSize (int): number of cells in a tile.\n            fileFormat (str): 'snappy' or 'mat'.\n\n        Return:\n            dict: metadata of the tiled library\n    \"\"\"\n    #     Parameters:\n    #         segLibFolder: folder containing the segment-based library\n    #         cellSize: cell size in meters\n    #         tiledLibFolder: folder for the tiled library\n    #         tileSize: number of cells in a tile\n    #         fileFormat: 'snappy' or 'mat'. 'snappy' format needs to install the 'fastparquet' python package\n    #     Return: metadata of the tiled library\n    # \"\"\"\n    # This function uses the fsp_info.csv file (under tiledLibFolder) to get FSP IDs\n    # fileFormat: 'snappy' or 'mat'. 'snappy' format needs to install the 'fastparquet' python package\n    # tileSize changed to the number of cells on May 27, 2024 to avoid partial cells within a tile and also works for GCS system\n\n    # create the tiledLibFolder folder for all tiled libraries if it doesn't exist\n    os.makedirs(tiledLibFolder,exist_ok=True)\n\n    # \n    # copy FSP and segment info CSV files to tiled library folder\n    # \n    shutil.copyfile(os.path.join(segLibFolder, fspInfoFileName), os.path.join(tiledLibFolder, fspInfoFileName))\n    shutil.copyfile(os.path.join(segLibFolder, segInfoFileName), os.path.join(tiledLibFolder, segInfoFileName))\n\n    #\n    # save tile size, cell size and spatial reference in a metadata file \n    #\n    # read in spatial reference for the library\n    srFile = os.path.join(segLibFolder,prjFileName)\n    with open(srFile, 'r') as srf:\n        srText = srf.read()\n    metaData = {'TileSize': tileSize, 'CellSize': cellSize, 'SpatialReference': srText}\n    # save metedata\n    with open(os.path.join(tiledLibFolder, metaDataFileName), 'w') as jf:\n        json.dump(metaData,jf)\n\n    #\n    # calculate library and segment extents\n    libExt, segExts = CalculateLibraryExtent(segLibFolder,cellSize)\n    minX,maxX,minY,maxY = libExt\n\n    # Calculate tile boundaries\n    tb = CalculateTileBoundary(minX,maxX,minY,maxY,tileSize*cellSize,tileSize*cellSize)\n    print('Number of (possible) tiles:', len(tb))\n    print('Tile extents:\\n', tb, '\\n')\n\n    #\n    # Create tiles\n    #\n    print('Build tiles (tiling FSP-FPP relations) ...')\n    hcs = cellSize/2\n    # read in FSP ID and coordinates\n    # need to set float_precision='round_trip' to prevent rounding while reading the text file! float_precision='high' DOESN'T work.\n    # For Verdigris 10-m library, FSP ID of 22246, its FspX of -1003.7918248322967 in fsp_info.csv was read into memory as -1003.7918248322968 without using float_precision='round_trip'\n    fspIds = pd.read_csv(os.path.join(segLibFolder,fspInfoFileName),float_precision='round_trip',index_col=False)[['FspId','FspX','FspY']]\n\n    # initialize the index DFs and tile ID\n    fspIdxDf = pd.DataFrame(columns=fspIdxColumnNames)\n    tileIdxDf = pd.DataFrame(columns=tileIdxColumnNames)\n    tileId = 1\n    for t in tb:\n        print('Processing tile: ', tileId)\n\n        # tile boundary, not the cell center boundary!\n        tminX, tmaxX,tminY, tmaxY = t\n        print('Tile extent (minX, maxX, minY, maxY) :',(tminX, tmaxX,tminY, tmaxY))\n\n        # find the segments that intersect the tile rectangle\n        segs = segExts[~((segExts['MinX']&gt;tmaxX) | (segExts['MaxX']&lt;tminX))] # rectangles are NOT on left or right of each other\n        segs = segs[~((segs['MinY']&gt;tmaxY) | (segs['MaxY']&lt;tminY))] # rectangles are NOT on top or bottom of each other\n        segs = segs['FileName'].to_list()\n        print('Number of segments interseting with the tile: ', len(segs))\n\n        # find the FPP-FSP relations in the tile from all the intersecting segments\n        # Initialize the df\n        tdf = pd.DataFrame()\n        for mf in segs: #segMatFileFullNames:\n            # read mat file\n            matVar = ReadMatFile(mf,matRelVarName)\n            # converting array to a dataframe\n            sdf = pd.DataFrame(matVar, columns=matRelColumnNames)\n\n            # rename columns to better names\n            betterColumnNames = [\"FspX\", \"FspY\", \"FppX\", \"FppY\", \"Dtf\", \"FilledDepth\"]\n            d=dict(zip(matRelColumnNames,betterColumnNames))\n            sdf.rename(columns=d,inplace=True) # inplace changing column name\n\n            # Code used to find out which segment file (e.g., SLIE_segment40.mat) in library 'midkan\" has problem \n            # where column \u201cDTF + fill depth\u201d is LESS than the \"DTF\" column\n            # t  =  sdf['FilledDepth'] - sdf['Dtf']\n            # t  =  t &lt; -0.1\n            # if t.any():\n            #     print('Segment: ', mf)\n            #     print(f'Found negative depression in {mf}!')\n\n            # select the FPPs within the tile\n            sdf = sdf[(sdf['FppX']&gt;=tminX) &amp; (sdf['FppX']&lt;=tmaxX) &amp; (sdf['FppY']&gt;=tminY) &amp; (sdf['FppY']&lt;=tmaxY)].copy() # tell pandas we want a copy to avoid \"SettingWithCopyWarning\" in line 86 when there is only one segment mat file read\n            # print('Segment:', segId, 'Number of FSP-FPP relations:', len(sdf))\n            if tdf.empty:\n                tdf = sdf\n            else:\n                # tdf = tdf.append(sdf)\n                tdf = pd.concat([tdf, sdf])\n        print('Total number of FSP-FPP relations in the tile:', len(tdf))\n\n        # save the FSP-FPP relations and update the index dfs\n        if (not (tdf.empty)) and (len(tdf) != 0): # there are some FPPs in the tile\n            # #calculate FilledDepth\n            # tdf['FilledDepth'] = tdf['FilledDepth']-tdf['Dtf'] # This is NOT necessary as FLDPLN model output changed to save \"sink fill depth\" in v8. Modified by Xingong on 7/1/24\n\n            # Calculate FSP center extent for the tile\n            fspMinX = tdf['FspX'].min()\n            fspMaxX = tdf['FspX'].max()\n            fspMinY = tdf['FspY'].min()\n            fspMaxY = tdf['FspY'].max()\n\n            # Calculate FPP center extent within the tile\n            fppMinX = tdf['FppX'].min()\n            fppMaxX = tdf['FppX'].max()\n            fppMinY = tdf['FppY'].min()\n            fppMaxY = tdf['FppY'].max()\n\n            # turn FPP coordinates into row and column within the tile\n            # Note that the row and column start at (fppMinX, fppMaxY)!\n            tdf['FppX'] = ((tdf.FppX-fppMinX)/cellSize).round()\n            tdf['FppY'] = ((fppMaxY-tdf.FppY)/cellSize).round()\n            # tdf['FppIdx'] = tdf.FppX * tRows + tdf.FppY\n\n            # rename ['FppX','FppY'] to ['Col','Row']\n            tdf.rename(columns={'FppX':'FppCol','FppY':'FppRow'},inplace=True)\n\n            # merge relations with FSP IDs\n            tdf = tdf.merge(fspIds,how='left',on=['FspX','FspY'])\n\n            # # check FSP IDs\n            # if tdf.isnull().values.any():\n            #     print('There are NAN in the dataframe!')\n\n            # remove FspX &amp; FspY\n            tdf.drop(['FspX','FspY'],axis=1,inplace=True)\n\n            # reorder columns\n            tdf = tdf[relColumnNames]\n\n            # set datatypes for the columns\n            tdf=tdf.astype(dtype={\"FspId\":np.int32, \"FppCol\":np.int32, \"FppRow\":np.int32, \"Dtf\":np.float32, \"FilledDepth\":np.float32},copy=False)\n            # convert float64 to float32 before saving and create index on FspX and FspY for speed up merge during mapping\n            # tdf.astype(np.float32,copy=False).set_index(keys=['FspX','FspY'],inplace=True)\n\n            # save the relations in the tile in a file\n            print('Saving FSP-FPP relations in a file...')\n            if fileFormat == 'snappy':\n                # filePathName = os.path.join(segLibFolder, tileFileMainName+'_'+str(tileId)+'.gzip') # for gzip format\n                filePathName = os.path.join(tiledLibFolder, tileFileMainName+'_'+str(tileId)+'.snz') # for snappy format\n\n                # save to parquet file. Can only save one DataFrame!\n                # tdf.to_parquet(filePathName,engine='fastparquet',compression='gzip',index=False) # gzip\n                tdf.to_parquet(filePathName,engine='fastparquet',compression='snappy',index=False) # snappy fast processing\n                # tdf.to_parquet(filePathName,engine='fastparquet',compression='snappy',index=True) # snappy fast processing\n            elif fileFormat == 'mat':\n                # separate columns by datatypes (int32 and float32)\n                fspFppsArray = tdf[relColumnNames[0:3]].to_numpy(dtype=np.int32)\n                dtfFilledDepthArray = tdf[relColumnNames[-2::]].to_numpy(dtype=np.float32)\n                # Save to compressed .mat file\n                dfDic = {'FspFpps': fspFppsArray,'DtfFilledDepth': dtfFilledDepthArray}\n                # Tile cannot be too large which may cause failure in writing into .mat file. See https://github.com/scipy/scipy/issues/12465\n                filePathName = os.path.join(tiledLibFolder, tileFileMainName+'_'+str(tileId)+'.mat')\n                sio.savemat(filePathName, dfDic, do_compression=True) \n            else:\n                print('Unsupported file format!')\n                return\n\n            # calculate the min and max DTF for each FSPs in the tile\n            fspDf = tdf.groupby(['FspId'], as_index=False).agg(MinDtf = ('Dtf', min),MaxDtf = ('Dtf', max))\n            print('Number of unique FSPs in the tile:', len(fspDf))\n            # print(fspDf)\n\n            # add tile ID to fsp-tile index\n            fspDf['TileId'] = tileId\n            # reorder columns\n            fspDf = fspDf[fspIdxColumnNames]\n            # append to the index table\n            # fspIdxDf = fspIdxDf.append(fspDf,ignore_index=True)\n            fspIdxDf = pd.concat([fspIdxDf, fspDf], ignore_index=True)\n            # print('Fsp-Tile index for the tile:')\n            # print(fspIdxDf)\n\n            # Calculate FSP &amp; FPP external extents for saving in the tile-index file\n            fspMinX,fspMaxX,fspMinY,fspMaxY = fspMinX-hcs,fspMaxX+hcs,fspMinY-hcs,fspMaxY+hcs\n            fppMinX,fppMaxX,fppMinY,fppMaxY = fppMinX-hcs,fppMaxX+hcs,fppMinY-hcs,fppMaxY+hcs\n            print('Tile FSP extent (fspMinX,fspMaxX,fspMinY,fspMaxY): ', (fspMinX,fspMaxX,fspMinY,fspMaxY))\n            print('Tile FPP extent (fppMinX,fppMaxX,fppMinY,fppMaxY): ', (fppMinX,fppMaxX,fppMinY,fppMaxY))\n\n            # Calculate the min &amp; max DTF within the tile\n            minTileDtf = fspDf['MinDtf'].min()\n            maxTileDtf = fspDf['MaxDtf'].max()\n\n            # calculate number of relations and number of FPPs in the tile\n            numOfRels = len(tdf)\n            numOfFpps = len(tdf[['FppCol','FppRow']].drop_duplicates()) # groupby for fast?\n\n            # add tile ID and additional tile-related info to the tile index file\n            tileIdx = pd.DataFrame([[tileId,tminX,tmaxX,tminY,tmaxY,fppMinX,fppMaxX,fppMinY,fppMaxY,fspMinX,fspMaxX,fspMinY,fspMaxY,minTileDtf,maxTileDtf,numOfRels,numOfFpps]],columns=tileIdxColumnNames)\n            # append to the index table\n            # tileIdxDf = tileIdxDf.append(tileIdx,ignore_index=True)\n            tileIdxDf = pd.concat([tileIdxDf, tileIdx],ignore_index=True)\n            # print('Tile index for the tile:')\n            # print(tileIdxDf)\n\n            # move to the next tile\n            tileId +=1\n\n    # Save the index as a file\n    # print('Number of items in the fsp-tile index:', len(fspIdxDf))\n    # print('fsp-tile Index table:\\n',fspIdxDf)\n    # save index to a csv file\n    print('Save fsp-tile index as a CSV file ...')\n    fspIdxDf.to_csv(os.path.join(tiledLibFolder, tileFileMainName+'_fsp_index.csv'),index=False)\n\n    # print('Number of items in the tile-extent index:', len(tileIdxDf))\n    # print('Tile index table:\\n',tileIdxDf)\n    # save index to a csv file\n    print('Save tile index as a CSV file ...')\n    tileIdxDf.to_csv(os.path.join(tiledLibFolder, tileFileMainName+'_tile_index.csv'),index=False)\n\n    return metaData\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use fldpln in a project:</p> <pre><code>import fldpln\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"}]}